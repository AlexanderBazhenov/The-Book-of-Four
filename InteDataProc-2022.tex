\documentclass[a5paper,openany]{book}
  
\usepackage{cmap}  
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc} 
\usepackage[russian]{babel} 
\usepackage{amsmath,amssymb} 
\usepackage{euscript,upref}  
\usepackage{array,longtable}
\usepackage{indentfirst} 
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{stmaryrd} 
\usepackage[justification=centering]{caption}
\usepackage{calrsfs} 
\usepackage{url}
%\usepackage{index}
\usepackage{imakeidx} 
\usepackage{multirow,makecell,array}
%\usepackage{setspace} 
%\usepackage{calligra}
\usepackage{pgf,tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{subcaption}
\usepackage{ifthen}
\usepackage{subfiles}
%\usepackage{hyperref}
\usepackage{stackengine}
\usepackage{scalerel}

\usetikzlibrary{arrows.meta}
%\pgfplotsset{compat=1.7}
\pgfplotsset{compat=newest}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% URL проекта - https://ru.overleaf.com/project/5e954c887ac0ac0001d54ece 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%234567890123456789012345678901234567890123456789012345678901234567890123456789012345678
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\textwidth=114truemm
\textheight=165truemm
\oddsidemargin=-1cm
\evensidemargin=\oddsidemargin
\topmargin=-1cm
\sloppy
  
\pagestyle{plain}
%\mathsurround=1pt
%\tolerance=400
%\hfuzz=2pt
\makeindex
   
\captionsetup{font=small,labelsep=period,margin=7mm} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Закомментировать следующую строку, если требуется откомпилировать печатный,
% а не электронный вариант книги (в электронном варианте в библиографии добавляются DOI)
\newcommand{\electronicbook}{}%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%           Определения новых команд и макросов
% 
%\DeclareMathAlphabet{\mathcalligra}{T1}{calligra}{m}{n}
%\DeclareFontShape{T1}{calligra}{m}{n}{<->s*[1.8]callig15}{}
\newcommand{\mbf}[1]{\protect\text{\boldmath$#1$}}
\newcommand{\mbb}{\mathbb} 
\newcommand{\mrm}{\mathrm} 
\newcommand{\mcl}{\mathcal} 
\newcommand{\msf}{\mathsf} 
\newcommand{\eus}{\EuScript} 
\newcommand{\ov}{\overline} 
\newcommand{\un}{\underline} 
\newcommand{\Ji}{\textsl{Ji}\,}
\newcommand{\m}{\mathrm{mid}\;}
\newcommand{\w}{\mathrm{wid}\;} 
\newcommand{\Uni}{\mathrm{Uni}\,} 
\newcommand{\Tol}{\mathrm{Tol}\,} 
\newcommand{\Uss}{\mathrm{Uss}\,} 
\newcommand{\Ab}{(\mbf{A}, \mbf{b})} 
\newcommand{\Arg}{\mathrm{Arg}\;} 
\newcommand{\sgn}{\mathrm{sgn}\;} 
\newcommand{\ran}{\mathrm{ran}\,} 
\newcommand{\rer}{\mathrm{rer}\:} 
\newcommand{\pro}{\mathrm{pro}\,} 
\newcommand{\dom}{\mathrm{dom}\,} 
\newcommand{\SEV}{\mathrm{SEV}\,} 
\newcommand{\WEV}{\mathrm{WEV}\,} 
\newcommand{\Rsv}{\mathrm{Rsv}\,} 
\newcommand{\calX}{\mathrsfs{X}} 
\newcommand{\cond}{\mathrm{cond}} 
\newcommand{\mode}{\mathrm{mode}\,} 
\newcommand{\dual}{\mathrm{dual}\,} 
\newcommand{\dist}{\mathrm{dist}\,} 
\newcommand{\Dist}{\mathrm{Dist}\,} 
\newcommand{\const}{\mathrm{const}} 
\newcommand{\USS}{\varXi_\mathit{\hspace{-0.5pt}uni}} 
\newcommand{\TSS}{\varXi_\mathit{\hspace{-0.5pt}tol}} 
\newcommand{\NExt}{_{\scalebox{0.57}{$\natural$}}} 
%\newcommand{\ih}{\scalebox{0.67}[0.87]{$\Box$\hspace*{1pt}}} 
\newcommand{\ih}{\scalebox{0.7}[1.0]{$\oblong$}} 
\newcommand{\doi}[1]{
    \ifdefined\electronicbook
        %DOI:\href{http://doi.org/#1}{#1}
        DOI:#1
    \fi}%
  
\renewcommand{\r}{\mathrm{rad}\;} 
\renewcommand{\vert}{\mathrm{vert}\,} 
  
%% Сотовые скобки
\def\rlwd{.4pt}% \rule width
% Сотовые скобки: левая
\def\<{\kern1pt%
\setstackgap{S}{0pt}\def\stackalignment{l}
\ThisStyle{\scalerel*{%
  \stackunder[-\rlwd]{%
    \stackon[-\rlwd]{\rule{\rlwd}{4.5pt}}{\rotatebox{45}{\rule{4pt}{\rlwd}}}%
  }{\rotatebox{-45}{\rule{4pt}{\rlwd}}}%
}{\SavedStyle[}}}
% Сотовые скобки: правая
\def\>{%
\setstackgap{S}{0pt}\def\stackalignment{r}
\ThisStyle{\scalerel*{%
  \stackunder[-\rlwd]{%
    \stackon[-\rlwd]{\rule{\rlwd}{4.5pt}}{\rotatebox{-45}{\rule{4pt}{\rlwd}}}%
  }{\rotatebox{45}{\rule{4pt}{\rlwd}}}%
}{\SavedStyle[}}\kern1pt}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\renewcommand{\textfraction}{0}
\renewcommand{\topfraction}{1}
\renewcommand{\bottomfraction}{1} 
\renewcommand{\indexname}{Предметный указатель} 
  
%%  переопределение команды paragraph 
%%  для более точной регулировки вертикального отступа
\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
                         {2.0ex \@plus1ex \@minus.2ex}%
                         {-1em}%
                         {\normalfont\normalsize\bfseries}} 
\makeatother
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%           Определение счётчиков 
%  
\newcounter{DefNum}[section]
\newcounter{ExmpNum}[section]
\renewcommand{\theExmpNum}{\thesection.\arabic{ExmpNum}}
\newcounter{IncluDefi}
\newcounter{IreneExmp} 
\newcounter{BazhenovExmp} 
\newcounter{RadarExmp} 
% \newcounter{ConstExmp} 
% \newcounter{VarExmp} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%           Определение необходимых окружений 
%
\newtheorem{definition}{Определение}[section] 
% \newenvironment{example}% 
%   {\par\addvspace{\medskipamount}\addtocounter{ExmpNum}{1} 
%   \noindent\textbf{Пример {\thesection}.\arabic{ExmpNum}.}}% 
%   {\hfill$\blacksquare$\par\medskip} 
\newenvironment{example}% 
  {\refstepcounter{ExmpNum}%
  \par\addvspace{\medskipamount} 
  \noindent\textbf{Пример {\theExmpNum}.}
  }% 
  {\hfill$\blacksquare$\par\medskip} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%                Определения новых цветов
%
\definecolor{MyRed}{rgb}{0.6,0.3,0.1}
\definecolor{MyGreen}{rgb}{0.2,0.6,0.3}
\definecolor{MyBlue}{rgb}{0.3,0.5,0.85}
\definecolor{Blau}{rgb}{0.5,0.5,0.9}
\definecolor{Gray1}{rgb}{0.6,0.6,0.65}
\definecolor{Gray2}{rgb}{0.5,0.55,0.5}
\definecolor{Gray3}{rgb}{0.6,0.55,0.55}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
%For contents 
%\renewcommand{\l@section}{\@dottedtocline{1}{0.5em}{1.5em}}
%\renewcommand{\l@subsection}{\@dottedtocline{1}{2.5em}{2.0em}}
%\makeatother
%\setlength{\marginparwidth}{2cm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\title{Обработка и анализ данных \\* 
               с интервальной неопределённостью}
   
\author{А.Н.\,Баженов, С.И.\,Жилин, С.И.\,Кумков, С.П.\,Шарый}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{document}
  
\maketitle 

% УДК  518+519.4+519.6, 658.562.012+519.254  
%\begin{abstract}
%Аннотация...
% В книге рассматриваются методы обработки результатов измерений в условиях интервальной 
% неопределённости их погрешностей, когда отсутствуют какие-либо их вероятностные 
% характеристики. В подобных ситуациях стандартные статистические методы, основанные 
% на теоретико-вероятностной модели погрешностей, могут применяться лишь формально, 
% только для качественного анализа данных. Развиваемые в книге подход основан на методах 
% интервального анализа и смежных дисциплин,  с использованием специальных программных 
% средств и ЭВМ. 
%\\
%\textbf{\textit{Ключевые слова: }} измерение, неопределённость, интервальный анализ.
%\end{abstract}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   
\tableofcontents      %  Содержание  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\chapter*{Введение}
\addcontentsline{toc}{chapter}{Введение}   
  
  
Главная цель нашей книги --- представить внутренне согласованную систему понятий и 
терминов, относящихся к обработке данных, которые имеют интервальную неопределённость 
(или, более общо, ограниченную по величине неопределённость), а также дать краткий обзор 
основных и наиболее значимых результатов этого научного направления. Его можно назвать 
<<статистикой интервальных данных>> или же <<анализом интервальных данных>>. 
  
Возникнув в последние десятилетия XX века как альтернатива традиционной <<вероятностной 
статистике>>, основанной на методах теории вероятностей, анализ интервальных данных  
вскоре сделался важным и практически востребованным. Его интенсивное развитие вызвало 
к жизни необходимость введения различных понятий, обозначений, конструкций и т.\,п., 
которые не всегда согласовывались друг с другом. К~настоящему моменту возникла 
настоятельная необходимость унификации понятий, терминов и обозначений в этой научной 
области, чтобы специалисты могли лучше понимать друг друга и были понятными для тех, 
кто применяет методы обработки интервальных данных. 
  
Помимо понятий и конструкций анализа данных  мы по необходимости рассматриваем 
многое из того, что обычно относится к области метрологии --- понятия измерения, 
результата измерений и погрешности, базовые методы их обработки. Это совершенно 
неизбежно при введении нового интервального типа данных для результатов измерений. 
  
В практике обработки экспериментальных данных сейчас повсеместно используются 
статистические методы, основанные на идеях и результатах теории вероятностей (см., 
например, \cite{GUM, JCGM102RU, GOSTDirect, R50MNK, GOSTIndirect}). Эти подходы 
опираются на использование ряда допущений о вероятностных свойствах погрешностей 
измерений, а также наличие выборок представительной длины (как минимум, в несколько 
десятков измерений). Однако практики часто сталкиваются с ситуациями, когда выборки 
измерений коротки (например, не более 20--30 или даже не более десятка), а погрешности 
измерений не могут быть адекватно описаны с помощью инструментов теории вероятностей 
или же информация о вероятностных характеристиках погрешностей отсутствует. В этих 
условиях погрешности и неопределённости измерений нужно описывать и обрабатывать уже 
по-другому. В частности, для анализа данных можно применить методы <<интервальной 
статистики>>, основанные на идеях и результатах интервального анализа, использующие 
его подходы, алгоритмы и соответствующее программное обеспечение. 
  
Напомним, что \emph{интервалом} вещественной оси называется множество всех чисел, 
расположенных между заданными числами, включая их самих. Интервалы, таким образом, 
дают одну из форм представления целых диапазонов значений интересующих нас величин, 
и в этом качестве интервалы, главным образом, и используются в самой математике и  
её практических приложениях. Интервальный анализ --- это исчисление интервалов, т.\,е. 
наука о том, как оперировать с ними наподобие того, как мы делаем это с обычными 
числами, и затем использовать построенную технику для решения различных задач, где 
интервалы встречаются в виде данных, входных или промежуточных.  \index{интервал} 
  
Интервальный анализ возник в середине XX века в связи с бурным развитием компьютерных 
вычислений, в которых требовалось строго и аккуратно оценивать эффект от замены точных 
значений на приближённые. Сам термин <<интервальный анализ>> был введён американцем 
Р.Э.\,Муром в начале 60-х годов прошлого века, но он лишь удачно завершил оформление 
нового научного направления, которое реально зародилось несколькими десятилетиями 
ранее. В его становление внесли свой вклад многие исследователи из СССР, США, 
Великобритании, Германии, Польши, Японии, начиная с первых десятилетий XX века. 
В частности, у нас в стране идеи интервального анализа развивались В.М.\,Брадисом, 
математиком-педагогом и вычислителем, автором широко известных математических таблиц 
для средней школы, а затем выдающимся математиком и экономистом Л.В.\,Канторовичем 
\cite{Kantorovich}. Интервальный анализ и его приложения интенсивно развивались 
научной школой Н.Н.\,Яненко в Сибири. 
  
В настоящее время интервальный анализ является развитой математической  дисциплиной 
со своим кругом задач и своими специфическими методами, которые проникают во многие 
прикладные отрасли знаний. Для знакомства с различными аспектами современного 
интервального анализа можно порекомендовать книги \cite{AlefeldHerzberg, ApplInteAnal, 
SSharyBook, MayerBook, MooreBakerCloud, NeumaierBook}. Но терминология интервальной 
статистики --- интервальных методов обработки данных --- по необходимости наследует 
многое из традиционной статистики, где сложился развитый понятийный аппарат, который 
можно и нужно использовать в новых условиях. 
  
Наша книга предназначена  для широкого круга читателей и, прежде всего,  для 
прикладников --- метрологов, инженеров и техников, сталкивающихся с необходимостью 
обработки данных, результатов измерений и наблюдений, к которым традиционные 
теоретико-вероятностные методы неприменимы. Немало полезного найдут в книге 
также математики и специалисты по математическому моделированию, развивающие 
и использующие интервальный анализ и смежные области знаний. 
  
О содержании книги можно составить представление по её оглавлению. Отдельным крупным 
темам, в том числе математическим, посвящены отдельные главы книги, и они получились 
неравноценными по объёму. Около половины всей книги занимает Глава~4, рассматривающая 
задачу восстановления зависимостей. Это вызвано, конечно, большой практической 
важностью задачи, а также количеством и качеством полученных по этой теме результатов. 
Несмотря на довольно общее название книги, в ней не получили отражение вопросы 
обработки интервальных данных, которые относятся, например, к кластерному анализу, 
факторному анализу, планированию эксперимента и пр. Причина состоит, главным образом, 
в малой разработанности соответствующих задач для интервальных данных. Они ещё 
ждут своих исследователей. С другой стороны, общность названия книги оправдывается 
тем, что в ней впервые столь тщательно прорабатываются самые общие вопросы, которые 
возникают во всех частных дисциплинах, имеющих дело с обработкой и анализом 
интервальных данных. 
   
Материал книги использовался в учебных курсах по интервальному анализу данных, 
которые первый и последний авторы читают в Санкт-Петербургском политехническом  
университете и Новосибирском государственном университете. 
  
Авторы искренне благодарны Ларисе Аркадьевне Игнатенковой, руководителю Центра 
<<СЕРТИМЕТ>> УрО РАН, за ценную информацию, консультации и обсуждения, 
Всероссийскому интервальному веб-семинару, на котором обсуждалась значительная 
часть материала книги, а также А.В.\,Пролубникову, Е.В.\,Чаусовой, А.А.\,Карповой, 
Д.Ю.\,Надёжину и С.С.\,Шарой за полезные замечания. 
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\chapter[Обзор методов статистики и обработки данных]% 
        {Обзор методов статистики\\* и обработки данных} 
  
  
Согласно <<Малой Советской Энциклопедии>> \cite{MalSovEncycl},  \emph{статистика} 
--- это <<отрасль знаний, в которой излагаются общие вопросы сбора, измерения и 
анализа массовых статистических (количественных или качественных) данных \ldots>>. 
Открыв учебник по общей теории статистики (к примеру, \cite{EliseevaYuzbashev}), 
мы найдём то же определение статистики, дополненное замечанием о том, что <<статистикой 
называют также отрасль практической деятельности, направленную на сбор, обработку, 
анализ и публикации статистических данных\,\ldots>>.\index{статистика} Сам термин 
<<статистика>> появился в середине XVIII века, но историю статистических исследований 
можно проследить практически во всех достаточно развитых человеческих обществах, начиная 
с самых ранних. Например, в Древнем Китае проводились переписи населения, в Древнем Риме 
осуществлялся учёт численности населения и имущества граждан и т.\,д. 
  
По мере усложнения организации человеческих обществ и развития науки и технологий 
взрывным образом увеличились объёмы данных, которые приходилось собирать и анализировать 
статистике. В Новое время развились математические методы и инструменты статистики, 
из которых на первое место выдвинулась теория вероятностей --- математическая дисциплина 
о закономерностях, которые могут обнаруживаться в массовых явлениях. Этот союз оказался 
столь плодотворным и крепким, привёл к такому громадному прогрессу в статистических 
методах, что в какой-то момент, уже в XX веке, <<математической статистикой>> стали 
называть математическую теорию статистических методов, основанных исключительно 
на аппарате теории вероятностей. Получалось, что никаких других математических 
методов в статистике как будто нет. 
  
Естественно, что это не так, и самые различные математические методы в статистике 
никуда не исчезли, они продолжали развиваться и использоваться, но \ldots как будто 
не входя в <<математическую статистику>>. Обнаружившийся явный перекос терминологии 
привёл к тому, что Дж.\,Тьюки, видный американский математик, в конце 50-х годов 
прошлого века предложил оформить новую научную дисциплину --- <<анализ данных>>,
в котором\index{анализ данных} охватывались те математические методы обработки 
данных, которые не подпадали под <<математическую статистику>> в узком смысле 
этого слова. Идеи анализа данных Дж.\,Тьюки изложены в его более поздних обобщающих 
статьях \cite{Tukey1962, Tukey1972}. Последнюю из этих работ нередко расценивают, 
как революционную и положившую начало современным наукам об обработке данных 
\cite{Donoho2017}. Далее мы в равной мере будем использовать оба термина --- 
<<статистика>> и <<анализ данных>>, а развиваемые в этой книге методы безусловно 
считаем <<статистическими методами>>, хотя на теорию вероятностей они не опираются. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\section{Данные, погрешности и их обработка} 
  
  
На практике данные почти всегда не вполне точны, поскольку в процессе их получения 
на результаты измерений, наблюдений и сбора информации влияют внешние неконтролируемые 
факторы, сами измерительные приборы не являются абсолютно точными т.\,д. и т.\,п. 
Таким образом, реально мы должны иметь дело с той или иной неопределённостью --- 
состоянием частичного знания об измеряемой величине, когда нам известно какое-то 
её значение, но оно приближённое, и имеется также некоторая информация (качественная 
и количественная) о погрешности этого значения.  \index{неопределённость} 
   
На результаты измерений могут оказывать влияние изменчивость самих измеряемых величин, 
их непостоянство во времени или пространстве. Измерения могут выполняться в условиях 
неблагоприятной среды, когда на них воздействуют внешние неконтролируемые факторы, так 
называемые <<шумы>>, и случайные возмущения. Измерения могут быть осложнены наличием 
у применяемой аппаратуры неточного базисного уровня, постоянной помехи и т.\,п. 
В процессе обработки данных с помощью численных алгоритмов на результат могут влиять 
неизбежные неточности расчётов на ЭВМ (ошибки представления и округления и т.\,п.). 
  
Если некоторая величина, входящая в математическую модель, является прогнозным 
значением, т.\,е. устанавливается на основе прежнего опыта или в результате процедуры 
предсказания, то у нас просто может не быть данных для её определения. Но даже если 
данных достаточно, то процедуры предсказания, как правило, неточны и их результаты 
имеют неопределённость. Процедура прогноза может отягощаться методическими ошибками, 
а необходимые для её проведения вычисления могут давать не вполне правильные результаты 
из-за погрешностей округления, ошибок дискретизации и т.\,п. Наконец, даже при наличии 
данных процедура предсказания или прогноза существенно зависит от полноты этих данных, 
их достаточной <<представительности>>, в отсутствие которой результат прогноза может 
быть весьма неточен. В этом случае часто говорят о <<неполноте информации>>, на которой 
основан прогноз. 
   
С другой стороны, математические модели могут опираться на значения некоторых экспертных 
оценок, и они тоже могут иметь неопределённость, неточность и неоднозначность. Эти оценки 
подвержены влиянию как субъективных, так и объективных факторов. К первым можно отнести 
ограниченную компетенцию экспертов, незнание ими всех тонкостей и деталей рассматриваемого 
вопроса. Объективные факторы --- это невозможность учёта всех этих деталей и подробностей 
рассматриваемого явления. Сюда же могут быть отнесены технические трудности, в частности, 
ошибки округления представляемых значений, погрешности численных методов и т.\,п.  
  
В целом, неопределённости и неточности являются неотъемлемой частью развитых 
математических моделей реальности и могут порождаться различными причинами, 
классификация которых неоднократно рассматривалась в литературе (см., например, 
\cite{VoschininIMRO, Malikov, NovitskiZograf, Rabinovich1978, NguyenKreinWuXiang, 
Rabinovich2005}). 
  
Что касается погрешностей измерений и наблюдений, то их традиционно разделяют 
на три класса:  \index{погрешность}
\begin{list}{}{\leftmargin=14mm\itemsep=5pt\topsep=4pt\parsep=0pt} 
\item[(а)] 
систематические погрешности, \index{систематическая погрешность}
\item[(б)] 
случайные погрешности, \index{случайная погрешность}
\item[(в)] 
промахи или выбросы. \index{промах}\index{выброс} 
\end{list} 
   
\emph{Систематической погрешностью} измерения называется составляющая погрешности 
измерения, которая остается постоянной или изменяется по какому-то определённому 
закону при повторных измерениях одной и той же величины. \emph{Случайными погрешностями} 
называются неопределённые по своей величине и природе погрешности, в появлении каждой 
из которых не наблюдается какой-либо явной закономерности (см. \cite{Malikov}). 
\emph{Промахами} или \emph{выбросами} называются погрешности, приводящие к явному 
искажению результата измерений (см. также другие определения в \S\ref{OutlierSect}). 
Выбросы (промахи) представляют собой как бы бракованные измерения, которые должны 
быть устранены из результатов обработки для получения адекватных результатов. 
Мы займёмся ими подробно в \S\ref{OutlierSect} и \S\ref{RegrOutlSect}. 
  
Обработка погрешностей зависит от их природы и происхождения. 
  
Для выявления выбросов и промахов организуют специальный этап общей технологии 
обработки данных --- \emph{предобработку}, которая предшествует применению формальных 
математических методов. На этом этапе выбросы и промахи должны быть обнаружены и 
удалены из обрабатываемых данных.               \index{предобработка} 
   
Систематическая погрешность, как следует из самого её названия, --- это погрешность, 
в проявлении которой есть некоторая <<система>>. Если мы выявим и поймём эту <<систему>>, 
т.\,е. обнаружим и оценим систематическую погрешность, то сможем организовать её 
корректировку в результатах измерений путём введения необходимой поправки. Это типичный 
способ борьбы с систематическими погрешностями. Иногда таким образом полностью устранить 
систематическую погрешность всё-таки не удаётся, и какая-то её часть остаётся. Тогда эта 
остаточная погрешность будет представлять собой систематическую составляющую погрешности 
результата скорректированной процедуры измерения. 
  
Что касается случайных погрешностей, то для дальнейшего чрезвычайно важно разъяснение 
из классической книги \cite{Malikov}, которое помещено подстрочным примечанием на 
стр.~88: <<Слову ``случайный'' мы придаём условный смысл, принятый в физических науках. 
Мы считаем случайными те явления, которые определяются сложной совокупностью переменных 
причин, трудно поддающихся анализу; к этим явлениям индивидуальный подход невозможен, 
и лишь для их совокупности могут быть установлены определённые закономерности>>. 
Таким образом, термин <<случайный>> в этом понимании означает, фактически, 
<<непредсказуемый>> или же такой, в котором  отсутствует закономерность. 
%  случайный = непредсказуемый - по Алимову,  
%  случайный = отсутствует закономерность - по Коломогорову.   
      
Как поступать с возможными случайными погрешностями в данных? Как их учитывать 
и корректировать? 
     
Прежде всего, сам факт присутствия таких погрешностей в данных можно учесть подходящей 
математической постановкой задачи обработки этих данных. Например, при восстановлении 
функциональных зависимостей (см. Главу~4) вместо задачи интерполяции данных нужно 
рассматривать задачу их аппроксимации (приближения), так как не имеет смысла требовать 
точных равенств значений функции измеренным значениям. Вообще, получение результата 
измерения или наблюдения как решения задачи некоторого математического приближения 
к данным, учитывающей модель исследуемого объекта или явления, является основой 
\emph{аппроксимационных методов} обработки данных.\footnote{Напомним, что популярный 
метод наименьших квадратов был впервые представлен в статье А.-М. Лежандра 1806 года 
именно как аппроксимационный метод. Его вероятностные интерпретации появились уже позже.}  
\index{аппроксимационные методы} 
  
Если о природе случайных погрешностей ничего более не известно, то на этом можно 
и нужно остановиться и применять далее аппроксимационные методы. Если о природе 
случайных погрешностей известно что-то определённое, то можно применить для обработки 
данных более изощрённые методы, учитывающие дополнительную информацию. Для некоторых 
<<видов случайности>> существуют развитые математические теории, их описывающие, и 
если известно, что действующие в рассматриваемых измерениях случайные погрешности 
отвечают этой конкретной разновидности, то можно применить соответствующую теорию. 
  
В настоящее время существуют несколько различных подходов к описанию 
случайности, и некоторые их них чрезвычайно развиты и популярны. Прежде всего, 
это теоретико-вероятностная модель погрешностей, основанная на аппарате математической 
теории вероятности и приводящая к \emph{теоретико-вероятностным методам} обработки 
данных. Формирование её идейной основы восходит к Г.\,Галилею и Б.\,Паскалю (см. 
\cite{Maistrov}). Математические основания этого подхода заложили на рубеже XVIII и 
XIX веков К.Ф.\,Гаусс и П.С.\,Лаплас. Согласно этому подходу случайные погрешности 
измерений и наблюдений являются <<случайными величинами>> в том смысле, как они 
рассматриваются в теории вероятностей, и нам (более или менее) известны характеристики 
этих случайных величин. Теоретико-вероятностная модель погрешностей за прошедшие два 
века получила очень большое развитие и распространение, сделавшись одним из основных 
инструментов обработки данных. Тем не менее, её применение вызывает необходимость 
отвечать на многие нетривиальные вопросы, и эти ответы подчас не вполне 
удовлетворительны.              \index{теоретико-вероятностные методы} 
  
% Классический пример апологетики применения теории вероятностей в статистике 
% - это известная книга Р. фон Мизеса "Вероятность и статистика", хотя само 
% её название как будто несёт мысль, что статистика и вероятность - различные 
% предметы. 
Теоретико-вероятностные методы не являются единственными, основанными на развитой 
модели случайных погрешностей. То же самое характерно, к примеру, для методов 
статистики нечётких или размытых данных (см.~\S\ref{FuzzyStatSect}). Для полноты 
классификации парадигм статистики упомянем ещё \emph{эвристические методы} обработки 
данных,\index{эвристические методы} которые применяются при анализе малоизученных 
явлений, когда отсутствует чёткая модель и нет представления об искомых характеристиках 
явления или объекта. В эвристических методах в основу обработки данных кладётся 
какая-то более или менее разумная, с точки зрения природы, система правил, определяющих 
искомые характеристики явления. 
  
Ниже мы конспективно перечислим некоторые из проблем, возникающих при применении  
теоретико-вероятностных методов в статистике, так как именно они чаще всего применяются
некритично и без опоры на конкретные практические условия, в которых решаются задачи
обработки данных. Для краткости мы будем называть их <<вероятностной статистикой>>. 
Наш короткий обзор естественно дополняет работы \cite{Alimov1980, AlimovKravtsov, 
VoschiBochkovSotirov, Voschinin2002, VoschininIMRO, Chaikovskii, Tutubalin1977, 
Tutubalin1993}, где читатель также может найти обсуждение проблем и трудностей 
вероятностной статистики.    \index{вероятностная статистика} 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section{Критика вероятностной статистики} 
   
   
\paragraph{Статистическая устойчивость.} 
Прежде всего, нужно принимать во внимание тот факт, что главной интерпретацией  
самого понятия вероятности является так называемая частотная интерпретация, при 
которой вероятность понимается как предел относительной частоты рассматриваемого 
события в серии однородных независимых испытаний (экспериментов и т.\,п.). 
\index{вероятность} 
   
Современная теории вероятностей, как математическая дисциплина, построена на 
аксиоматике А.Н.\,Колмогорова (см. \cite{Kolmogorov}, а также \cite{Gnedenko,HCramer}). 
Математическая вероятность определяется в ней, как некоторая специальная мера 
на множестве событий, но внимательный взгляд обнаруживает, что она формализует именно 
частотное понимание вероятности. Наконец, именно частотная интерпретация вероятности 
является основой подавляющего большинства приложений теории вероятностей к практике, 
в частности, в статистике (см., например, \cite{vonMises, Tutubalin1972, Tutubalin2008, 
Eliasberg83}). Фактически, существование такой относительной частоты, как объективной 
характеристики реальных явлений и процессов, является фундаментом самого существования 
теории вероятностей и залогом её успешного применения к моделированию окружающего нас 
мира. Но важно осознавать, что эта модель не универсальна, она является определённой 
идеализацией, имеющей свою сферу применимости, весьма широкую, но всё-таки 
ограниченную. 
  
%%  пояснить сформулированную ниже мысль лучше   
Многие явления окружающего нас мира, в отношении которых вполне применимо обыденное 
слово <<случайный>>, не обладают свойством существования устойчивой относительной 
частоты, так как при росте числа наблюдений она для них не устанавливается, а имеет 
тенденцию к постоянным колебаниям (см., например, \cite{GorbanBook, GorbanPaper}). 
Для описания и анализа подобных явлений традиционная теория вероятностей непригодна. 
Свойство установления относительной частоты событий, наблюдаемой в конечных 
экспериментах, называется \emph{статистической устойчивостью} (иногда также 
<<статистической однородностью>>). Часто саму теорию вероятностей определяют как 
<<математическую теорию статистически устойчивых явлений>> (так делается, к примеру, 
в классической книге \cite{HCramer}).  \index{статистическая устойчивость} 
  
Для обеспечения устойчивости относительной частоты в экспериментах используют различные 
специальные приёмы обработки данных, выбор подходящей классификационной схемы событий 
(например, иерархическая классификация наблюдений) и пр. Но эти приёмы не универсальны 
и не всегда обеспечивают желаемую статистическую устойчивость. 
    
Так или иначе, если нет статистической устойчивости, то теоретико-вероятностные 
конструкции нельзя напрямую применять к решению задач, для которых ответ подразумевает 
именно частотный смысл. В этом случае и традиционная математическая статистика, 
основанная на теории вероятностей, также не может служить подходящим инструментом 
для обработки данных.   
  
Стоит отметить, что известны также другие интерпретации понятия вероятности --- 
субъективная вероятность (см., к примеру, \cite{deGroot}), логическая вероятность 
и некоторые другие. Они получили распространение в теории полезности и теории принятия 
решений (см. \cite{HRaifa}), но в контексте метрологии почти не употребляются. 
\index{вероятность} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
  
\paragraph{Проблема малых выборок.} 
Вероятностные закономерности проявляются как тенденции, которые наиболее ярко видны 
для массовых явлений. При малом или небольшим количестве испытаний выводы теории 
вероятностей могут оказаться весьма далёкими от истинной картины явления, и это 
составляет <<проблему малых выборок>> в вероятностной статистике. Фактически, 
при обработке экспериментальных данных почти всегда стоит вопрос о том, достаточен ли 
объём выборки (количество измерений и т.\,п.) для того, чтобы выводы, получаемые 
на основе теоретико-вероятностной модели погрешностей, имели приемлемую практическую 
достоверность. Связанный с этим вопрос: какие методы следует применять для обработки 
выборок, являющихся <<малыми>>, где теория вероятностей не способна адекватно описать 
поведение погрешностей?  
  
Существующие промышленные стандарты и методики обработки экспериментальных данных (см. 
\cite{GOSTDirect,GOSTIndirect}) регламентируют способы работы с выборками размера 
более $15$, тогда как обработка выборок из не более чем $15$ измерений стандартами 
вообще не рассматривается. При этом результаты обработки выборок размера от $16$ 
до $50$ рекомендуется рассматривать как не очень надёжные и сопровождать оговорками. 
Тем не менее, в практике измерений такие короткие выборки встречаются и не являются 
большой редкостью. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
  
\paragraph{Неизвестные характеристики вероятностные распределений.} 
Если законы теории вероятностей применимы к анализу погрешностей, то каков конкретный 
вид вероятностных распределений погрешностей? Каковы его числовые характеристики? 
Это непростые вопросы, на которые не всегда можно хоть как-то ответить на практике. 
   
Например, считается, что типичным законом вероятностного распределения погрешностей 
измерений непрерывных величин является нормальное гауссовское распределение, плотность 
вероятности которого задаётся функцией 
\begin{equation*} 
p(x) = \frac{1}{\sigma\sqrt{2\pi}}\;\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right),  
\index{нормальное распределение} 
\end{equation*} 
где $\mu$ --- среднее значение, $\sigma$ --- среднеквадратичное отклонение от этого 
среднего (см. Рис.~\ref{NormDistrPic}). Но насколько оно соответствует действительности? 
По этому поводу широко известно высказывание А.\,Пуанкаре из его книги \cite{Poincare}, 
впервые изданной в 1912 году: <<Этот закон не получают с помощью строгих рассуждений; 
доказательства, которые можно было бы привести, окажутся, кроме всего прочего, грубыми; 
некоторые опираются на утверждение, что вероятность уклонения пропорциональна самому 
уклонению. Тем не менее, все верят в этот закон \ldots\,  потому что экспериментаторы 
думают, что это математическое утверждение, а математики --- что это результат 
экспериментов>>.  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{figure}[ht]
\centering\small  
\setlength{\unitlength}{1mm} 
\begin{picture}(95,44) 
\put(0,0){\includegraphics[width=95mm]{pictures/NormalDistr.eps}} 
\put(91,1){$x$}  \put(37,40){$p(x)$} 
\end{picture} 
\caption{Типичные графики плотности вероятности}
нормальных гауссовских распределений. 
\label{NormDistrPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
За сто с лишним лет, прошедших со времени написания цитированного отрывка, были 
получены несколько математически строгих доказательств возникновения нормального 
закона распределения в стандартных ситуациях обработки данных, которые 
основываются на простых и естественных начальных допущениях. Читатель может 
познакомиться с ними в специализированных монографиях или даже в университетских 
учебниках по теории вероятностей. Было показано, что нормальное распределение 
в самом деле уникально по своим свойствам,  а потому при прочих равных условиях 
его предпочтительное использование имеет смысл. По этой причине первая часть 
сурового вердикта А.\,Пуанкаре стала неактуальной. Но вопрос о практической 
приложимости нормального распределения, как и исходных допущений, из которых 
он выводится, своей остроты нисколько не потерял. Обширная практика XX века, 
опиравшаяся на массовое промышленное производство, вкупе со специальными 
исследованиями показали, что вероятностные распределения погрешностей измерений 
в различных ситуациях могут сильно отличаться от нормального гауссовского 
(см. \cite{NovitskiZograf, Orlov1991,Orlov2016}). В частности, это особенно 
характерно для электрических измерений, проводимых в режиме реального времени. 
  
Конкретный вид функций распределения случайных величин, которые фигурируют в задачах 
обработки данных, может оказывать существенное влияние на способ их решения. Напомним, 
что популярный метод наименьших квадратов для определения параметров зависимостей 
по экспериментальным данным с более общей точки зрения является методом максимума 
правдоподобия для случая нормально распределённых погрешностей данных. Если же 
погрешности имеют другую функцию распределения, то метод максимального правдоподобия 
для восстановления зависимости превращается в другие численные методы --- метод 
наименьших модулей для распределения Лапласа \cite{MudrovKushko,MudrKush-LAD} или 
метод чебышёвского сглаживания (минимаксное приближение данных) для равномерно 
распределённых погрешностей. 
  
С другой стороны, для того, чтобы выяснить, какое вероятностное распределение имеют 
анализируемые данные, подчас требуется большая дополнительная работа. Например, 
для надёжного различения нормального гауссовского и логистического распределений 
необходимо не менее 2500 измерений (см., к примеру, \cite{Orlov2006}). Реальные 
объёмы выборок обычно значительно меньше. 
    
Описанные затруднения отчасти решаются с помощью так называемых непараметрических 
методов статистики (см., к примеру, \cite{FPTarasenko,CorderForeman,Wasserman}), 
но и эти методы не вполне свободны от некоторых априорных допущений о характере 
вероятностных распределений и об их конкретных параметрах.      
\index{непараметрическая статистика} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\paragraph{Неадекватность базовых вероятностных конструкций.} 
Эта группа вопросов касается часто используемых в теории вероятностей понятий 
\emph{независимости} и \emph{корреляции} случайных величин. Имеют ли данные 
корреляцию между собой? Или же они независимы?\index{корреляция}\index{независимость} 
Многие классические результаты вероятностной статистики требуют, как известно, 
независимости рассматриваемых случайных величин, представляющих результаты измерений, 
либо заданного уровня их корреляции. Проверка этих условий на практике почти невозможна. 
В частности, независимость величин при применении теоретико-вероятностных моделей либо 
постулируют (<<так должно быть>>), либо выводят из физических свойств рассматриваемого 
объекта или явления (<<эти параметры не могут влиять сильно друг на друга в силу 
особенностей конструкции и функционирования системы, поэтому они независимы>>). 
    
Здесь уместно процитировать самый первый параграф из книги П.В.\,Новицкого и 
И.А.\,Зограф \cite{NovitskiZograf}: <<Во-первых, применение методов математической 
статистики к обработке результатов измерений правомерно лишь в предположении 
о независимости между собой отдельных получаемых отсчётов. И, во-вторых, большинство 
приводимых далее формул теории вероятностей правомерно только для непрерывных 
распределений, в то время как распределения погрешностей вследствие неизбежного 
квантования отсчётов, строго говоря, всегда дискретны. Таким образом, условия 
непрерывности и независимости для случайных погрешностей соблюдаются лишь приближённо, 
а иногда могут и не соблюдаться, т.\,е. в математике под термином ``непрерывная 
случайная величина'' понимается существенно более узкое, ограниченное рядом условий, 
понятие, чем под термином ``случайная погрешность'' в измерительной технике>>. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\paragraph{<<Робастность>> модели обработки данных.} 
Под этим требованием понимается адекватная устойчивость оценок, получаемых на основе тех 
или иных моделей, к малым возмущениям в данных, т.\,е. к  вероятностным характеристикам 
распределений, их форме и параметрам. Некоторые вероятностно-статистические методы 
не обладают этим свойством, давая в некоторых задачах ответы, чувствительность которых 
к возмущениям в данных неразумно велика, тогда как решение задачи от входных данных 
столь сильно зависеть не должно.                 \index{робастность} 
%  привести пример содержательный! простейший пример - неустойчивость оценок МНК  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
  
\paragraph{Удобство вычислительных методов.} Насколько удобны и практичны 
вычислительные технологии для решения соответствующих задач статистики и обработки 
данных? 
  
Некоторые традиционные методы вероятностной статистики вполне удовлетворяют этому 
условию. Например, широчайшее распространение метода наименьших квадратов в задачах 
обработки данных обусловлено, помимо ясного теоретико-вероятностного смысла, ещё 
также его удобной вычислительной схемой. В линейном случае решение задачи наименьших 
квадратов сводится к решению нормальной системы линейных алгебраических уравнений или 
же к ортогональному или сингулярному разложениям матрицы исходной системы. 
  
Но в более сложных ситуациях методы вероятностной статистики технологической 
простотой уже не обладают. Например, тот же самый метод наименьших квадратов 
в случае нелинейной зависимости требует решения нетривиальной задачи оптимизации 
(см., к примеру, \cite{DemidenkoBook}). Альтернативные подходы к обработке данных 
также сводятся к решению аналогичных задач вычислительной оптимизации, что повышает 
их <<конкурентоспособность>>. 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
  
\paragraph{Неоднозначность теоретико-вероятностных методик.}  Некоторые из популярных 
понятий и методов вероятностной статистики носят неочевидный и неоднозначный характер, 
так что их практическое применение вызывает большие вопросы. Это относится, в частности, 
к популярному понятию статистической значимости, по поводу которого в последние годы 
вспыхнула оживлённая дискуссия \cite{Amrhein2019, Wasserstein}.  
\index{статистическая значимость} 
  
Дело в том, что практическое применение понятия статистической значимости требует 
фиксации её определённого уровня, относительно которого и делаются окончательные 
заключения о свойствах тех или иных явлений, феноменов и т.\,п. Но каким назначить 
числовое значение этого уровня в каждой конкретной ситуации --- большой вопрос, который 
в каждом отдельном случае должен решаться и обосновываться исследователем. Недостаточное 
внимание к этому процессу вкупе с некорректным применением самого понятия статистической 
значимости  приводит к неправильным выводам (см. подробности в \cite{Amrhein2019, 
Wasserstein} и других публикациях по этой теме). 
  
Сюда же можно отнести проблему <<тяжёлых хвостов>> вероятностных распределений.  
Носители большинства распределений, используемых в теории вероятностей, бесконечны, 
что противоречит физическому смыслу и реальности. Тем не менее, в силу чисто 
математических причин (удобства математического аппарата) подобные распределения 
широко используются при решении реальных задач обработки данных. Обрубание <<хвостов>> 
этих распределений в соответствии с физическим смыслом задач вызывает большие (иногда 
почти непреодолимые) математические трудности в применении привычного аналитического 
аппарата. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\medskip 
  
Многие их затронутых выше вопросов поднимались в многолетней дискуссии по вопросам 
применимости и адекватности теоретико-вероятностных методов в статистике, состоявшейся 
в Советском Союзе в 70--90-е годы XX века. Она была инициирована Ю.И.\,Алимовым (см., 
в частности, \cite{Alimov1980}), а со стороны специалистов по теории вероятностей ему 
противостоял В.Н.\,Тутубалин (см., в частности \cite{Tutubalin1977}). Итоги дискуссии 
были подведены в двух заключительных публикациях \cite{AlimovKravtsov, Tutubalin1993}, 
которые содержат ценнейшие мысли о взаимном соотношении статистики и теории 
вероятностей, о границах применимости вероятностных подходов в статистике. 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
   
\section[Альтернативы вероятностной статистике]% 
        {Альтернативы \\* вероятностной статистике} 
  
При неудовлетворённости теоретико-вероятностным описанием погрешностей часто удобнее 
работать с альтернативными моделями представления неопределённостей и неточностей 
в данных. В настоящее время предложены и развиваются несколько таких моделей, и 
главные и наиболее популярные из них основаны на методах интервального анализа и  
теории нечётких множеств. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\subsection{Статистика интервальных данных} 
\label{InteStatistiSect}    
   
   
\textit{Интервальной неопределённостью} называется состояние частичного знания 
о величине, которая не известна точно, но известны нижняя и верхняя границы её возможных 
значений, или,\index{интервальная неопределённость} иными словами, известен интервал 
возможных значений этой величины. В интервальном описании вместо вероятностных 
распределений мы считаем заданными интервальные оценки результатов измерений, т.\,е. 
нижние и верхние границы возможных значений измеряемой величины. 
  
Интервальная неопределённость является частным случаем более общего понятия ограниченной 
неопределённости. Напомним,\index{ограниченное множество} что множество $S$ в $\mbb{R}^n$ 
называется \emph{ограниченным}, если существует такая константа $C$, что $\|x\|\leq C$ 
для всех точек $x\in S$ и некоторой нормы $\|\cdot\|$ в $\mbb{R}^n$ (см. строгие 
определения в \S\ref{MeasSetInte}). Соответственно, \emph{ограниченной неопределённостью} 
называется частичное знание об интересующей нас величине, когда известно, что она 
принадлежит\index{ограниченная неопределённость} какому-то ограниченному множеству 
возможных значений. 
  
В принципе, для описания неопределённости можно использовать и неограниченные множества, 
но их недостаток состоит в том, что даже не слишком длинные цепочки операций с такими 
множествами часто становятся бессодержательными, давая в результате всю числовую ось 
$\mbb{R}$ или всё пространство $\mbb{R}^n$. 
  
В одномерном случае интервалы являются практически наиболее важными ограниченными 
множествами, так что другие множества неопределённости используются нечасто. Но 
в многомерном случае множествами возможных значений величины, имеющей ограниченную 
неопределённость, могут быть брусы (прямоугольные параллелепипеды с гранями, 
параллельными координатным осям), многогранники, параллелотопы (зонотопы), эллипсоиды 
и прочие объекты. Всех их мы относим к инструментам интервальной статистики и 
интервального анализа данных. 
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\subsection{Статистика нечётких данных} 
\label{FuzzyStatSect} 
    
При нечётком описании результатов измерений и наблюдений мы полагаем, что вместо их 
точных значений нам известны так называемые функции принадлежности нечётких чисел, 
возникающих в результате измерений \cite{Semenov2012,NguyenKreinWuXiang}. 
\index{нечёткие методы} 
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{figure}[ht]
\centering\small  
\setlength{\unitlength}{1mm} 
\begin{picture}(110,35) 
\put(0,0){\includegraphics[width=50mm]{pictures/FuzzyNumber-11.eps}} 
\put(47,4){$x$}
\put(34,20){$\mu_{1}(x)$}
\put(58,0){\includegraphics[width=50mm]{pictures/FuzzyNumber-22.eps}} 
\put(87,20){$\mu_{2}(x)$} 
\put(105,4){$x$}
\end{picture} 
\caption{Нечёткие числа $\mu_{1}$ и $\mu_{2}$ с их функциями} 
   принадлежности $\mu_{1}(x)$ и $\mu_{2}(x)$. 
\label{FuzzyNumbers}  
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Нечётким множеством} (см. \cite{DuboisPrade,Zadeh}) называется множество $X$, 
образованное элементами произвольной природы, которое дополнено так называемой 
\emph{функцией принадлежности} $\mu: X\to[0, 1]$, значение которой $\mu(x)$ на элементе 
$x\in X$ показывает <<степень принадлежности>> $x$ множеству $X$ (Рис.~\ref{FuzzyNumbers}). 
У стандартной функции принадлежности множества (называемой также \emph{индикаторной 
функцией} множества) значения могут быть равны только $0$ или $1$: $0$ соответствует 
состоянию <<не является элементом множества>>, а $1$ соответствует состоянию <<принадлежит
множеству>>. Допущение для функции $\mu$ непрерывного ряда значений из интервала $[0, 1]$ 
позволяет характеризовать ситуации с <<частичной принадлежностью>> множеству $X$, когда 
мы не вполне уверены, принадлежит ли элемент множеству. В новой ситуации мы можем 
оперировать количественной мерой этой принадлежности и строить на её основе наши 
выводы и заключения. \index{нечёткое множество}\index{функция принадлежности} 
  
Для построения содержательной теории нечёткого вывода и нечётких неопределённостей 
обычно ограничивают общность функции принадлежности $\mu$, требуя, чтобы она была 
\emph{квазивогнутой}. Напомним, что функция $f:\mbb{R}^{n}\to\mbb{R}$ называется 
квазивогнутой, если для любых аргументов $x$, $y$ и всякого $\lambda\in[0, 1]$ 
справедливо неравенство 
\begin{equation*} 
f\bigl(\lambda x+(1-\lambda )y\bigr) \geq \min\bigl\{ f(x), f(y) \bigr\}.
\end{equation*} 
Можно показать, что выписанное условие равносильно тому, что все множества уровня 
$\{ x\in\mbb{R}^{n} \mid f(x)\geq\alpha \}$ функции $f$ являются выпуклыми. 
В одномерном случае они являются интервалами. Нечёткие множества с квазивогнутыми 
функциями принадлежности называются \emph{нечёткими числами}, и они могут быть 
эквивалентным образом заданы как семейства вложенных друг в друга интервалов, 
которые соответствуют различными уровням принадлежности. \index{нечёткое число}  
  
Нечёткое или размытое описание неопределённости является одной из возможных альтернатив 
вероятностному описанию в тех ситуациях, когда мы не можем разумно задать или определить 
вероятностную функцию распределения погрешностей, как какой-то случайной величины. 
Для обработки данных, имеющих нечёткую неопределённость, предложены различные подходы 
(см., например, \cite{Semenov2012, SemenovKK16,NguyenKreinWuXiang}), в частности, 
большое развитие получили методы восстановления зависимостей по нечётким данным. 
   
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
   
\subsection{Почему интервалы?}   
   
В чём преимущества и недостатки интервалов в сравнении с другими способами описания 
неопределённости? Это очень содержательный и важный вопрос, ответ на который мы здесь 
кратко наметим. Имеются две важных причины, по которым интервалы нужны и важны 
при обработке данных.  
  
Во-первых, интервалы являются удобным средством описания и представления популярного 
типа неопределённостей, часто встречающихся в реальной жизни --- ограниченных 
по величине неопределённостей. Интервалы проще, чем вероятностные распределения или 
нечёткие множества. Для описания одномерного интервала нужно всего два числа (левый 
и правый концы или середина и радиус, см. \S\ref{IntervalSect}), тогда как для описания 
распределения вероятностей или нечёткого множества нужно задать функции, т.\,е. 
существенно больше значений. 
  
Интервал --- это <<бесструктурный объект>>, который более беден в своих выразительных 
возможностях, в описании деталей неопределённости, чем вероятностное распределение или 
нечёткое число. Но следствием этой <<бедности>> является лучшая развитость теории 
интервального анализа и интервальных вычислительных методов. Кроме того, интервалы 
и интервальные арифметики оказываются замечательными (или даже уникальными) во многих 
отношениях, в частности, по своим алгебраическим свойствам. Более сложные способы 
описания неопределённости гораздо более бедны с алгебраической точки зрения. 
  
Ещё один аргумент за использование интервалов для описания неопределённости состоит 
в том, что они являются предельным случаем сумм независимых ограниченных величин. 
В большинстве практических ситуаций погрешность измерения возникает в результате 
накопления и наложения большого количества независимых факторов. Оказывается, что 
если некоторая величина есть сумма большого количества малых независимых слагаемых, 
то множество всевозможных значений этой величины близко к интервалу, причем чем меньше 
значимость (вклад) суммируемых компонент, тем меньше отличие результата от непрерывного 
интервала. То есть, возможные <<дырки>> между отдельными компонентами связности таких 
сумм <<затягиваются>>, а результирующее множество становится <<всё более связным>> 
и всё более близким к интервалу. Этот результат составляет содержание <<предельной 
теоремы Крейновича>>, на которой мы остановимся поподробнее. 
     
Предположим, что у нас имеется прибор для измерения значений некоторой физической 
величины $x$ с гарантированной точностью $\epsilon$ (сообщаемой обычно производителем 
прибора). Слово <<гарантированный>> означает, что для результата измерения $\tilde{x}$ 
истинное значение величины $x$ удовлетворяет неравенству $|x-\tilde{x}|\leq\epsilon$, 
или, иными словами, возможные значения погрешности $(x-\tilde{x})$ принадлежат 
интервалу $[-\epsilon, \epsilon]$. Являются ли все точки интервала возможными 
для значения погрешности? 
  
В некоторых экзотических случаях ответ на этот вопрос отрицателен. Например, при 
измерении напряжённости электромагнитного поля вблизи элементов памяти современного 
цифрового компьютера наводки от микросхем памяти будут основным источником погрешности 
измерения. Поскольку элемент памяти может находиться в одном из двух возможных 
состояний (в зависимости от того, представляет он 0 или 1), то и создаваемая его 
полем погрешность может принимать лишь значения, близкие к этим двум крайним. 
  
Но в большинстве практических ситуаций погрешность измерения возникает в результате 
накопления и наложения большого количества независимых факторов, а приведённый выше 
пример одного источника ошибок действительно экзотичен. Оказывается, что если некоторая 
величина есть сумма большого количества малых независимых компонент, то множество 
всевозможных значений этой величины близко к интервалу, причем чем меньше значимость 
(вклад) суммируемых компонент, тем меньше отличие результата от непрерывного интервала. 
Действительно весомый аргумент в пользу применения интервалов для представления данных 
с ограниченными неопределённостями! 
  
Совершенно аналогичная ситуация имеет место в теории вероятностей, где согласно 
центральной предельной теореме сумма достаточно большого количества слабо зависимых 
случайных величин, примерно одинаковых по величине (так что ни одно из слагаемых 
не доминирует над остальными), имеет распределение, близкое к широко известному 
нормальному вероятностному распределению. В более общей ситуации распределение 
большого числа малозначащих случайных величин близко к одному из так называемых 
\emph{безгранично делимых распределений}, к которым принадлежит и нормальное 
гауссово распределение (см. \cite{Gnedenko}). Поэтому можно с уверенностью считать, 
что распределение суммарной погрешности в большинстве случаев также безгранично 
делимо --- факт, широко используемый в большинстве статистических приложений. 
  
Перейдем к формальным определениям и утверждениям. Cуммой $A+B$ двух множеств $A, 
B\subset\mbb{R}^n$ будем называть множество $\{\,a+b \mid a\in A, b\in B\,\}$, 
полученное как множество всевозможных сумм <<по представителям>>. Эту операцию 
называют обычно \emph{суммой Минковского} \cite{Berger}. В качестве расстояния 
$\,\dist(A,B)$ между множествами $A$ и $B$ рассматриваем известную в геометрии 
конструкцию \emph{хаусдорфова расстояния} между множествами (расстояния Хаусдорфа), 
которое использует в качестве основы расстояние между точками множеств. Напомним, что 
если на $\mbb{R}^n$ задано некоторое расстояние $d$, то \emph{хаусдорфово расстояние} 
между компактными множествами $A, B\subseteq\mbb{R}^n$ определяется \cite{Berger}, как 
\begin{equation}
\label{HausdorffMetric}
\arraycolsep 1pt
\begin{array}{rl}
\dist(A,B) = \max 
  \bigl\{\,\sup_{a\in A}\inf_{b\in B}\,d(a,b), \sup_{b\in B}\inf_{a\in A}
  \,d(a,b)\,\bigr\}.
\end{array} 
\end{equation} 
Оно имеет ясный и естественный геометрический смысл, будучи максимумом из таких 
минимальных возможных неотрицательных чисел $r_A$ и $r_B$, что $r_B$-окрестность 
множества $A$ относительно расстояния $d$ содержит $B$, а $r_A$-окрестность 
множества $B$ относительно расстояния $d$ содержит множество $A$. 
    
\bigskip\noindent  
\textbf{Предельная теорема Крейновича} \cite{Kreinovich} 
\index{теорема Крейновича предельная}{\sl Сумма замкнутых множеств вещественной 
оси $\mbb{R}$, диаметр каждого из которых не превосходит $\delta$, отличается 
в хаусдорфовой метрике от интервала не более чем на $\delta$. Обратно, если 
для любого $\delta > 0$ множество $E$ вещественной оси может быть представлено 
как конечная сумма замкнутых множеств диаметра не более $\delta$, то $E$ является 
интервалом.} 
    
\bigskip 
В работе \cite{Kreinovich} приводятся также некоторые обобщения этого результата 
на многомерный случай. В 2018 году М.М.\,Рогинская и В.С.\,Шульман получили 
окончательное решение вопроса об описании предельного поведения сумм малых 
ограниченных множеств в линейных пространствах, в частности, в $\mbb{R}^n$ 
\cite{RoginsShulman}. Оказалось, что пределы таких сумм являются выпуклыми 
множествами (см. также изложение этих результатов в \cite{SSharyBook}). 
  
Наконец, ещё одна причина использования интервалов состоит в том, что некоторые 
физические (химические, биологически и т.\,п.) величины принципиально не могут быть 
выражены точечными значениями, но лишь интервалами. Поэтому интервалы представляют 
собой новый удобный тип данных, которым уместно дополнить те элементарные типы 
данных, которые уже используются в метрологии. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
  
\begin{example} 
С 2009 года атомные веса некоторых элементов в периодической системе химических 
элементов Д.И.\,Менделеева стали выражаться интервалами \cite{IUPAC-2016}. Это событие 
стало итогом длительного, продолжительностью более полувека, процесса осознания химиками 
неизбежной и неустранимой изменчивости величины атомных масс элементов в зависимости 
от того, где и как взята их проба. С середины XX века вместе с развитием измерительной 
техники и экспериментальных методик постепенно стало ясно, что различие результатов 
измерений атомных масс в различных пробах веществ носит принципиальный характер. 
  
Дело в том, что почти каждый химический элемент представлен в природе смесью своих 
изотопов, т.\,е. разновидностями атомов, сходных по своим химическим свойствам 
(структуре электронных оболочек), но отличающихся массой ядер. И относительная 
доля различных изотопов существенно меняется в зависимости от места и характера 
взятия пробы. Например, в тканях живых организмов преобладают более лёгкие изотопы 
химических элементов, нежели в неживой природе. Отличаются друг от друга 
относительные доли изотопов элементов на суше и в морях, и т.\,п. 
   
В периодической таблице Менделеева, поддерживаемой Международным союзом теоретической 
и прикладной химии IUPAC приводятся интервальные границы стабильных изотопов химических 
элементов \cite{IUPACtable}. Например, для кислорода, имеющего 3 изотопа с атомными 
массами 16, 17 и 18 на стр.~1858 статьи \cite{IUPAC-2016} приводятся данные, часть 
которых представлена в Табл.~\ref{IUPACOxygen}. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[h!tb]
\centering
\caption{Стабильные изотопы кислорода.} 
	\begin{tabular}{|c|c|}
		\hline
		Стабильный  изотоп & Молярная доля \\
		\hline
		$^{16}O$ & [0.997 38, 0.997 76] \\
		$^{17}O$ & [0.000 367, 0.000 400] \\
		$^{18}O$ & [0.001 87, 0.002 22] \\			
		\hline
	\end{tabular}
\label{IUPACOxygen}
\end{table} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
Для каждого стабильного изотопа некоторых популярных элементов в работе \cite{IUPAC-2016}
приведены границы, в пределах которых данный изотоп встречается в различных породах, 
атмосфере, водной среде в различных местах Земли. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[ht]
\centering\small
  \unitlength=1mm
  \begin{picture}(55,46)
    \put(0,1){\includegraphics[width=55mm]{pictures/DistrPlot.eps}} 
    \put(44,0){\mbox{атомный вес}}     
    \put(-10,42){\mbox{\begin{tabular}{c}доля\\[-2pt] изотопа\end{tabular}}}         
    \put(36,26){\mbox{гистограмма частот}} 
  \end{picture}
\caption{Как образуется интервал атомных весов элемента.} 
\label{AtomHystPic} 
\end{figure} 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Первоначально в 2009 году интервалы атомных весов были назначены для 10 химических 
элементов, но далее работа по <<интервализации>> продолжилась, и теперь в периодической 
таблице Д.И.\,Менделеева атомные веса б\'{о}льшей части элементов выражаются интервалами. 
Среди них --- такие широко распространённые и важные элементы как водород, углерод, азот, 
кислород, кремний, сера и др. Интервалы дают двусторонние границы значений атомного веса 
для любой пробы <<нормального материала>> включающего эти элементы. При этом особо 
подчёркивается \cite{IUPAC-2016}, что внутри заданных интервалов не предполагается 
наличия какого-либо вероятностного распределения. Различные версии современной 
периодической таблицы Д.И.\,Менделеева имеют немного отличающийся вид, так что 
обозначения в них могут слегка отличаться: иногда интервалы обозначаются традиционно 
$[\un{a}, \ov{a}]$, а иногда $a\pm\Delta$. 
  
Рис.~\ref{AtomHystPic} наглядно показывает, как получаются эти интервалы на основе 
общего массива данных об атомных весах элементов. 
\end{example} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\paragraph{Ранняя терминология} 
  
В работах 80--90-х годов для обозначения рассматриваемого нами подхода к обработке 
интервальных данных очень часто использовался (в том числе и некоторыми авторами 
настоящей книги) термин  <<нестатистический подход к анализу данных>> (см., к примеру, 
\cite{VoschininSotirov,VoschiBochkovSotirov,ZhilinDiss}). Как было осознано 
впоследствии, он не слишком удачен для наименования методов анализа и обработки 
данных с интервальными (ограниченными) неопределённостями. 
  
Прежде всего, отрицание, как способ организации определения, не вполне продуктивен, 
поскольку не даёт конструктивного определения понятия. Термин <<нестатистический>> 
обозначает, каким этот подход не является, но не даёт понять, каким же он является 
и в чём его основная идея. 
  
Но главный недостаток эпитета <<нестатистический>> состоит в том, что он противоречит 
содержанию термина <<статистика>>, как в широком (см. определение в начале главы), 
так и в узком смысле. Напомним, что в узком смысле (согласно <<Математической 
энциклопедии>> \cite{MathEncycl}) \emph{статистика} --- это некоторая функция 
от  результатов наблюдений, например, выборочное среднее данных измерений. 
\index{статистика} 
  
Приёмы обработки интервальных данных, развиваемые в нашей работе и многих других, 
также относятся к статистике, которая не опирается на теорию вероятностей, но 
статистикой быть не перестаёт. Анализ и обработка интервальных данных --- это тоже 
ветвь большой науки статистики, возникшая в XX веке, и термин <<нестатистический>> 
в отношении её методов и моделей отражал лишь ошибочное отождествление всей статистики 
вообще с вероятностной статистикой. Аналогично и с термином <<статистика>> в узком 
смысле, его отрицание имеет ещё меньше смысла. 
  
Полезно посмотреть на англоязычные термины, которые обозначали развиваемую нами 
область знаний с 90-х годов прошлого века: analysis of data with unknown-but-bounded 
errors (анализ данных с неизвестной, но ограниченной погрешностью), set membership 
estimation, set theoretic estimation (теоретико-множественное оценивание); 
см. \cite{BoundApprHandbook}. Некоторые из них используются до сих пор. 
 
Известно, что в конце 60-х -- начале 70-х годов XX века Дж.\,Тьюки предложил термин 
<<анализ данных>> для обозначения тех способов и методик обработки данных, которые 
не могут быть уложены в жёсткое <<прокрустово ложе>> теоретико-вероятностных методов. 
Термин привился и ныне широко используется, так что нашу науку можно называть 
<<интервальным анализом данных>>. Тем не менее, на наш взгляд, проблему адекватного 
наполнения понятия <<математическая статистика>> всё-таки нужно решать, и решать 
принципиально. По этой причине мы  сознательно оперируем оборотами <<вероятностная 
статистика>>, <<интервальная статистика>> и <<статистика интервальных  данных>> 
и хотим, чтобы они укоренились в научном языке. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
  
\section[Место и особенности интервального подхода]%  
        {Место и особенности \\* интервального подхода} 
  
  
В последние годы появилось немало исследований, в которых рассматриваются интервальные 
неопределённости измерений (наблюдений), но в рамках теоретико-вероятностных моделей 
реальности (см. \cite{LemeshkoPostovalov, Orlov2006, OrlovLutsenko, BillardDiday}). 
При этом тоже говорится об <<интервальной статистике>> и <<статистике интервальных 
данных>>. Отличие нашей работы от этих исследований состоит в том, что мы рассматриваем 
ситуацию, когда аппарат теории вероятностей по каким-то причинам не применим к описанию 
погрешностей измерений. Соответственно, методы вероятностной статистики, рекомендуемые 
стандартами \cite{GUM, JCGM102RU}, тогда не работают. Возможно лишь их формальное 
использование, которое может приводить к некорректным результатам и недостоверным 
выводам. 
  
В этой книге рассматривается подход к обработке данных, имеющих интервальную 
или, более общо, ограниченную неопределённость и основанный на методах интервального 
анализа, с использованием специальных программных средств и ЭВМ. Мы представляем 
систему понятий, обозначений и т.\,п., относящуюся к этому подходу, которая призвана 
стать общей методической базой для исследований и работ в этой области. 
  
Отличительной чертой представляемого подхода является его применимость к выборкам 
любого объёма, начиная с нескольких измерений (в предельном случае --- одного). 
Как следствие, проблемы <<малых выборок>>, характерной для вероятностной статистики, 
в интервальном подходе вообще не существует. Это свойство особенно ценно в случаях, 
когда технические или экономические причины не позволяют проводить много экспериментов. 
В частности, такова ситуация с алгоритмами обработки результатов разрушающих 
измерений, измерений быстропротекающих процессов в реальном масштабе времени 
или при межлабораторных  сравнениях характеристик малого числа образцов. 
  
Основанием для подобной <<всеядности>> интервальных методов служит тот факт, что они 
не опираются на теорию вероятностей, закономерности которой выявляются в массовых 
явлениях, достаточно длинных выборках и т.\,п. Если же массовости нет, то, как 
отмечалось ранее, и теория вероятностей работает плохо или вообще не работает. 
Интервальные методы имеют \emph{аппроксимационный} характер, т.\,е. осуществляют 
приближение (аппроксимацию) данных в нужном нам смысле. Соответственно, для 
их применимости никакой массовости данных не требуется.  
\index{аппроксимационные методы} 
  
В каком-то смысле анализ интервальных данных идейно близок к математической теории 
приближения функций (теории аппроксимации), которая тоже исследует различные 
способы приближения функций непрерывного и дискретного аргументов (см., к примеру, 
\cite{Akhiezer,DemidMaronShuval,Remez}). В настоящее время её методы и результаты 
активно применяются при обработке данных. Но постановки задач, рассматриваемые 
в классической теории приближения, как правило, оперируют точными данными, 
не имеющими никакой неопределённости. Новизна развиваемых в книге методов 
--- в учёте интервальной неопределённости исходных данных. 
  
Для выборок большого объема, которые производятся в отношении явлений, адекватно 
описываемых теорией вероятностей, т.\,е. в традиционной ситуации, методы вероятностной 
статистики дают, как правило, более точные результаты, чем интервальные методы 
обработки данных (см., к примеру, исследование вопроса в работе \cite{ZhilinDiss}). 
Преимущество вероятностных методов статистики можно обнаружить даже в тех случаях, 
когда информация о распределении погрешностей неполна и известно лишь о существенном  
преобладании плотности вероятности в центре интервала над её значениями у концов 
\cite{Zhilin2003,OskorbinZhilinDronov}. Это объясняется тем, что статистические методы, 
использующие теорию вероятностей, аккуратно учитывают частоту повторяемости тех или 
иных значений измеряемой величины, а интервальные методы никак её не используют. 
Но если теория вероятностей по каким-либо причинам не приложима к рассматриваемым 
большим выборкам данных, то интервальные методы снова могут выйти на первые роли 
и оказать услугу в решении подобных задач. 
  
Развиваемые в книге идеи впервые были оформлены в явном виде в пионерской работе 
Л.В.\,Канторовича \cite{Kantorovich}, и далее неоднократно использовались (или 
даже переоткрывались) разными авторами. Характерной особенностью и, на наш взгляд, 
существенным недостатком большинства этих работ являлось то, что в них использовался 
традиционный математический аппарат, не использующий методы интервального 
анализа.\footnote{В работе Л.В.\,Канторовича  \cite{Kantorovich}, фактически, 
крупными мазками намечено развитие интервального анализа в линейных частично 
упорядоченных пространствах, хотя в явном виде слова <<интервал>> и <<интервальный>> 
в ней не используются.} Соответственно, для обозначения аналогичных нашему подходов 
в литературе использовались разные термины --- минимаксный подход и др. 
\index{минимаксный подход} 

Между тем, для оперирования с интервальными данными и интервальными величинами 
наиболее приспособлены и удобны именно методы интервального анализа, интенсивно 
развиваемые с 60-х годов XX века. Для описания и обработки ограниченных по величине 
неопределённостей, к примеру, в книгах \cite{Eliasberg76,Eliasberg83} используется 
аппарат неравенств (с модулями), который имеет чуть б\'{о}льшую общность, чем 
интервальный анализ, но явно проигрывает в отношении удобства, мощи, гибкости и 
наглядности. Читатель может убедиться в этом непосредственно, сравнив для примера 
описание методики восстановления линейной зависимости в \S 7.6.2 из книги 
\cite{Eliasberg76} с изложением аналогичных методов из Главы~4 нашей книги. 
  
Разумеется, представляемый в книге подход не лишён и недостатков (хотя некоторые 
из них имеют характер <<детских болезней>>). В частности, в нём довольно сложно 
учитываются наличие зависимостей (корреляции) между данными задачи и/или искомыми 
параметрами. Пока ещё довольно слабо изучено соотношение оценок, получаемых с помощью 
интервальных методов, с оценками традиционной вероятностной статистики. Прояснение 
этих вопросов является предметом будущих исследований. 
  
Что же касается достоинств, то в дополнение к сказанному выше их можно обозначить 
следующим образом. Интервальный подход позволяет построить достаточно простую и 
элегантную методику определения выбросов в данных. В задаче восстановления зависимостей 
просто и естественно учитываются как неопределённости в независимых переменных, так и 
в значениях функции. Оценка неопределённости полученных результатов рассчитывается 
автоматически в процессе вычислений, не требует дополнительного анализа и напрямую 
зависит от неопределённости исходных данных задачи. 
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\chapter[Базовые понятия и математический аппарат]% 
        {Базовые понятия и \\* математический аппарат} 
\label{PrimaryConceptChap} 
  
  
В этой главе книги вводятся базовые понятия и математический аппарат, относящиеся 
к обработке интервальных данных. Фактически, в первой части главы приводятся самые 
основные понятия и факты интервального анализа. Изложение математики, по-возможности, 
кратко и не содержит второстепенных деталей и доказательств. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section{Интервалы} 
\label{IntervalSect} 
  
    
\paragraph{Вещественные интервалы.} 
Первичное понятие интервального анализа данных и интервальной статистики 
--- \emph{интервал}. Это простое подмножество множества всех вещественных 
(действительных) чисел, которое задаёт целый диапазон значений интересующей нас 
величины. С помощью интервалов можно описывать и моделировать неопределённости 
и неоднозначности. 
  
Интервалы могут определяться на вещественной оси, на комплексной плоскости, 
а также в многомерных пространствах. Кроме того, существуют различные определения 
интервалов, и некоторые их них не равносильны друг другу, задавая разные 
математические объекты. Далее нас будут интересовать, главным образом, вещественные 
интервалы, вещественные интервальные векторы и матрицы, так как именно они играют 
главную роль в измерениях и их обработке. 
  
\begin{definition} 
\textsl{Интервалом} $[a,b]$ вещественной оси $\mbb{R}$ называется  
множество всех чисел, расположенных между заданными числами $a$ и $b$, 
включая их самих, т.\,е.                           \index{интервал} 
\begin{equation*} 
[a, b] := \{\, x\in\mbb{R} \mid a\leq x\leq b\,\}. 
\end{equation*} 
При этом $a$ и $b$ называются \textsl{концами} интервала $[a,b]$, \textsl{левым} 
(или нижним) и \textsl{правым} (или верхним) соответственно. 
\end{definition}
  
Аналогичные термины, которые часто используются в математических текстах, --- 
это \emph{числовой промежуток} (замкнутый), \emph{отрезок}, \emph{сегмент} 
вещественной оси. Мы всюду используем термин \emph{интервал}, так как именно он 
дал название интервальному анализу, т.\,е. той дисциплине, которая легла в основу 
математических методов обработки данных с ограниченной неопределённостью. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[hbt]
\centering\small 
\setlength{\unitlength}{1mm}
\begin{picture}(70,17)
\put(0,0){\includegraphics[width=70mm]{pictures/Intervs.eps}}
\put(-5,6.6){\vector(1,0){80}} \put(71.5,7.6){$\mbb{R}$} 
\put(4.2,9.5){$a$} \put(30.5,9.5){$b$} 
\put(56.5,2.3){$\mbf{x}$} 
\end{picture}
\caption{Интервалы на вещественной оси.} 
\label{IntervalsPic} 
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Множество всех интервалов из $\mbb{R}$ обозначается символом $\mbb{IR}$. 
В противоположность интервалам и интервальным величинам мы будем называть 
\emph{точечными} те величины, значениями которых являются отдельные точки 
--- на вещественной оси, плоскости или, более общо, в каком-либо пространстве. 
Само множество вещественных чисел $\mbb{R}$ можно рассматривать как подмножество 
множества интервалов, т.\,е. как интервалы с совпадающими концами. Их обычно 
называют \textit{вырожденными интервалами}, тогда как интервалы с ненулевой шириной 
--- \emph{невырожденные}. Итак, $\mbb{R}\subseteq\mbb{IR}$.   
\index{интервал вырожденный}\index{интервал невырожденный} 
   
Наша система обозначений следует неформальному международному стандарту на обозначения 
в интервальном анализе, выработанному в 2002--2010 годах \cite{InteNotation}. 
В частности, интервалы и другие интервальные величины (векторы, матрицы и др.) всюду 
в тексте обозначаются жирным математическим шрифтом, например, $\mbf{A}$, $\mbf{B}$, 
$\mbf{C}$, \ldots, $\mbf{x}$, $\mbf{y}$, $\mbf{z}$, тогда как неинтервальные 
(точечные) величины никак специально не выделяются. Для интервала $\mbf{a}$ 
посредством $\un{\mbf{a}}$ или $\,\inf\mbf{a}$ обозначается его левый конец, тогда 
как $\ov{\mbf{a}}$ или $\,\sup\mbf{a}$ --- это его правый конец. В целом, $\mbf{a} 
= [\un{\mbf{a}}, \ov{\mbf{a}}]$, так что 
\begin{equation}
\label{LowUpRepres}
\mbf{a} = \{\,x\in\mbb{R} \mid \,\un{\mbf{a}}\leq x\leq \ov{\mbf{a}}\,\}.
\end{equation} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\paragraph{Характеристики интервала.} 
Любой интервал полностью задаётся двумя числами --- своими концами, но на практике 
широко используются также другие характеристики интервалов и представления интервалов 
на их основе. 
  
Важнейшими характеристиками интервала являются его \emph{середина} (центр) 
\begin{equation*}
\textstyle\index{середина интервала} 
\m\mbf{a} = \frac{1}{2}(\ov{\mbf{a}} + \un{\mbf{a}}),
\end{equation*}
и его \emph{радиус} 
\begin{equation*}
\textstyle\index{радиус интервала} 
\r\mbf{a} = \frac{1}{2}(\ov{\mbf{a}} - \un{\mbf{a}}).
\end{equation*} 
Нередко вместо радиуса рассматривается эквивалентное понятие \emph{ширины} 
интервала  \index{ширина интервала} \label{InteWid}
\begin{equation*}
\w\mbf{a} = \ov{\mbf{a}} - \un{\mbf{a}}. 
\end{equation*}
В целом, $\mbf{a} = \m\mbf{a} + [-1, 1]\cdot\r\mbf{a}$, что равносильно представлению 
\begin{equation}
\label{MidRadRepres}
\mbf{a} = \{\,x\in\mbb{R} \mid \,|x-\m\mbf{a}|\leq \r\mbf{a}\,\}.
\end{equation} 
Таким образом, задание середины и радиуса интервала также однозначно определяет его, 
чем часто пользуются и в теории, и на практике. 
  
Середина интервала --- это точка, которая <<представляет его>> наилучшим образом, 
так как наименее удалена от остальных точек этого интервала. Более точно, для любого 
$\tilde{x}\in\mbf{x}$ 
\begin{equation} 
\label{MidOptimal}
\max_{x\in\mbf{x}}\, |x - \tilde{x}| \  
   \geq \  \max_{x\in\mbf{x}}\, |x - \m\mbf{x}|. 
\end{equation} 
Радиус и ширина характеризуют разброс (рассеяние) точек интервала, т.\,е. абсолютную 
меру неопределённости или неоднозначности, выражаемой этим интервалом. Как уже 
говорилось выше, интервалы нулевой ширины (нулевого радиуса) обычно называют  
\textit{вырожденными}. Они отождествляются с вещественными числами, так что, 
к примеру, $[1, 1]$ --- это то же самое, что и $1$, а $[0, 0] = 0$. 
  
Важной характеристикой интервала является его \textit{модуль} или \textit{абсолютное 
значение}, определяемое как максимум модулей точек из интервала: 
\begin{equation} 
\label{InteModule}
\index{магнитуда}\index{модуль интервала}  
|\mbf{a}|\  = \;\max\,\{\,|a| \mid a\in\mbf{a}\,\} \ 
            = \;\max\,\{\,|\un{\mbf{a}}|, |\ov{\mbf{a}}|\,\}.  
\end{equation} 
Модуль интервала часто называется также термином \emph{магнитуда}. 
\index{магнитуда}\index{абсолютное значение} 
  
Модуль интервала --- это наибольшее отклонение его точек от нуля, фактически, аналог 
обычного модуля числа. С другой стороны, на практике часто бывает нужно знать наименьшее 
отклонение точек интервала от нуля, т.\,е. насколько далеко интервал отделён от нуля. 
Соответствующей характеристикой интервала является его \emph{мигнитуда}, которая 
определяется как  \index{мигнитуда} 
\begin{equation*} 
\langle\mbf{a}\rangle\  = \;\min\,\{\,|a| \mid a\in\mbf{a}\,\} \ 
= \left\{ 
\begin{array}{cl}
\min\,\{\,|\un{\mbf{a}}|, |\ov{\mbf{a}}|\,\}, & \text{ если } 0\not\in\mbf{a},\\[2mm]  
0, & \text{ если } 0\in\mbf{a}. 
\end{array} 
\right. 
\end{equation*} 
Фактически, мигнитуда интервала --- это антипод его магнитуды \eqref{InteModule}, 
т.\,е. абсолютной величины (модуля). 
  
Среди интервалов особую роль играют интервалы вида $[-a, a]$, имеющие своей серединой 
нуль. Мы будем называть их \emph{уравновешенными интервалами}. Среди всех интервалов 
с данным абсолютным значением (модулем) именно уравновешенные интервалы имеют наибольшую 
ширину. И наоборот, среди интервалов фиксированной ширины\index{уравновешенный интервал} 
уравновешенные интервалы имеют наименьшее абсолютное значение. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\paragraph{Расстояние между интервалами.} 
Для интервалов $\mbf{a}$ и $\mbf{b}$ \index{расстояние}\index{метрика} наиболее 
популярное и содержательное определение расстояния имеет следующий вид: 
\begin{equation}
\label{IntvalDist}
\dist(\mbf{a}, \mbf{b}) \  = \  \max 
  \bigl\{|\un{\mbf{a}} - \un{\mbf{b}}|, 
         |\ov{\mbf{a}} - \ov{\mbf{b}}| \bigr\}.
\end{equation}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\paragraph{Отношения между интервалами.}  
Интервалы являются множествами, составленными из вещественных чисел, и неудивительно, 
что большую роль для них играют теоретико-множественные отношения и операции 
(объединение, пересечение и др.). Особенно важно отношение включения одного интервала 
в другой:  
\begin{equation} 
\label{InclInteOrder} 
\mbf{a}\subseteq\mbf{b} \  \text{ равносильно тому, что } \ 
   \un{\mbf{a}}\geq\un{\mbf{b}}\;\text{ и }\;\ov{\mbf{a}}\leq\ov{\mbf{b}}.  
\end{equation} 
Отношение включения является частичным порядком и превращает множество интервалов 
в частично упорядоченное множество, важную и хорошо изученную математическую структуру
(см. \cite{DiscrMath,Shreider}). 
  
Помимо порядка по включению на множестве интервалов огромную роль играют также другие 
отношения, которые обобщают известный порядок <<$\leq$>> на вещественной оси $\mbb{R}$. 
Фундаментальным фактом является то, что порядок <<$\leq$>> между вещественными числами 
может быть обобщен на интервалы многими осмысленными способами (и даже бесконечно 
большим числом способов). Некоторые наиболее популярные из этих отношений приводятся 
в стандарте \cite{IEEE-1788} на интервальные вычисления на ЭВМ, который, в свою очередь, 
опирается на работу \cite{Allen}. Необходимо также отметить, что значительная часть 
бинарных отношений на $\mbb{IR}$, обобщающих порядок <<$\leq$>> на $\mbb{R}$, 
не являются полноценными отношениями порядка. 
  
Важную роль играет следующее упорядочение интервалов, для которого обычно используют 
тот же символ <<$\leq$>>, что и для чисел: 
  
\begin{definition} 
Для интервалов $\mbf{a}$, $\mbf{b}\in\mbb{IR}$ условимся считать, что
\textsl{$\mbf{a}$ не превосходит $\mbf{b}$} и писать <<$\,\mbf{a}\leq 
\mbf{b}$>> тогда и только тогда, когда $\,\un{\mbf{a}}\leq\un{\mbf{b}}\,$
и $\,\ov{\mbf{a}}\leq\ov{\mbf{b}}$. 
\end{definition} 
  
Соответственно, интервал называется \textit{неотрицательным}, т.\,е. <<$\,\geq 0$>>, 
если неотрицательны оба его конца. Интервал называется \textit{неположительным}, 
т.\,е. <<$\,\leq 0$>>, если неположительны оба его конца. 

  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
    
\paragraph{Теоретико-множественные операции над интервалами.} 
Если интервалы $\mbf{a}$ и $\mbf{b}$ имеют непустое пересечение, т.\,е. $\mbf{a}
\cap \mbf{b} \neq\varnothing$, то можно дать простые выражения для результатов 
теоретико-множественных операций пересечения и объединения через концы этих интервалов 
\begin{align*} 
\mbf{a}\cap\mbf{b} &= 
  \bigl[\max\{\un{\mbf{a}}, \un{\mbf{b}}\}, \min\{\ov{\mbf{a}}, \ov{\mbf{b}}\}\bigr], 
  \\[5pt] 
\mbf{a}\cup\mbf{b} &= 
  \bigl[\min\{\un{\mbf{a}}, \un{\mbf{b}}\}, \max\{\ov{\mbf{a}}, \ov{\mbf{b}}\}\bigr].  
\end{align*} 
Если же $\mbf{a}\cap\mbf{b} = \varnothing$, т.\,е. интервалы $\mbf{a}$ и $\mbf{b}$ 
не имеют общих точек, то эти равенства уже неверны. 
  
Обобщением операций пересечения и объединения являются операции взятия минимума 
и максимума относительно включения <<$\subseteq$>>:  
\begin{align} 
\mbf{a}\wedge\mbf{b} &= \label{InteMinExpr}
  \bigl[\max\{\un{\mbf{a}}, \un{\mbf{b}}\}, \min\{\ov{\mbf{a}}, \ov{\mbf{b}}\}\bigr], 
  \\[2mm]
\mbf{a}\vee\mbf{b} &= \label{InteMaxExpr}
  \bigl[\min\{\un{\mbf{a}}, \un{\mbf{b}}\}, \max\{\ov{\mbf{a}}, \ov{\mbf{b}}\}\bigr].  
\end{align} 
Они также понадобятся нам при обработке интервальных измерений. Первая из этих операций, 
<<$\wedge$>>, не всегда выполнима во множестве обычных интервалов, но это затруднение 
преодолевается путём расширения множества интервалов специальными элементами 
--- неправильными интервалами (см. ниже \S\ref{KaucherArithmSect}). 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   
\paragraph{Относительная ширина интервала.} 
Ширина интервала и его радиус являются мерами абсолютного рассеяния точек интервала. 
Но иногда требуется охарактеризовать относительную меру этого рассеяния, т.\,е., 
фактически, перейти к аналогу относительной погрешности. В интервальном анализе 
не существует одной общеупотребительной конструкции для этой цели, так как  
для различных типов интервалов приходится использовать разные меры. Основная 
причина такого положения состоит в том, что интервалы, содержащие нуль, фактически, 
представляют относительную неопределённость, которая превосходит 100\%, что очень много 
и совершено нетипично для этой величины. Для адекватного оперирования с <<относительным 
рассеянием>> в самом общем случае требуются другие инструменты. 
  
Для узких и <<не очень широких>> интервалов, не содержащих нуля, полезна 
относительная мера неопределённости        \index{относительный радиус} 
\begin{equation*} 
\rer\mbf{a} = \frac{\r\mbf{a}}{|\m\mbf{a}|},  
\end{equation*} 
которую мы будем называть \emph{относительным радиусом} интервала. В самом 
общем случае полезной характеристикой интервала является так называемый 
функционал Рачека $\chi$: 
\begin{equation*} 
\chi(\mbf{a}) = 
\left\{ \ 
\begin{array}{ll}
\un{\mbf{a}}/\ov{\mbf{a}}, & \text{ если } \;\un{\mbf{a}}\leq\ov{\mbf{a}},\\[1mm] 
\ov{\mbf{a}}/\un{\mbf{a}}, & \text{ иначе. } 
\end{array}
\right. 
\end{equation*} 
Нетрудно видеть, что $-1\leq\chi(\mbf{a})\leq 1$, причём $\chi(\mbf{a}) = 1$ для 
$\mbf{a} = a\in\mbb{R}$ и $\chi(\mbf{a}) = -1$ для уравновешенных интервалов с 
$\un{\mbf{a}} = -\ov{\mbf{a}}$. 
  
По существу,  функционал Рачека характеризует <<относительную узость>> интервала, 
или, иначе, меру его <<сосредоточенности>> или <<уравновешенности>>. Чем больше 
значения $\chi$, тем ближе интервал к точечной величине, и чем меньше $\chi$, 
тем сильнее интервал отличается от точки, т.\,е. одного определённого значения. 
Обсуждение свойств функционала Рачека и его приложений можно найти в работе 
\cite{IreneJCT1997} или в книге \cite{SSharyBook}. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\paragraph{Интервалы и вероятностные распределения.}  
Отметим, что интервал полностью определяется двумя своими концами и представляет 
собой объект, который не несёт никакой дополнительной структуры. Все точки интервала 
равноценны (равнозначны, равновозможны), и для каждой из них интервал даёт двустороннее 
приближение. В частности, интервал $\mbf{a}$ нельзя отождествлять с равномерным 
вероятностным распределением на $[\un{\mbf{a}}, \ov{\mbf{a}}]$ с постоянной плотностью 
$1/\w\mbf{a}$, так как в пределах $\mbf{a}$ с тем же успехом может быть определено 
любое другое вероятностное распределение или даже какое-то распределение, меняющееся 
во времени, --- случайный процесс. Более того, никакого вероятностного распределения 
на интервале $\mbf{a}$ может не быть, так как условия его существования (практически 
не вполне тривиальные) не будут выполнены.\footnote{Невозможность разумной вероятностной 
интерпретации интервалов, которая согласовывалась бы с другими их свойствами, показана, 
в частности, в работе \cite{Bronstejn}.} Но интервал $\mbf{a}$ всё равно будет описывать 
область возможных значений интересующей нас величины и может служить объектом, с которого 
начинаются дальнейшие математические конструкции и математическое моделирование реальности. 
  
В некоторых ситуациях всё-таки приходится назначать на интервалах какие-то 
вероятностные распределения. Это может диктоваться, к примеру, спецификой решаемой 
задачи, где эти распределения заданы по условиям задачи и составляют её неотъемлемую 
часть. С другой стороны, вероятностные распределения на интервалах необходимо также 
задавать для того, чтобы организовать сравнение результатов интервальных и 
теоретико-вероятностных методов в аналогичных задачах. В последнем случае равномерное
вероятностное распределение является одним из наиболее подходящих для этой цели, 
так как оно имеет максимальную энтропию (см. обоснование, к примеру, в работе 
\cite{KreinovichShary}), т.\,е. является наименее информативным среди прочих 
распределений. Но помимо равномерного распределения на интервалах могут применяться 
и какие-то другие, если они более подходят к конкретной задаче.  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\section[Классическая интервальная арифметика]% 
        {Классическая \\* интервальная арифметика} 
  
Значения физических величин входят в математические выражения для физических законов, 
в различные формулы, где они складываются и вычитаются, умножаются и делятся, возводятся 
в степень и т.\,д. То же самое необходимо проделывать и с интервалами их значений. 
Как  следствие, мы приходим к необходимости введения арифметических и различных других 
операций на множестве интервалов. 
  
Основным руководящим принципом является здесь определение операций между интервалами 
через результаты операций между их членами, т.\,е. <<по представителям>>. Наиболее 
простым и популярным является определение результата интервальной операции 
как множества всевозможных результатов этой же операции между числами из интервалов. 
Например, для интересующей нас бинарной (двухместной) операции <<$\star$>> можно 
считать, что 
\begin{equation} 
\label{IAMainPrinciple} 
\mbf{a}\star\mbf{b}\; = 
   \;\bigl\{\, a\star b \mid a\in\mbf{b}, \,b\in\mbf{b}\, \bigr\}.   
\end{equation} 
Аналогичным образом определяются интервальные аналоги для унарных (одноместных) 
операций. Тем самым получаем правила действий над  интервалами, позволяющие оценивать 
области значений выражений, составленных из операций, для которых реализован этот 
принцип. 
  
Если рассматриваются арифметические операции, т.\,е. $\star\in\{ +, -, \cdot, / \}$, 
то нетрудно показать, что задаваемые правилом \eqref{IAMainPrinciple} множества сами 
являются интервалами, исключая единственный неудобный случай деления на интервал 
$\mbf{b}$, который содержит нуль \cite{SSharyBook,MooreBakerCloud}. Конструктивные 
формулы, расшифровывающие этот общий принцип для отдельных арифметических операций, 
выглядят следующим образом: 
\begin{align}
& \mbf{a} + \mbf{b} = \left[\,\un{\mbf{a}} + \un{\mbf{b}},\,\ov{\mbf{a}}
 +\ov{\mbf{b}}\,\right],  \label{Addition}\\[5pt]
& \mbf{a} - \mbf{b} = \left[\,\un{\mbf{a}} - \ov{\mbf{b}},\,\ov{\mbf{a}}
 - \un{\mbf{b}}\,\right],  \label{Subtraction}\\[5pt]
& \mbf{a}\cdot\mbf{b} = \left[\,\min\{\un{\mbf{a}}\,\un{\mbf{b}},
 \un{\mbf{a}}\,\ov{\mbf{b}},\ov{\mbf{a}}\,\un{\mbf{b}},\ov{\mbf{a}}\,
 \ov{\mbf{b}}\}\right.,\left.\max\{\un{\mbf{a}}\,\un{\mbf{b}},
 \un{\mbf{a}}\,\ov{\mbf{b}},\ov{\mbf{a}}\,\un{\mbf{b}},\ov{\mbf{a}}\,
 \ov{\mbf{b}}\}\,\right],  \label{Multiplication}\\[5pt]
& \mbf{a}/\mbf{b} = \mbf{a}\cdot\left[\,1/\ov{\mbf{b}},\,1/\un{\mbf{b}},
 \,\right]\qquad\mbox{ для } \ \mbf{b}\not\ni 0.  \label{Division}
\end{align}  
Множество всех интервалов вещественной оси с операциями сложения, вычитания, 
умножения и деления, определёнными посредством \eqref{Addition}--\eqref{Division}, 
называется \textit{классической интервальной арифметикой}, и часто его обозначают 
также $\mbb{IR}$.                 \index{классическая интервальная арифметика} 
                                  \index{интервальная арифметика классическая} 
  
\begin{example} 
Пусть на участке электрической цепи постоянного тока приложенное напряжение находится 
в пределах интервала $[23, 25]$ \,Вольт, причём и сопротивление этого участка меняется 
(к примеру, в зависимости от изменяющейся температуры) в пределах $[100, 120]$ Ом. 
Каким будет ток в этом участке цепи? 
  
Для ответа на поставленный вопрос воспользуемся законом Ома, который даёт выражение 
для тока в виде 
\begin{equation*}
I = \frac{U}{R},    
\end{equation*}
где $U$ --- напряжение на участке цепи, $R$ --- её сопротивление. Подставляя вместо 
этих переменных интервалы их изменения и заменяя операцию деления на интервальное 
деление \eqref{Division}, получим интервал значений тока через участок цепи 
\begin{equation*}
\frac{[23, 25]\;\text{Вольт}}{[100, 120]\;\text{Ом}} \ 
   = \  \bigr[\,\tfrac{23}{120}, \tfrac{25}{100}\,\bigr]\;\text{Ампер} \  
   \approx \  [0.19167, 0.25]\;\text{Ампер}. 
\end{equation*} 
  
Это точный интервал значений тока, так как математическое выражение для него является 
очень простым и позволяет точно оценивать свою область значений с помощью классической 
интервальной арифметики. Для более сложных выражений оценка, полученная приведённым 
выше простым способом, может не быть равной области значений, но лишь содержит её, 
или, как часто говорят, является её внешней оценкой. (см. подробности 
в \cite{SSharyBook,MooreBakerCloud}). 
\end{example} 
  
Отметим, что формулы интервальной арифметики совпадают с известными формулами 
для оперирования с абсолютными погрешностями (см., к примеру, \cite{ANKrylov} и другие 
пособия) для операций сложения и вычитания. Для умножения и деления формулы преобразования 
обычно приводятся для относительных погрешностей, так как для абсолютных погрешностей 
они довольно сложны и малообозримы. Но интервальная арифметика позволяет обрабатывать 
и этот случай, так как имеет алгоритмический характер и её формулы легко реализуются 
на компьютере. 
  
Отметим важный частный случай интервального умножения, соответствующий произведению 
числа на интервал: 
\begin{equation} 
\label{NumIntProduct}
a\cdot\mbf{b} \ = \ 
\left\{ 
\begin{array}{ll}
\bigl[\,a\un{\mbf{b}}, a\ov{\mbf{b}}\,\bigr], & \text{если} \  a\geq 0, \\[2mm] 
\bigl[\,a\ov{\mbf{b}}, a\un{\mbf{b}}\,\bigr], & \text{если} \  a\leq 0. 
\end{array} 
\right. 
\end{equation} 
  
Алгебраические свойства интервальной арифметики, как алгебраической системы с двумя 
бинарными операциями <<$+$>> и <<$\cdot$>>, являются довольно слабыми и во многом 
необычными. Она не является ни кольцом, ни полем --- известными и хорошо изученными 
алгебраическими системами. Дело в том, что, во-первых, интервальные арифметические 
операции в большинстве случаев необратимы (см. следующий параграф). Во-вторых, 
операции сложения и умножения не связаны друг с другом привычным соотношением 
дистрибутивности. Вместо него имеет место более слабая \textit{субдистрибутивность}: 
\begin{equation*} 
\index{субдистрибутивность} 
\mbf{a}\,(\mbf{b} + \mbf{c}) \,\subseteq\, \mbf{a}\mbf{b} + \mbf{a}\mbf{c}. 
\end{equation*} 
Например, $[0, 1]\,(1 - 1) = 0 \;\subset\; [-1, 1] = [0, 1]\cdot 1 + [0, 1]\cdot(-1)$. 
  
Наиболее ценной является возможность применения интервальной арифметики в цепочках 
операций, которая позволяет получать внешние интервальные оценки областей значений 
целых выражений. Мы кратко коснёмся этого вопроса ниже в \S\ref{TransfInteSect}, 
а детальное рассмотрение темы читатель может увидеть в книгах \cite{ApplInteAnal, 
SSharyBook, MooreBakerCloud,NeumaierBook} и других публикациях по интервальному 
анализу. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section[Преобразования характеристик интервалов]%  
        {Преобразования \\* характеристик интервалов} 
   
  
В этом параграфе мы конспективно приводим свойства важнейших характеристик интервалов, 
необходимые при проведении с интервалами выкладок и оценок. Их доказательства читатель 
может увидеть, например, в \cite{SSharyBook,NeumaierBook}.  
   
Рассмотрим сначала свойства абсолютной величины и мигнитуды. Прежде всего, отметим 
их монотонность по включению: 
\begin{equation} 
\label{MMIncMo} 
\mbf{a}\subseteq\mbf{b} \quad\Rightarrow\quad 
     |\mbf{a}|\leq|\mbf{b}|  \ \mbox{ и } \ 
  \langle\mbf{a}\rangle\geq\langle\mbf{b}\rangle, 
\end{equation} 
что очевидно вытекает из определений этих величин. Связь мигнитуды и магнитуды 
с операцией сложения: 
\begin{equation}   
\label{MagTrIneq} 
|\mbf{a}| - \langle\mbf{b}\rangle \leq |\mbf{a}\pm\mbf{b}| 
     \leq|\mbf{a}| + |\mbf{b}|. 
\end{equation} 
Фактически, правое неравенство здесь является интервальным аналогом известного 
неравенства треугольника для модуля чисел. 
  
Связь мигнитуды и магнитуды с операциями умножения, деления и взятия обратной 
величины описывается следующими свойствами: 
\begin{align}      
& |\mbf{ab}| = |\mbf{a}|\,|\mbf{b}|,\qquad  \langle\mbf{ab}\rangle 
     = \langle\mbf{a}\rangle\,\langle\mbf{b}\rangle, 
     \label{MagMigProduct}                                      \\[2mm]
& |\mbf{a}/\mbf{b}| = |\mbf{a}|/\langle\mbf{b}\rangle,\qquad 
     \langle\mbf{a}/\mbf{b}\rangle = \langle\mbf{a}\rangle/|\mbf{b}|, 
     \quad\mbox{ если } \ 0\not\in\mbf{b}, 
     \label{MagMigQuotient}                                     \\[2mm]
& \Bigl|\frac{1}{\mbf{a}}\Bigr| = \langle\mbf{a}\rangle^{-1}, \quad 
     \mbox{ если } \ 0\not\in\mbf{a}. \label{MagMigInv} 
\end{align} 
Они аналогичны свойствам для вещественных чисел. 
   
Обратимся к свойствам середины интервала. Середина суммы (или разности) интервалов 
равна сумме (разности) середин интервалов: 
\begin{equation}
\label{MidPM} 
\m(\mbf{a}\pm\mbf{b}) = \m\mbf{a}\pm\m\mbf{b}. 
\end{equation} 
При умножении интервала на число его середина также умножается на это число: 
\begin{equation} 
\label{MidMult} 
\m(a\mbf{b}) = a\cdot\m\mbf{b}, \quad\mbox{ если } \  a\in\mbb{R}. 
\end{equation} 
    
Очень важны свойства радиуса интервала, так как радиус является аналогом 
абсолютной погрешности. Монотонность радиуса по включению: 
\begin{equation}
\label{RIncMo} 
\mbf{a}\subseteq\mbf{b} \quad\Rightarrow\quad  \r\mbf{a}\leq\r\mbf{b}. 
\end{equation} 
Аналогом формулы преобразования абсолютной погрешности приближённых величин 
в сложении и вычитании является следующее свойство радиуса: 
\begin{equation} 
\label{RadPM} 
\r(\mbf{a}\pm\mbf{b}) = \r\mbf{a} + \r\mbf{b}. 
\end{equation} 
Абсолютная однородность радиуса интервала: 
\begin{equation} 
\label{RadScaProd}  
\r(a\mbf{b}) = |a|\cdot\r\mbf{b}, \quad\mbox{ если } \ a\in\mbb{R}. 
\end{equation}                                             
Иными словами, при умножении интервала на число его радиус умножается на модуль 
(абсолютное значение) этого числа. 
  
Следующие два свойства радиуса дают, фактически, оценки снизу и сверху для абсолютной 
погрешности произведения: 
\begin{align}                                             
& \r(\mbf{ab})\,\geq\, 
  \max\,\bigl\{|\mbf{a}|\cdot\r\mbf{b},\,\r\mbf{a}\cdot|\mbf{b}|\bigr\} 
                                                      \label{LowProd}      \\[2mm] 
& \r(\mbf{ab})\, \leq\, |\mbf{a}|\cdot\r\mbf{b} + \r\mbf{a}\cdot|\mbf{b}|. 
                                                           \label{UppProd} 
\end{align} 
Неравенства \eqref{LowProd} и \eqref{UppProd} можно немного уточнить, и 
модифицированные оценки читатель найдёт в книге \cite{NeumaierBook}. 
Оценка радиуса обратной величины: 
\begin{equation} 
\label{QuotRad} 
\r\Bigl(\frac{1}{\mbf{a}}\Bigr) \; = \;        
  \frac{\r\mbf{a}}{\langle\mbf{a}\rangle\,|\mbf{a}|},
  \quad \mbox{ если } \ 0\not\in\mbf{a}. 
\end{equation} 
  
Из \eqref{RadPM} следует, что при сложении и вычитании интервалов ширина результата 
является не меньшей, чем у операндов (аналогично абсолютной погрешности). 
Из \eqref{LowProd} следует, что при умножении ненулевых интервалов ширина результата 
не может занулиться, если она не была равна нулю хотя бы для одного из сомножителей. 
Фактически, эти неравенства показывают, что для интервалов с ненулевой шириной 
не существует обратных по сложению и умножению среди обычных классических интервалов. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section[Независимые и связанные интервальные величины]% 
        {Независимые и связанные \\* интервальные величины} 
\label{IndepIntvalSect} 
  
В теории вероятностей и вероятностной статистике большую роль играют понятия 
независимости случайных величин и их корреляции, т.\,е. степени их зависимости друг 
от друга (см., например, \cite{HCramer}). В частности, многие красивые и важные 
результаты вероятностной статистики верны лишь при определённых условиях на зависимость 
и независимость фигурирующих в них случайных величин. Например, известная теорема 
Гаусса-Маркова об оптимальности оценок метода наименьших квадратов верна при условии, 
что погрешности данных являются независимыми случайными величинами. 
  
В интервальном анализе также существует понятие зависимости и независимости величин, 
которое играет важнейшую роль в теории и практических применениях интервальных 
методов (см. \cite{SSharyBook}). 
   
Прежде всего заметим, что интервал сам по себе описывает лишь границы возможных значений 
той или иной переменной величины. Для более тонкого анализа нередко требуется указание 
того, какая именно переменная может пробегать этот интервал, так как один и тот же 
интервал может представлять значения совершенно разных переменных. Следуя \cite{SSharyBook}, 
станем говорить, что задана \emph{интервальная величина} (интервальный параметр), если 
имеется переменная, которая может принимать значения в пределах некоторого интервала. 
На формальном математическом языке интервальной величиной является упорядоченная пара, 
которую мы будем обозначать специальными скобками $\lfloor a,\mbf{a}\rceil$, где $a$ 
--- переменная и $\mbf{a}$ --- интервал её возможных значений, $a\in\mbf{a}$. Допуская 
некоторую вольность, в тех случаях, где это не приводит к недоразумениям, договоримся 
употреблять для обозначения интервальной величины $\lfloor a,\mbf{a}\rceil$ также 
записи <<$a\in\mbf{a}$>> и <<$\mbf{a}\ni a$>>.      \index{интервальная величина} 
  
\begin{definition} {\rm(см. \cite{SSharyBook})}   
Для интервальных величин $\lfloor a_{1}, \mbf{a}_1\rceil$, $\lfloor a_{2}, \mbf{a}_2 
\rceil$, \ldots, $\lfloor a_{n}, \mbf{a}_n\rceil$ назовём их \textsl{совместной 
областью значений}\index{совместная область значений} множество всевозможных значений 
упорядоченного набора соответствующих переменных $(\,a_{1}, a_{2}, \ldots, a_{n})$ 
в $\mbb{R}^n$. 
\end{definition}  
  
Совместная область значений $\mcl{S}$ интервальных величин $\lfloor a_{1}, \mbf{a}_1 
\rceil$, \ldots, $\lfloor a_{n}, \mbf{a}_n\rceil$ --- это некоторое множество 
в $\mbb{R}^n$, и из принадлежностей $a_{1}\in\mbf{a}_1$, $a_{2}\in\mbf{a}_2$, \ldots, 
$a_{n}\in\mbf{a}_n$ следует, что 
\begin{equation} 
\label{JointRange}
\mcl{S}\subseteq\mbf{a}_{1}\times\mbf{a}_{2} \times \cdots\times\mbf{a}_{n}. 
\end{equation} 
Точного равенства в этом включении может не быть, и, в действительности, соотношение 
левой и правой частей включения \eqref{JointRange} оказывает огромное влияние 
на интервальное оценивание результатов различных операций и отображений. 
  
\begin{definition} {\rm(см. \cite{SSharyBook})}\label{DependInteDefi} 
Интервальные величины $\lfloor a_{1}, \mbf{a}_1\rceil$, $\lfloor a_{2}, 
\mbf{a}_2\rceil$, \ldots, $\lfloor a_{n}, \mbf{a}_n\rceil$ назовём \textsl{независимыми 
(несвязанными)}, если совместная область значений этих величин совпадает с прямым 
декартовым произведением интервалов их изменения $\mbf{a}_{1}\times\mbf{a}_{2}\times
\cdots\times\mbf{a}_{n}$. В~противном случае интервальные величины называются 
\textsl{зависимыми} или \textsl{связанными}. \index{зависимые интервальные величины} 
\index{независимые интервальные величины}\index{связанные интервальные величины} 
\end{definition}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[hbt]
\centering\small 
\setlength{\unitlength}{1mm}
\begin{picture}(40,38)
\put(0,0){\includegraphics[width=44mm]{pictures/DepnDefi1.eps}}
\put(41,3){\mbox{$a_1$}}
\put(7,34){\mbox{$a_2$}}
\put(11,3){\mbox{$\un{\mbf{a}}_1$}} 
\put(34,3){\mbox{$\ov{\mbf{a}}_1$}}
\put(1.5,13){\mbox{$\un{\mbf{a}}_2$}} 
\put(1.5,29){\mbox{$\ov{\mbf{a}}_2$}}
\end{picture}
\hspace*{14mm} 
\begin{picture}(40,38)
\put(0,0){\includegraphics[width=44mm]{pictures/DepnDefi2.eps}}
\put(41,3){\mbox{$a_1$}}
\put(7,34){\mbox{$a_2$}}
\put(11,3){\mbox{$\un{\mbf{a}}_1$}} 
\put(34,3){\mbox{$\ov{\mbf{a}}_1$}}
\put(1.5,13){\mbox{$\un{\mbf{a}}_2$}} 
\put(1.5,29){\mbox{$\ov{\mbf{a}}_2$}}
\end{picture} 
\caption{Пояснение к определению независимых} 
и связанных интервальных величин. 
\label{DepdIllustrPic} 
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Определение~2.4.1 для двух интервальных величин $\lfloor a_{1}, \mbf{a}_{1} \rceil$ и 
$\lfloor a_{2}, \mbf{a}_{2}\rceil$ можно пояснить следующим образом. Их независимость 
означает, что никакая из переменных $a_1$ и $a_2$ не оказывает влияние на изменение 
другой переменной в пределах своего интервала. Тогда при любых значениях $a_2$ интервал 
изменения $a_1$ равен $\mbf{a}_1$ (левый рисунок на Рис.~\ref{DepdIllustrPic}), а при 
любых значениях $a_1$ интервал изменения $a_2$ равен $\mbf{a}_2$ (правый рисунок 
на Рис.~\ref{DepdIllustrPic}). В целом, в обоих случаях совместная область значений 
переменных $a_1$ и $a_2$, т.\,е. пары $(a_{1}, a_{2})$, совпадает с прямым 
произведением $\mbf{a}_{1}\times\mbf{a}_{2}$. 
  
Удобно говорить также, что на рассматриваемые интервальные величины $a_{1}\in
\mbf{a}_1$, $a_{2}\in\mbf{a}_2$, \ldots, $a_{n}\in\mbf{a}_n$ \emph{наложены связи}, 
если имеются в виду какие-то соотношения для $a_1$, $a_2$, \ldots, $a_n$ в виде 
равенств, неравенств и т.\,п., которые ограничивают область их совместных значений. 
Например, связь вида $a_1^2 + a_2^2 + \ldots + a_n^2 \leq c^2$ в какой-нибудь 
механической конструкции означает, что переменные $a_{1}$, $a_{2}$, \ldots, $a_{n}$ 
могут изменяться в пределах своих интервалов $\mbf{a}_1$, $\mbf{a}_2$, \ldots, 
$\mbf{a}_n$, но евклидово расстояние точки $(a_{1}, a_{2}, \ldots, a_{n})^\top$ 
до начала координат не превосходит значения $c$. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[!htb] 
\centering\small 
\unitlength=1mm 
\begin{picture}(97,40)
\put(0,5){\includegraphics[width=44mm]{pictures/DepenDiagram1.eps}} 
\put(41,4){$z_1$}  \put(5,35){$z_2$} 
\put(20,4){$\mbf{z}_1$}  \put(-1,20){$\mbf{z}_2$} 
\put(57,5){\includegraphics[width=44mm]{pictures/DepenDiagram2.eps}} 
\put(98,4){$z_1$}  \put(62,35){$z_2$} 
\put(78,4){$\mbf{z}_1$}  \put(56,20){$\mbf{z}_2$} 
\end{picture} 
\caption{Диаграммы зависимости для независимых} 
и связанных интервальных величин. 
\label{DepenDiagramsPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
Конкретный вид зависимости (связанности) интервальных величин удобно представлять 
наглядно графически на чертеже, изображающем совместное множество значений этих 
величин на фоне декартова произведения $\mbf{a}_{1} \times\mbf{a}_{2} \times\ldots
\times \mbf{a}_{n}$. Мы будем называть такие чертежи \emph{диаграммами зависимости} 
(диаграммами связанности). \index{диаграмма зависимости} 
  
Классическая интервальная арифметика по самому своему построению предназначена 
для работы с независимыми интервальными величинами. Если интервалы данных задачи 
представляют зависимые (связанные) интервальные величины, то результаты классической 
интервальной арифметики, получаемые по формулам \eqref{Addition}--\eqref{Division}, 
могут быть гораздо более широкими, чем реальная область значений соответствующей 
арифметической операции. В целом, при связанности интервальных величин, участвующих 
в задаче, для вычисления областей значений функций, получения точных оценок и т.\,п. 
требуются дополнительные меры. Для учёта зависимости (связанности) величин 
в современном интервальном анализе успешно применяются, в частности, аффинная 
арифметика \cite{StolfiFigueiredo} и различные функциональные арифметики. 
  
Практическая проверка условий Определения~\ref{DependInteDefi}, необходимых 
для выяснения независимости или связанности интервальных величин является 
нетривиальной задачей. Фактически, она подразумевает построение совместной области 
значений интервальных величин, что нетрудно сделать в теории и для модельных 
примеров, но в реальной жизни подчас невозможно. Часто подменяют эту проверку 
на проверку зависимости или независимости рассматриваемых величин в рамках 
рассматриваемых физических (химических, экономических и т.\,п.) моделей. 
  
С другой стороны, для исследования независимости измерений, выполняемых в разные 
моменты времени, можно привлекать физический \emph{принцип причинности}: событие, 
произошедшее в момент времени $t_1$, может повлиять на другое событие, произошедшее 
в момент времени $t_2$ только при условии $t_{1} < t_{2}$.\index{принцип причинности} 
  
Интервальные измерения обладают важным свойством (впервые отмеченным 
в \cite{SSharyBook}), которое решительно упрощает некоторые математические модели 
обработки измерений. Интервальные результаты измерений, выполняемые в разные моменты 
времени, как правило, оказываются независимыми интервальными величинами в силу 
физического принципа причинности. В самом деле, пусть $\mbf{a}_1$ и $\mbf{a}_2$ 
--- интервалы результатов измерений величин $a_1$ и $a_2$, полученные в моменты 
времени $t_1$ и $t_2$, причём $t_{1} < t_{2}$. Ясно, что конкретные значения $a_2$ 
никак не могут влиять на возможность $a_1$ принимать значения из $\mbf{a}_1$, 
поскольку из будущего влиять на прошлое мы не можем. Следовательно, переменная $a_1$ 
может принимать любые значения из $\mbf{a}_1$ вне зависимости от значений $a_2$, 
так что совместная область значений пары $(a_{1}, a_{2})$ --- брус $\mbf{a}_{1}\times
\mbf{a}_{2}$. Эта ситуация соответствует левому чертежу на Рис.~\ref{DepdIllustrPic}, 
и, таким образом, интервальные величины $\lfloor a_{1}, \mbf{a}_{1} \rceil$ и 
$\lfloor a_{2}, \mbf{a}_{2}\rceil$ независимы. Отметим, что хотя мы обозначили 
переменные $a_1$ и $a_2$, т.\,е. разными идентификаторами, они могут означать 
одну и ту же физическую величину в разные моменты измерений.  
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\section{Преобразования интервалов} 
\label{TransfInteSect}
  
Одним из наиболее важных вопросов теории и практики обработки данных является вопрос 
о том, как преобразуются объекты, описывающие неопределённости, при различных операциях 
и отображениях. Пусть величина $x$ не известна точно, и мы имеем её приближённое 
описание $\mcl{A}(x)$ в рамках какой-либо модели неопределённости. Если поведение  
интересующего нас объекта описывается функциональной зависимостью $y = f(x)$, то каким 
будет приближённое описание для переменной $y$ в рамках этой же модели? 
  
Поставленный вопрос возникает, в частности, при косвенных измерениях, % дописать 
результат которых получается после вычисления какой-то функции от результатов 
первичных измерений (см.~\S\ref{MeasuResultSect}). 
  
В теории вероятностей этот вопрос решается индивидуальным рассмотрением различных 
частных случаев, которые соответствуют теи или иным конкретным распределениям и 
отображениям. Существуют, конечно, общие подходы и даже аналитические формулы, 
применяя которые мы, по крайней мере, теоретически можем найти результат преобразования 
произвольного вероятностного распределения (их можно увидеть, к примеру, в книгах 
\cite{ESVentsel, Gnedenko}). Но для серьёзных практических задач, в которых возникает 
необходимость работы с функциями нескольких переменных и т.\,п., значимость этих подходов 
из-за своей сложности очень невелика. В случае функций одной переменной формулы 
преобразования вероятностного распределения вполне обозримы, но для функций многих 
переменных их сложность становится почти неподъёмной. Как правило, в теории вероятностей 
подробно рассматриваются простейшие и самые важные случаи, как например, линейное 
преобразование нормального распределения, распределение хи-квадрат --- сумма квадратов 
нормальных распределений и т.\,п. Остальные случаи предлагается сводить, по-возможности, 
к этим <<каноническим>>. 
  
В нетривиальных ситуациях практически единственным способом определения результатов 
преобразования различных вероятностных распределений до недавних пор являлся метод 
Монте-Карло (см., к примеру, \cite{Sobol}). В последние годы перспективным подходом 
численного нахождения результатов преобразований вероятностных распределений 
зарекомендовал себя вычислительный вероятностный анализ \cite{Dobronets}. Его 
основная идея аналогична идее интервального анализа, а основой вычислительной 
техники являются элементарные операции с гистограммами. 
  
В интервальном анализе вопрос преобразования интервалов решается более технологично. 
Так как интервал --- это множество, на котором не задано никакой дополнительной 
структуры, то преобразования интервалов, фактически, сводятся к вычислению образов 
несложных множеств при различных отображениях. Это задача неплохо разработана 
в современной математике, для многих важных случаев она доведена до популярных и 
практичных  вычислительных алгоритмов. Немало хороших методов для её решения разработано 
в самом интервальном анализе. Кроме того, напомним, что оцениванием областей значений 
функций занимаются и другие ветви прикладной и вычислительной математики. Это, в первую 
очередь, теория оптимизации и её вычислительный отдел.\footnote{Теорию оптимизации 
часто называют неточным и двусмысленным американизированным термином <<математическое 
программирование>>.} Если обозначить посредством ran область значений функции, то 
для любой непрерывной функции $f:\mbf{X}\to\mbb{R}$ имеет место равенство 
\begin{equation*}
\ran(f,\mbf{X})\, := \, \bigl\{\, f(x) \mid x\in\mbf{x}\,\bigr\} \ 
= \  \Bigl[ \  \min_{x\in\mbf{X}} f(x) , \ 
                                  \max_{x\in\mbf{X}} f(x) \  \Bigr]. 
\end{equation*}
Численные методы оптимизации позволяют находить экстремумы функций, т.\,е. крайние 
точки областей значений, и эти методы напрямую приложимы к нашей задаче оценивания 
образа интервала при различных отображениях. 
  
Таким образом, простота и скупость интервального описания неопределённости 
оборачивается в рассматриваемом вопросе большими технологическими выгодами. Тем 
не менее, стоит отметить некоторые принципиальные ограничения на трудоёмкость 
решения задачи оценивания области значений функции и родственной ей задачи 
нахождения глобального оптимума. 
  
\bigskip\noindent  
\textbf{Теорема А.А.\,Гаганова} \cite{Gaganov} \  \index{теорема Гаганова} 
{\sl Для любого $\varepsilon > 0$ задача оценивания области значений полинома 
от нескольких переменных с абсолютной погрешностью $\varepsilon\,$ является 
NP-трудной.\\ 
Для $0 <\varepsilon < \tfrac{1}{8}$ задача оценивания области значений полинома 
от нескольких переменных с относительной погрешностью $\varepsilon\,$ является 
NP-трудной.\footnote{Появление константы $\tfrac{1}{8}$ в формулировке теоремы 
вызвано особенностью математического метода её доказательства. В данный момент 
не вполне ясно, принципиально ли это ограничение или нет.}} 
   
\bigskip 
Напомним, что NP-трудность задачи и родственное ему понятие NP-полноты задачи 
на современном уровне развития теории сложности вычислений означает <<труднорешаемость>>  
задачи (см. подробности в \cite{GareyJohnson}). Более точно, в этом случае не существует 
алгоритма для решения задачи, вычислительная сложность которого была бы ограничена 
некоторым полиномом от длины описания входных данных. Иными словами, для решения такой 
задачи потребуются алгоритмы, вычислительная сложность которых растёт экспоненциально 
с ростом размеров задачи.    \index{задача NP-полная}\index{задача NP-трудная} 
   
Результатом, аналогичным по смыслу теореме А.А.\,Гаганова, является теорема 
Кирфотта-Крейновича \cite{KearfottKreinovich}, которая утверждает, что за пределами 
класса выпуклых целевых функций поиск глобального экстремума является NP-трудным 
(труднорешаемым). \index{теорема Кирфотта-Крейновича} 
   
Тем не менее, за последние десятилетия развит богатый набор эффективных интервальных 
методов оценивания областей значений функций, основы которых мы кратко рассмотрим 
ниже. 
  
Напомним, что функция $f:\mbb{R}^{n}\to\mbb{R}$ называется \emph{рациональной}, если 
она задаётся аналитическим выражением, которое является конечной комбинацией переменных 
$x_1$, $x_2$, \ldots, $x_n$ и констант с четырьмя арифметическими операциями ---  
сложением, вычитанием, умножением и делением.\index{рациональная функция} 
    
%  пояснить, что такое вхождение, единственное вхождение  и т.п. вещи.  
%  увязать с задачей оценивания погрешностей косвенных измерений. Об этом написать 
%  более подробно 
  
\addvspace{\bigskipamount}\noindent
\textbf{Теорема} {\rm(основная теорема интервальной арифметики).}
\index{основная теорема интервальной арифметики}\\*%
{\sl Пусть $f(\,x_{1}, \ldots, x_{n})$ --- рациональная функция вещественных 
аргументов $x_{1}$, \ldots, $x_{n}$ и для неё определён результат  $f\NExt 
(\,\mbf{x}_{1}, \ldots, \mbf{x}_{n})$ подстановки вместо аргументов интервалов их 
изменения $\mbf{x}_{1}$, $\mbf{x}_{2}$, \ldots, $\mbf{x}_{n}\in\mbb{IR}$ и выполнения 
всех действий над ними по правилам интервальной арифметики. Тогда 
\begin{equation}
\label{MainIArInclu}
\bigl\{\, f( x_{1}, \ldots, x_{n}) \bigm| x_{1}\in\mbf{x}_{1}, 
   \ldots, x_{n}\in\mbf{x}_{n}\,\bigr\} \;\subseteq \  
   f\NExt(\mbf{x}_{1}, \ldots, \mbf{x}_{n}),
\end{equation}
т.\,е. $f\NExt(\mbf{x}_{1}, \ldots, \mbf{x}_{n})$ содержит область значений 
функции $f$ на множестве $(\mbf{x}_{1}, \ldots, \mbf{x}_{n})$. Если выражение 
для $f(x_{1}, \ldots, x_{n})$ содержит не более чем по одному вхождению каждой 
переменной в первой степени, то в \eqref{MainIArInclu} вместо включения выполняется 
точное равенство.} 
  
\bigskip 
Доказательство этого важного результата несложно, и читатель может найти его, 
например, в книгах \cite{SSharyBook,MooreBakerCloud}. 
 
Интервальная функция $f\NExt (\,\mbf{x}_{1}, \ldots, \mbf{x}_{n})$, построение 
которой описывается в формулировке основной теоремы интервальной арифметики, 
называется \emph{естественным интервальным расширением} рациональной функции 
$f(x_{1}, \ldots, x_{n})$.\index{естественное интервальное расширение} Оно является 
самой простой конструкцией интервального расширения, и точность оценивания с его 
помощью областей значений функций имеет первый порядок. 
  
Нахождение интервальных оценок областей значений с помощью основной теоремы интервальной 
арифметики мы будем называть \emph{естественным интервальным оцениванием}. 
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\section{Примеры интервальных расчётов} 
  
  
Интервальные оценки областей значений выражений существенно зависят от вида этих 
выражений и формы их записи. Обсуждение этого вопроса можно увидеть, к примеру, 
в книге \cite{SSharyBook}. Приведём два практических примера интервальных расчётов 
с формулами, которые встречаются в естественнонаучных законах. Вычисления далее 
проведены в системе компьютерной математики Octave, версия 6.2.0, с использованием 
пакета \texttt{interval} \cite{OctaveInterval}. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\begin{example} 
В современной биохимии важное значение имеют органические вещества, называемые  \textit{ферментами} (другое их название ---  энзимы). Они играют огромную роль 
в процессах обмена веществ в живых организмах, на много порядков повышая скорость 
химических реакций. Предложенное в 1913 году уравнение Михаэлиса-Ментен описывает 
зависимость скорости реакции, катализируемой ферментом, от концентрации субстрата 
\cite{Biokhimiya}: 
\begin{equation} 
\label{MichaelisMenten} 
v \; = \; V_{\max}\, \frac{S}{S+K_M},
\end{equation}
где $v$ --- скорость реакции, $V_{\max} $ --- максимальная скорость реакции, 
$K_M$ --- константа Михаэлиса, $S$ ---  концентрация субстрата. 

Для иллюстративного расчета зададимся значением $V_{\max} =  1.$, т.\,е. скорость 
реакции $v$ будем выражать в единицах  $V_{\max}$. Рассмотрим для определённости 
реакцию гидролиза аденозинтрифосфата, катализируемого миозином, для которой 
$K_M = 1.44 \cdot 10^{-4} $ моль/л. Возьмём интервал концентрации $\mbf{S}$ равным 
интервалу с серединой $K_M$ и радиусом, составляющим 10\% от $K_M$, т.\,е. 
\begin{equation*} 
\mbf{S} = [1.2959, 1.5841] \cdot 10^{-4}. 
\end{equation*} 
   
Для вычисления интервала $\mbf{v}$ выражение \eqref{MichaelisMenten} представим двумя 
способами: в исходном виде, и с делением числителя и знаменателя на $S$, а затем 
подставим вместо переменной $S$ интервал её изменения $\mbf{S}$. Во втором 
случае интервал $\mbf{S}$ входит в выражение \eqref{v22} один раз, и согласно 
основной теореме интервальной арифметики результат естественного интервального 
оценивания совпадает с точной областью значений выражения: 
\begin{align}
\mbf{v}_1 &= V_{\max}\;\frac{\mbf{S}}{\mbf{S}+K_M} = [0.42857, 0.57895], \label{v11}\\[2mm] 
\mbf{v}_2 &= V_{\max}\;\frac{1}{1+K_M/\mbf{S}} = [0.47368, 0.52381]. \label{v22}
\end{align} 
  
Средние величины интервалов \eqref{v11} и \eqref{v22} отличаются незначительно: 
\begin{equation*} 
\m \mbf{v}_1 =  0.5038  \   \approx \m \mbf{v}_2 =  0.4987. 
\end{equation*} 
В то же время радиусы результатов вычислений $ \mbf{v}$ для выражений \eqref{v11} 
и \eqref{v22} существенно различны: 
\begin{align*}
\r \mbf{v}_1 &=  0.075188, \\
\r \mbf{v}_2 &= 0.025063.
\end{align*} 
При этом имеет место соотношение:
\begin{equation*} 
\r \mbf{v}_1 > \r \mbf{v}_2
\end{equation*} 
в силу неоднократного вхождения $\mbf{S}$ в выражение \eqref{v11}. 
\end{example} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{example}
Полное сопротивление $Z$ цепи переменного тока складывается, как известно, из активной 
и реактивной составляющих, так что общая формула 
\begin{equation}
Z = \sqrt{ R^2 + (X_L - X_C)^2 }, 
\label{Zseq} 
\end{equation}
где  $R$ --- активное сопротивление, $X_L = \omega\cdot L$ --- индуктивное 
сопротивление, $X_C = 1/\omega C$ --- ёмкостное сопротивление (см., к примеру, 
\cite{YavorskiDetlaf}). Как будет изменяться ток в цепи переменного тока 
с заданными $R$, $L$, $C$, когда круговая частота тока меняется в интервале 
$[\,\omega_1, \omega_2 ]$? Напомним, что круговая частота связана с частотой 
колебаний $f$ соотношением $\omega = 2 \pi f$. 
  
Для ответа на вопрос воспользуемся законом Ома для цепи переменного тока 
\begin{equation*}
I = U/Z, 
\end{equation*} 
в котором нужно найти интервал изменения $Z$. Выражение для полного сопротивления $Z$ 
характерно тем, что индуктивное и емкостное сопротивления зависят от частоты 
противоположным образом. 
  
В качестве середины интервала частот возьмём $f= 13.56$ МГц. Диапазон вокруг этой 
частоты может использоваться во всем мире без лицензий \cite{ISM}, он имеет специальное 
обозначение --- ISM (от фразы <<Industrial, Scientific, Medical>>). 
  
Рассмотрим практическое применение оборудования в диапазоне ISM: генератор 
плазменного источника возбуждения-ионизации пробы для элементного анализа 
с индуктивно-связанной плазмой (ИСП). Индуктор плазмотрона ИСП (диаметр и длина 20 мм, 
три витка) имеет индуктивность порядка $L=100$ нГ и реактивное сопротивление 8.52 Ом 
на частоте ISM. В таком случае ёмкость резонансного контура имеет такое же реактивное 
сопротивление при $C = 1.38$ нФ. 
  
Пусть круговая частота $ \omega$ меняется в интервале $\,\pm 10\%$ относительно 
центра  диапазона ISM, т.\,е. 
\begin{equation}
\label{OmegaInt}
\mbf{\omega} = [\,\omega_1, \omega_2 ] = [7.668, 9.372] \cdot 10^7 \ \text{Гц}.
\end{equation}
Активное сопротивление примем равным 1 Ом. Примерно такую величину имеет, например, 
сопротивление частично ионизированного аргона в плазмотроне. Построим зависимости 
$X_L$, $X_C$ и $Z$ от частоты в интервале $\mbf{\omega}$, задаваемом \eqref{OmegaInt}. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{figure}[ht]
	\centering\small 
	\setlength{\unitlength}{1mm} 
	\begin{picture}(80,57) 
	\put(6,2){\includegraphics[width=75mm]{pictures/ZvsF.png}} 
	\put(40,0){$\omega, 10^{7}$ Гц}
	\put(-12,27){$X_L, X_C, Z,$ Ом}
	\put(25,39){$X_L$}
	\put(60,39){$X_C$}
	\put(43,17){$Z$}
	\put(10,52){$10$}
	\put(11,29){$5$}
	\put(11,8){$0$}
	\put(15,4){$7.67$}
	\put(43,4){$8.52$}
	\put(70,4){$9.37$}
	\end{picture} 
	\caption{Зависимости $X_L, X_C$ и $Z$ от частоты.} 
	\label{Xresonance}  
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Определим границы изменения величины $Z$. В точке резонанса  $X_L = X_C$ и суммарное 
сопротивление $LC$-части схемы равно нулю, а полное сопротивление равно активному, 
т.\,е. $Z=R$. Таким образом, 
\begin{equation*} 
\min_{\omega} Z = R =1. 
\end{equation*} 
  
Величины индуктивного $X_L$ и ёмкостного $X_C$ сопротивлений зависят от частоты 
противоположным образом, но обе --- монотонно. Поэтому максимальные значения 
величины $X_{LC} = | X_L - X_C | $ достигаются на одном из концов интервала 
диапазона $\mbf{\omega}$: 
\begin{equation*} 
\max_{\omega} X_{LC} = \max \left\{ X_{LC} ( \un {\mbf{\omega}}), X_{LC} 
  ( \ov {\mbf{\omega}}) \right\} =  \max \left\{ 1.6401, 1.7822 \right\} = 1.7822. 
\end{equation*} 
Соответственно, 
\begin{equation*} 
\max Z = \sqrt{R^2 + \max_{\omega} X^2_{LC}} =2.0436.
\end{equation*} 
Окончательно получаем точный интервал величины полного сопротивления для круговой 
часты, изменяющейся в интервале $\mbf{\omega}$: 
\begin{equation*} 
\mbf{Z} = [1, 2.0436].
\end{equation*} 
  
Вычислим теперь интервал изменения полного сопротивления $\mbf{Z}$ как естественное 
интервальное расширение формулы \eqref{Zseq}, представленной двумя способами: 
\begin{align}
\mbf{Z}_1 &= \sqrt{R^2 + \left( \mbf{\omega} L - \frac{1}{\mbf{\omega} C} \right) ^2}, 
\label{Z1} \\[3mm] 
\mbf{Z}_2 &= \sqrt{R^2 + \mbf{\omega}^2\left( L - \frac{1}{\mbf{\omega}^2 C} \right)^2}. 
\label{Z2}
\end{align} 
Вычисления дают величины сопротивлений соответственно
\begin{align*}
\mbf{Z}_1 &= [1, 2.0436] \ \text{Ом}, \\[1mm]
\mbf{Z}_2 &= [1, 2.3968] \ \text{Ом}.
\end{align*} 
Нижние концы интервалов $\mbf{Z}_1$, $\mbf{Z}_2$ соответствуют частотному резонансу 
$X_L = X_C$ в середине диапазона, верхние относятся к краям выбранного диапазона, 
иначе говоря --- к расстройке резонанса. 
  
Результат интервального вычисления по выражению \eqref{Z2} получился шире, чем 
при вычислении согласно выражению \eqref{Z1}, так как в \eqref{Z2} присутствует 
дополнительное умножение на величину $\mbf{\omega}^2$. 
\end{example}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section{Интервальная арифметика Кэхэна} 
\label{KahanArithmSect} 
  
  
Интервальная арифметика Кэхэна помимо традиционных интервалов имеет своими элементами 
бесконечные и полубесконечные интервалы вида 
\begin{equation*} 
[-\infty, p], \qquad [q, +\infty], \qquad  [-\infty, p]\cup[q, +\infty]. 
\end{equation*} 
Её идея была высказана У.\,Кэхэном в 1968 году, но развёрнутое изложение этой арифметики 
было дано позже в работе \cite{Laveuve}. Заметим, что мы пишем по традиции бесконечные 
концы как бы принадлежащими интервалу, хотя на самом деле это условность, и 
$\pm\infty$ --- это просто символы.     \index{интервальная арифметика Кэхэна}
  
В контексте метрологии арифметика Кэхэна нужна для того, чтобы описывать односторонние 
ограничения на рассматриваемые величины, к примеру, $x\geq 0$ или $Y_{ij}\leq 123.45$ 
и им подобные. Вместо выписанных неравенств можно указать на интервальном языке 
\begin{equation*} 
x\in[0, +\infty] \qquad\text{ или }\qquad Y_{ij}\in[-\infty, 123.45]. 
\end{equation*} 
  
Для любых обычных интервалов $\mbf{a}, \mbf{b}\in\mbb{IR}$ результаты их сложения, 
вычитания, умножения и деления $\mbf{a}/\mbf{b}$ при $0\notin\mbf{b}$ в классической 
интервальной арифметике и арифметике Кэхэна полностью совпадают. Но в арифметике 
Кэхэна дополнительно определено деление интервалов $\mbf{a}$ и $\mbf{b}$ c $0\in 
\mbf{b}$, которое и приводит к бесконечным интервалам. Дело в том, что при делении 
$\mbf{a}/\mbf{b}$ даже в случае присутствия нуля в интервале-делителе $\mbf{b}$ 
результаты операции $a/b$ для остальных $a\in\mbf{a}$ и $b\in\mbf{b}$ определены. 
На их основе, руководствуясь фундаментальным свойством 
\eqref{IAMainPrinciple} 
\begin{equation*}
\mbf{a}\star\mbf{b} = \{\; a\star b \mid a\in\mbf{a}, \ b\in\mbf{b}\;\}
\quad\mbox{ для } \ \star\in\{\,+\, ,\, -\, ,\, \cdot\, ,\, /\,\},
\end{equation*}
можно придать интервальному делению $\mbf{a}/\mbf{b}$  расширенный смысл. 
  
Будем считать результатом этого деления такое множество (<<обобщённый интервал>>), 
которое образовано результатами всевозможных делений $a/b$ для тех представителей 
$a\in\mbf{a}$ и $b\in \mbf{b}$, для которых это имеет смысл. Иными словами, положим 
\begin{equation*} 
\mbf{a}/\mbf{b}\; := \  
  \bigl\{\,a/b \mid\, a\in\mbf{a},\, b\in\mbf{b}\setminus\{0\}\,\bigr\}. 
\end{equation*}   
Для удобства мы выпишем соответствующие 
результаты в развёрнутой форме: 
\begin{align*}
\mbf{a}/\mbf{b} \  &= \
\frac{[\,\un{\mbf{a}},\,\ov{\mbf{a}}\,]}%
{[\,\un{\mbf{b}},\,\ov{\mbf{b}}\,]} \nonumber\\[7pt] 
& = \ 
\begin{cases}
\quad \mbf{a}\cdot[\,1/\ov{\mbf{b}},1/\un{\mbf{b}}\,],
   & \mbox{если $\; 0\not\in\mbf{b}$},                 \\[3pt]
\quad [-\infty, +\,\infty\,]\,,
   & \mbox{если $\; 0\in\mbf{a}\;$ и $\; 0\in\mbf{b}$},\\[3pt]
\quad [\,\ov{\mbf{a}}/\un{\mbf{b}}, +\,\infty\,]\, ,
   & \mbox{если $\;\ov{\mbf{a}} < 0\;$ и
                  $\;\un{\mbf{b}} < \ov{\mbf{b}} = 0$,}\\[3pt]
\quad [-\infty, \ov{\mbf{a}}/\ov{\mbf{b}}]
    \cup[\,\ov{\mbf{a}}/\un{\mbf{b}}, +\,\infty\,]\, ,
   & \mbox{если $\;\ov{\mbf{a}} < 0\;$ и
                  $\;\un{\mbf{b}} < 0 < \ov{\mbf{b}}$},\\[3pt]
\quad [-\infty, \ov{\mbf{a}}/\ov{\mbf{b}}\,],
   & \mbox{если $\;\ov{\mbf{a}} < 0\;$
               и $\; 0 = \un{\mbf{b}} < \ov{\mbf{b}}$},\\[3pt]
\quad [-\infty, \un{\mbf{a}}/\un{\mbf{b}}\,],
   & \mbox{если $\; 0 < \un{\mbf{a}}\;$ и
                    $\;\un{\mbf{b}} < \ov{\mbf{b}}=0$},\\[3pt]
\quad [-\infty, \un{\mbf{a}}/\un{\mbf{b}}\,]
     \cup[\un{\mbf{a}}/\ov{\mbf{b}}, +\,\infty\,]\, ,
   & \mbox{если $\; 0 < \un{\mbf{a}}\;$ и
                  $\;\un{\mbf{b}} < 0 < \ov{\mbf{b}}$},\\[3pt]
\quad [\,\un{\mbf{a}}/\ov{\mbf{b}}, +\,\infty\,]\, ,
 &\mbox{если $\; 0 < \un{\mbf{a}}\;$ и 
                 $\; 0 = \un{\mbf{b}} < \ov{\mbf{b}}$},\\[3pt]
\quad \varnothing, &\mbox{если $\; 0\not\in\mbf{a}\;$ и $\; 0 = \mbf{b}$}.
\end{cases}
%\label{KahanDivision}
\end{align*}
  
\begin{example} 
Вычислим 
\begin{equation*} 
\frac{[1, 2]}{[-3, 4]} = 
   \bigl[-\infty, -\tfrac{1}{3}\bigr]\cup\bigl[\tfrac{1}{4}, +\infty\bigr]\,, 
\end{equation*} 
что соответствует седьмому случаю в выписанном выше определении деления. 
\end{example} 
  
Основной недостаток интервальной арифметики Кэхэна, применяемой самостоятельно, 
--- это более или менее быстрый, но всегда неизбежный, выход на бессодержательный 
результат $[-\infty, \infty]$, т.\,е. получение всей числовой оси $\mbb{R}$. После 
этого дальнейшие вычисления теряют смысл, так как в результате любой арифметической 
операции с этим интервалом получается он сам и никакого уточнения информации о решении 
не происходит. Тем не менее, в ограниченной мере применять арифметику Кэхэна можно 
и нужно. 
  
Ещё один способ преодолеть появления бессодержательности результатов арифметики 
Кэхэна --- её применение совместно с теоретико-множественными операциями, особенно 
с пересечением, которое требуется в некоторых интервальных алгоритмах. Ясно, 
что пересечение с любым конечным интервалом сразу отсекает бесконечные <<хвосты>>, 
и далее с полученным интервалом (или интервалами --- их может образоваться два 
от интервалов вида $[-\infty, p]\cup[q, +\infty]$) никаких проблем не возникает. 
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\section[Полная интервальная арифметика Каухера]%  
        {Полная интервальная \\* арифметика Каухера} 
\label{KaucherArithmSect} 
  
Помимо классической интервальной арифметики часто возникает необходимость работать 
с полной интервальной арифметикой Каухера, которую обозначают $\mbb{KR}$. Она является 
алгебраическим и порядковым пополнением арифметики $\mbb{IR}$, примерно так же, 
как множество целых чисел пополняет натуральный ряд. 
  
Элементами арифметики $\mbb{KR}$ являются пары чисел, взятые в квадратные скобки,  
вида $[\alpha, \beta]$, которые мы также будем называть \textit{интервалами}. 
При этом возможны ситуации, когда $\alpha\leq\beta$ или $\alpha > \beta$. Если 
$\alpha\leq \beta$, то $[\alpha, \beta]$ обозначает обычный интервал вещественной оси, 
и мы называем его \textit{правильным}. В частности, обычные вещественные числа являются 
вырожденными правильными интервалами. Если же $\alpha >  \beta$, то $[\alpha, \beta]$ 
--- \textit{неправильный интервал}. Таким образом, $\mbb{IR}\subset\mbb{KR}$. 
\index{интервальная арифметика Каухера} \index{правильный интервал}
\index{неправильный интервал} 
  
Неправильные интервалы можно рассматривать как математические абстракции (аналогичные, 
к примеру, отрицательным или мнимым числам), но которым могут быть даны осмысленные 
физические интерпретации. В контексте нашей работы полная интервальная арифметика 
Каухера $\mbb{KR}$ и неправильные интервалы, как мы увидим, по существу возникают 
при математической обработке интервальных результатов наблюдений и измерений. 
  
Правильные и неправильные интервалы, две <<половинки>> $\mbb{KR}$, переходят друг 
в друга в результате отображения \emph{дуализации}\index{дуализация}. Оно обозначается 
символом dual и меняет местами (переворачивает) концы интервала, т.\,е. 
\begin{equation*}
\label{Dualization}
\dual\mbf{a}\, := \,[\;\ov{\mbf{a}},\,\un{\mbf{a}}\;].
\end{equation*} 
\textit{Правильной проекцией} интервала $\mbf{a}$ из $\mbb{KR}$ называется интервал, 
обозначаемый $\pro\mbf{a}$ и такой, что 
\begin{equation*} 
\pro\mbf{a} = 
\left\{ \ 
\begin{array}{cl}
\mbf{a}, & \text{если $\mbf{a}$ --- правильный,} \\[1mm] 
\dual\mbf{a}, & \text{если $\mbf{a}$ --- неправильный.} 
\end{array} 
\right. 
\end{equation*} 
С помощью правильной проекции из произвольного интервала получается его <<правильный 
образ>>, с которым можно обращаться как с обычным числовым интервалом в $\mbb{R}$, 
т.\,е. множеством всех точек вещественной оси между двумя заданными концами. 
  
Арифметические операции между интервалами в $\mbb{KR}$ продолжают операции в $\mbb{IR}$. 
В частности, формулы \eqref{Addition}--\eqref{Subtraction} для сложения и вычитания 
также определяют сложение и вычитание в $\mbb{KR}$. Умножение и деление между 
интервалами из $\mbb{KR}$ определяется более сложно, и их подробное описание можно 
найти в \cite{SSharyBook}. Но умножение интервала из $\mbb{KR}$ на число определяется 
совершенно так же, как и для обычных правильных интервалов, т.\,е. согласно правилу 
\eqref{NumIntProduct}: 
\begin{equation*}
a\cdot\mbf{b} \ = \ 
\left\{ 
\begin{array}{ll}
\bigl[\,a\un{\mbf{b}}, a\ov{\mbf{b}}\,\bigr], & \text{если} \  a\geq 0, \\[2mm] 
\bigl[\,a\ov{\mbf{b}}, a\un{\mbf{b}}\,\bigr], & \text{если} \  a\leq 0. 
\end{array} 
\right. 
\end{equation*}
  
Наиболее важным для нас в интервальной арифметике Каухера является обратимость 
арифметических операций. В частности, для любого интервала имеется противоположный ему, 
т.\,е. обратный по сложению. Для интервалов, не содержащих нуля, имеются обратные к ним 
по умножению. Для сложения \eqref{Addition} обратной операцией является не операция 
интервального вычитания \eqref{Subtraction}, а другая операция, которую мы будем 
называть <<алгебраическим вычитанием>> и обозначать знаком <<$\ominus$>>: 
\begin{equation}
\label{AlgebrMinus} 
\mbf{a}\ominus\mbf{b} 
  = [\un{\mbf{a}} - \un{\mbf{b}}, \ov{\mbf{a}} - \ov{\mbf{b}}].
\end{equation} 
Иногда математических текстах тем же символом обозначается так называемая <<разность 
Хукухары>> двух множеств (Hukuhara difference), которая по своему смыслу отчасти 
похожа на алгебраическое вычитание интервалов, но всё-таки имеет другое назначение. 
Нетрудно проверить, что для любых интервалов $\mbf{a}$, $\mbf{b}$ из $\mbb{KR}$ 
справедливы равенства 
\begin{equation*} 
\mbf{a}\ominus\mbf{a} = 0,  \hspace{15mm} 
(\mbf{a} + \mbf{b})\ominus\mbf{b} = \mbf{a}, \hspace{15mm} 
(\mbf{a}\ominus\mbf{b}) + \mbf{b} = \mbf{a}. 
\end{equation*} 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\begin{example} 
Для интервала $[1, 2]$ противоположным по сложению является интервал $[-1, -2]$. 
Это неправильный интервал, но 
\begin{equation*} 
[1, 2] + [-1, -2] = [1-1, 2-2] = [0, 0] = 0,   
\end{equation*} 
т.\,е. в сумме с исходным интервалом он даёт нейтральный элемент $0$. Заметим, 
что для обычного интервального вычитания 
\begin{equation*} 
[1, 2] - [1, 2] = [1-2, 2-1] = [-1, 1]. 
\end{equation*} 
Это иллюстрирует тот отмеченный выше факт, что обычное интервальное вычитание 
не является операцией, обратной к интервальному сложению. 
\end{example} 
   
\emph{Абсолютное значение} интервалов из $\mbb{KR}$ определяется, как абсолютное 
значение их правильных проекций, т.\,е. 
\begin{equation*} 
|\mbf{a}|\  = \;\max\,\{\,|\un{\mbf{a}}|, |\ov{\mbf{a}}|\,\}.  
\end{equation*} 
   
Полная интервальная арифметика Каухера $\mbb{KR}$ пополняет классическую интервальную 
арифметику $\mbb{IR}$ не только в алгебраическом смысле, но также и относительно 
естественного порядка по включению <<$\subseteq$>>. 
  
\begin{definition} 
\label{IncluDefi}  
Будем говорить, что для интервалов $\mbf{a}$, $\mbf{b}\in\mbb{KR}$ выполняется 
включение $\mbf{a}\subseteq\mbf{b}$, если  
\begin{equation*} 
\un{\mbf{a}}\geq\un{\mbf{b}}\quad\text{ и }\quad\ov{\mbf{a}}\leq\ov{\mbf{b}}, 
\end{equation*} 
т.\,е. справедливы те же соотношения \eqref{InclInteOrder} между концами интервалов, 
что и в случае классической интервальной арифметики. 
\end{definition} 
  
Относительно введённого таким образом отношения включения в $\mbb{KR}$ для любых 
двух интервалов существует минимальный и максимальный по включению интервалы, т.\,е. 
результаты операций $\mbf{a}\wedge\mbf{b}$ и $\mbf{a}\vee\mbf{b}$  всегда определены. 
  
\begin{example} 
\begin{equation*} 
[1, 2]\wedge [3, 4] = [3, 2], \hspace{18mm} [1, 2]\vee [3, 4] = [1, 4]. 
\vspace{-4mm} 
\end{equation*} 
\end{example} 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
   
\paragraph{Расстояние на множестве интервалов.}  
Расстояние между интервалами $\mbf{a}$ и $\mbf{b}$ из $\mbb{IR}$ или $\mbb{KR}$ 
определяется как                      \index{расстояние}\index{метрика} 
\begin{equation}
\label{InteDist}
\dist(\mbf{a}, \mbf{b}) \  = \  \max 
  \bigl\{|\un{\mbf{a}} - \un{\mbf{b}}|, 
         |\ov{\mbf{a}} - \ov{\mbf{b}}| \bigr\}.
\end{equation}
Оно обладает всеми свойствами абстрактного расстояния (метрики) и ещё некоторыми 
хорошими свойствами в связи с интервальными арифметическими операциями. Кроме того, 
легко убедиться, что 
\begin{equation*}
\dist(\mbf{a}, \mbf{b}) \  = \  |\mbf{a}\ominus\mbf{b}|.
\end{equation*}
Эта формула является полным аналогом расстояния между точками вещественной оси, как 
модуля их разности, т.\,е. $|a - b|$. 
  
Нетрудно показать, что справедливо также следующее равносильное представление 
расстояния \eqref{InteDist} между интервалами: 
\begin{equation*}
\dist(\mbf{a}, \mbf{b}) \  = \  |\m\mbf{a} - \m\mbf{b}| + |\r\mbf{a} - \r\mbf{b}|.
\end{equation*}
  
\begin{example}
Рассмотрим интервал $[3, 5]$ и точку $3.6$ внутри него. Расстояние от этой точки,  
отождествляемой с вырожденным интервалом $[3.6, 3.6]$, до данного интервала равно 
\begin{equation*} 
\dist(3.6, [3, 5]) = \max\bigl\{|3.6 - 3|, |3.6 - 5|\bigr\} = 1.4. 
\end{equation*} 
  
Рассмотрим дуальный интервал к интервалу $[3, 5]$. Это интервал $\dual[3, 5] = [5, 3]$.  
Расстояние его до исходного интервала равно $\dist([3, 5], [5, 3]) = 2$. 
\end{example}
  
Расстояние важно для определения отклонения интервалов друг от друга и, как следствие, 
для определения погрешности интервальных измерений. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section{Твины --- интервалы интервалов} 

На практике концы интервалов, представляющие результаты измерений, сами могут быть 
известны неточно, так что возникает необходимость работы с интервалами, имеющими  
интервальные концы. Такие объекты известны в интервальном анализе и называются 
\textit{твинами} (по английски twin, как сокращение фразы \un{tw}ice \un{in}terval, 
<<двойной интервал>>).\index{твин} Они были введены в научный оборот в начале 
80-х годов XX века в работах испанских исследователей, и заинтересованный читатель 
может найти подробности в книге \cite{ModalIABook}. Отдельные аспекты применения 
твинов рассматриваются в статье \cite{Twins1981}. Краткое изложении основ теории 
твинов дано в статье \cite{Nesterov1997}, а развёрнутое --- в диссертации 
\cite{Nesterov1999}. 
Из современных публикаций можно отметить статью \cite{ThickSet}, в которой рассматривается обращение твинов.

  
Tвин, как <<интервал интервалов>>  или интервал с интервальными концами, можно 
представить как 
\begin{equation} 
\label{Twin}
\mbf{X} = 
[\mbf{a}, \mbf{b}] = \bigl[\,[\un{\mbf{a}}, \ov{\mbf{a}}], [\un{\mbf{b}}, \ov{\mbf{b}}]\,\bigr].
\end{equation}

\begin{figure}[hbt]
	\centering\small 
	\setlength{\unitlength}{1mm}
	%	\begin{picture}(70,17)
	\begin{picture}(70,13)
	\put(0,0){\includegraphics[width=70mm]{pictures/Twinfig.eps}}
	\put(-5,6.6){\vector(1,0){80}} \put(71.5,7.6){$\mbb{R}$} 
	%	\put(21,10){$\un{\mbf{a}}$} \put(30,10){$\ov{\mbf{a}}$} 
	\put(20,1){$\un{\mbf{a}}$} \put(30, 1){$\ov{\mbf{a}}$} 
	%	\put(41,10){$\un{\mbf{b}}$} \put(50,10){$\ov{\mbf{b}}$} 
	\put(41,11){$\un{\mbf{b}}$} \put(51,11){$\ov{\mbf{b}}$} 
	\put(35,-2){$\mbf{X}$} 
	\end{picture}
	\caption{Твины на вещественной оси.} 
	\label{TwinsPic2} 
\end{figure}
На рисунке \ref{TwinsPic2} твин $\mbf{X}$ представлен в графической форме. Концы твина, 
т.\,е. интервалы $\mbf{a}$ и $\mbf{b}$, даны более тёмной заливкой, чем остальная часть 
твина. 
  
Твин является множеством всех интервалов, больших или равных $[\un{a}, \ov{a}]$ и меньших
или равных $[\un{b}, \ov{b}]$, и точное определение зависит от смысла, который вкладывается 
в понятия <<больше или равно>>, <<меньше или равно>>. Поскольку интервалы могут быть 
упорядочены различными способами, то существуют различные виды твинов. Двум основным 
частичным порядкам на $\mathbb{IR}$ и $\mathbb{KR}$, <<$\subseteq$ >> и <<$\leq$>>,  
соответствуют два основных типа твинов. Разработаны различные операции с твинами, 
а также способы оценок значений функций от них. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\paragraph{Представление твинов на плоскости $( \inf, \sup )$.}
\index{твины, графическое представление}
  
%Приведём несколько иллюстративных примеров операций с твинами, следуя \cite{ModalIABook}.
Рассмотрим представление твинов на плоскости $( \inf, \sup )$.
В первую очередь, представим графически интервалы на плоскости $( \inf, \sup )$. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[ht] 
	\centering\small  
	\setlength{\unitlength}{1mm}
	\begin{picture}(60,40)
	\put(10,0){\includegraphics[width=60mm]{pictures/InteInfSup.eps}}
	\put(17,28){\circle*{1.5}}%{\oval(1.5,1.5)}
	\put(17,28){\line(1,0){8}}
	\put(17,28){\line(0,-1){24}}
	\put(13,28){$\mbf{A}$} 	
	\put(29,31){правильные} 
	\put(30,27.5){интервалы}	
	\put(37,16){\circle*{1.5}}%{\oval(1.5,1.5)}		
	\put(33,17){$\mbf{C}$} 
	\put(43,10){\circle*{1.5}}%{\oval(1.5,1.5)}		
	\put(44,7){$\mbf{B}$} 
	\put(47,21){неправильные} 
	\put(48,17.5){интервалы}
	\put(63,6){$\inf$} 
	\put(18,34){$\sup$}
\end{picture}
\caption{Представление интервалов на плоскости $(\inf, \sup)$ \cite{ModalIABook}.}
\label{IntervalR2}  
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
Прямая $\inf \mbf{x} = \sup \mbf{x}$, проходящая через начало координат, делит 
интервальную плоскость $( \inf, \sup )$ на 2 части. Левая верхняя часть плоскости 
соответствует правильным интервалам, правая нижняя --- неправильным. На прямой лежат 
точечные интервалы, обычные вещественные числа. На рисунке \ref{IntervalR2} точка 
$\mbf{A}$ представляет правильный интервал, точка $\mbf{B}$ --- неправильный, 
а $\mbf{C}$ --- точечный интервал. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  

\begin{figure}[htb] 
	\centering\small  
	\begin{picture}(60,110)
	\put(-50,0){\includegraphics[width=60mm]{pictures/TwinGraph.eps}}
	\put(-6,92.5){\circle*{3}}%{\oval(1.5,1.5)}
	\put(16.5,76){\circle*{3}}%{\oval(1.5,1.5)}
	\put(-1,98){$\mbf{A}_{\subseteq}$}	
	\put(8,66){$\un{\mbf{A}}$} 
	\put(-17,93){$\ov{\mbf{A}}$} 	
	\put(55,20){$\inf$} 
	\put(-50,107){$\sup$}
	\end{picture}
	\hspace{5mm}			
	\begin{picture}(60,110)
	\put(0,0){\includegraphics[width=60mm]{pictures/TwinGraph.eps}}
	\put(66.5,92.5){\circle*{3}}%{\oval(1.5,1.5)}
	\put(44,76){\circle*{3}}%{\oval(1.5,1.5)}	
	\put(47,98){$\mbf{A}_{\leq}$} 	
	\put(33,66){$\un{\mbf{A}}$} 
	\put(70,93){$\ov{\mbf{A}}$} 
	\put(105,20){$\inf$} 
	\put(-1,107){$\sup$}
\end{picture}
\caption{Представление $\subseteq$-твинов (слева) и $\leq$-твинов (справа)} 
на интервальной плоскости в координатах $(\inf, \sup)$.\label{TwinR2a} 
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
Рассмотрим представление двух типов твинов, соответствующих двум основным частичным 
порядкам на $\mathbb{IR}$ и $\mathbb{KR}$, <<$\subseteq$ >> и <<$\leq$>>. Графически 
они представлены на Рис.~\ref{TwinR2a}. 
  
В качестве примеров твинов разных типов для Рис.~\ref{TwinR2a} можно привести численные 
значения: 
\begin{equation*}
\mbf{A}_{\subseteq} = \bigl[\,[2,3], [1,4 ]\,\bigr], 
\hspace{18mm} 
\mbf{A}_{\leq} = \bigl[\,[1,2], [3,4 ]\,\bigr].
\end{equation*}

Рассмотрим несколько примеров примения твинов к практическим задачам. Везде будут 
использоваться $\leq$-твины. Операции с $\subseteq$-твинами можно применять для 
одновременного определения внешних и внутренних оценок функциональных выражений. 
При этом в большом количестве случаев возникают ситуации с пустым множеством 
внутренней оценки или неправильным интервалам. Это требует весьма непростого анализа. 
Развёрнутое обсуждение круга проблем, связанных с вырождением внутренних оценок, дано 
в  диссертации \cite{Nesterov1999}. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{example}
Рассмотрим измерение так называемых осцилляций нейтрино, результаты измерений которых 
удобно представить в виде твинов. 
	
При измерении осцилляций нейтрино в атмосфере Земли экспериментаторы традиционно 
используют безразмерную величину $R$, характеризующую отношение числа разных сортов 
нейтрино. Подборка значений $R$ из разных экспериментов приведена на стр.~872 
в публикации \cite{UFN1997}. Приведем часть данных из этой публикации: 
\begin{align}
R_1 &=  0.60^{+0.07}_{-0.06} \pm 0.05, && \text{<<Kamiokande>>}, \label{Kamiokande}\\[4pt]  
R_2 &=  0.54 \pm 0.05 \pm 0.12, &&   \text{<<IMB>>}. \label{IMB}
\end{align} 
Результаты измерений даны  в форме <<базовое значение, статистическая погрешность, 
систематическая погрешность>> а после числовых данных приводится название проекта, 
в котором они были получены.                                
В первом примере (<<Kamiokande>>) статистическая погрешность дана в виде границ, 
несимметричных относительно среднего значения. Такая ситуация возникает при оценке 
значения величины, входящей в нелинейную функцию. Она соответствует общему случаю 
модели погрешности \eqref{GeneralErrorModel}, рассмотренной выше, 
в параграфе \ref{MeasuResultSect}. 
	
В виде твинов данные \eqref{Kamiokande} и \eqref{IMB} можно представить следующим 
образом. В качеcтве первого шага выразим результаты в виде обычных интервалов $\mbf{r}_1$ 
и $\mbf{r}_2$, с учётом только статистических погрешностей: 
\begin{align}
\mbf{r}_1  &= [\; 0.6-0.06, \  0.6+0.07 \;] \  = \  [\; 0.54, 0.67 \;], \label{r1} \\[3pt]  
\mbf{r}_2  &= [\; 0.54-0.05,  0.54+0.05 \;] = [\; 0.49, 0.59 \;]. \label{r2} 
\end{align}	
При этом $\w{\mbf{r}_1} = 0.13 > \w{\mbf{r}_2} = 0.1$ ввиду более широких статистических 
оценок для величины $R_1$.	
  
Далее, произведём учёт систематической погрешности, произведя <<интервализацию>> концов 
интервалов $\mbf{r}_1$ и  $\mbf{r}_2,$ вычитая и добавляя величины систематических 
погрешностей к величинам $\un{\mbf{r}}_1$, $\ov{\mbf{r}}_1$, $\un{\mbf{r}}_2$, 
$\ov{\mbf{r}}_2$. 
 
Обозначим получившиеся твины как $\mbf{R}_1$ и $\mbf{R}_2$:  
\begin{align}
\mbf{R}_1  
&= \bigl[\;[ 0.54-0.05,  0.54+0.05], \  [0.67-0.05,  0.67+0.05]\;\bigr] \notag\\[3pt] 
&= \bigl[\;[ 0.49,  0.59],  \ [0.62,  0.72 ]\;\bigr], \label{R1}     \\[3mm] 
\mbf{R}_2  
&= \bigl[\;[ 0.49-0.12,  0.49+0.12], \  [0.59-0.12,  0.59+0.12]\;\bigr] \notag\\[3pt] 
&= \bigl[\;[ 0.37,  0.61], \  [0.47,  0.71 ]\;\bigr]. \label{R2} 
\end{align}

На рисунке \ref{TwinsRnu2} графически представлены твины $\mbf{R}_1$ и $\mbf{R}_2$. 
Численные значения концов правого интервала смещены вверх. На сей раз твин $\mbf{R}_1$  
<<\'{у}же>>, чем  твин $\mbf{R}_2$, ввиду более широких систематических погрешностей 
для величины $R_2$. Следует заметить также, интервалы  $\un{\mbf{R}}_2$, $\ov{\mbf{R}}_2$  
в форме \eqref{Twin} имеют ненулевое пересечение. Это пересечение дано более тёмной 
заливкой, чем концы твина. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{figure}[hbt]
	\centering\small 
	\setlength{\unitlength}{1mm}
	\begin{picture}(70,13)
	\put(0,0){\includegraphics[width=70mm]{pictures/TwinRnu.eps}}
	\put(-5,6.6){\vector(1,0){80}} \put(71.5,7.6){$\mbb{R}$} 
			\put(19,2){{\footnotesize 0.49}} 
			\put(32.5,2){{\footnotesize 0.59}} 
			\put(37,10){{\footnotesize 0.62}} 
			\put(50,10){{\footnotesize 0.72}}  
	\put(35,-2){$\mbf{R}_1$}  
	\end{picture}
	%\caption{Данные по массе нейтрино как твин.} 
	%	\label{TwinsRnu} 
\end{figure}
%\vspace{-10mm}
\begin{figure}[hbt]
	\centering\small 
	\setlength{\unitlength}{1mm}
	\begin{picture}(70,13)
	\put(0,0){\includegraphics[width=70mm]{pictures/TwinR2nu.eps}}
	\put(-5,6.6){\vector(1,0){80}} \put(71.5,7.6){$\mbb{R}$} 
			\put(2,2){{\footnotesize 0.37}} 
			\put(16,10){{\footnotesize 0.47}} 
			\put(35.5,2){{\footnotesize 0.61}} 
			\put(48.5,10){{\footnotesize 0.71}} 
	\put(30,-1){$\mbf{R}_2$}  
	\end{picture}
	\caption{Данные по физике нейтрино в форме твинов типа <<$\leq$>>.} 		
	\label{TwinsRnu2} 
\end{figure} 
	 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
Далее можно проводить различный содержательный анализ величин $\mbf{R}_1$ и $\mbf{R}_2$, 
используя аппарат, представленный в \cite{ModalIABook}. 
\end{example}  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{example} 
Рассмотрим измерение температуры с помощью термометра сопротивления. 
	
В повседневной лабораторной и промышленной практике широко применяются 	термометры 
сопротивления. Один из типов таких датчиков, платиновый термометр Pt100, имеет 
номинальное сопротивление 100 Ом при температуре $0^{\circ}C$ и систематическую 
погрешность 
\begin{equation*} 
\Delta t =\pm 0.35\;^{\circ} C. 
\end{equation*} 
% Приближённая формула R(T) = 100 + 0.39083×T - 5.775×10-5×T2 
Пусть измеряемая  температура находится в диапазоне [19.5, 20.5] $^{\circ} C$, которую 
представим как интервал  $\mbf{t}$: 
\begin{equation}\label{temp}
\mbf{t}= [19.5, 20.5] \ ^{\circ} C.
\end{equation}
Аналогично рассмотренному выше примеру, представим границы $\un{\mbf{t}}, \ \ov{\mbf{t}}$ 
интервала $\mbf{t}$ как интервалы. С учётом систематической погрешности твин температур 
$\mbf{T}$, даваемый датчиком, составит 
\begin{equation}
\label{TwinTemp}
\mbf{T} = \bigl[\,[19.15, 19.85], \ [20.15, 20.85] \,\bigr] \ ^{\circ} C.
\end{equation}

Графическое представление твина $\mbf{T}$ \eqref{TwinTemp} дано на рисунке \ref{TwinsTemp}. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
	
\begin{figure}[ht]
	\centering\small 
	\setlength{\unitlength}{1mm}
	\begin{picture}(70,13)
	\put(0,0){\includegraphics[width=70mm]{pictures/TwinTemp.eps}}
	\put(-5,6.6){\vector(1,0){80}} \put(71.5,7.6){$\mbb{R}$} 
				\put(18,2){{\footnotesize 19.15}} 
				\put(30,2){{\footnotesize 19.85}} 
				\put(36.5,10){{\footnotesize 20.15}} 
				\put(49,10){{\footnotesize 20.85}} 	
	\put(35,-2){$\mbf{T}$}  
	\end{picture}
	\caption{Результат измерения температуры в виде твина.} 
	\label{TwinsTemp} 
\end{figure}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  	
	
Форма записи температуры в виде твина $\mbf{T}$ \eqref{TwinTemp} выразительно и полно 
представляет информацию об измеряемых данных. В случае, если концы интервала в выражении 
\eqref{temp} могут меняться независимо друг от друга, возможны различные ситуации. 
В частности, может реализоваться ситуация, подобная рассмотренной выше для твина 
$\mbf{R}_2$. Также может оказаться, что значения температур для левого конца будут 
выше, чем для правого. 
\end{example}
 
\paragraph{Примеры арифметических операций с твинами.} % \textcolor{red}{(неокончательный вариант)}.} 
Как уже сказано выше, арифметические операции с твинами требуют весьма изощрённого 
аппарата. В связи с этим, расмотрим наиболее простой пример. %
Сложение двух твинов $\mbf{A}$ и $\mbf{B}$ можно определить <<по представителям>> \cite{ModalIABook}, как сложение их левых и правых интервалов.
\begin{align*} %\label{TwinSum}
& \mbf{A} + \mbf{B} = 
\bigl[ \; \un{\mbf{A}} + \un{\mbf{B}} , \  \ov{\mbf{A}} + \ov{\mbf{B}} \; \bigr].
\end{align*}
Здесь $\un{\mbf{A}}, \ov{\mbf{A}}, \un{\mbf{B}}, \ov{\mbf{B}}$ ---  левые и правые 
интервалы твинов $\mbf{A}$ и $\mbf{B}$. Также, <<по представителям>>, будем трактовать 
умножение на положительную вещественную величину. 
 
Применительно к случаю рассмотренных выше твинов величин $\mbf{R}_1$ \eqref{R1} и 
$\mbf{R}_2$ \eqref{R2}, вычислим их среднее как полусумму 
\begin{equation*}
\mbf{R} = \tfrac{1}{2}( \mbf{R}_1 + \mbf{R}_2 ) = 
\bigl[ \;[0.43, 0.60], \  [0.545, 0.715]\; \bigr].
\end{equation*}

Представим этот результат графически на вещественной оси --- см. Рис.~\ref{TwinsRnuSum}. 
\begin{figure}[htb]
	\centering\small 
	\setlength{\unitlength}{1mm}
	\begin{picture}(70,16)
	\put(0,3){\includegraphics[width=70mm]{pictures/TwinRnuSum.eps}}
	\put(-5,9.6){\vector(1,0){80}} \put(71.5,10.6){$\mbb{R}$} 
	\put(15,5){{\footnotesize 0.43}} 
	\put(26,13){{\footnotesize 0.55}} 
	\put(36,5){{\footnotesize 0.60}} 
	\put(51,13){{\footnotesize 0.72}} 
	\put(24,0){$\frac{1}{2}( \mbf{R}_1 + \mbf{R}_2 )$}  
	\end{picture}
	\caption{Среднее значение данных\\* в физике нейтрино в форме твинов.} 
	\label{TwinsRnuSum} 
\end{figure} 

В качестве другого примера можно привести пересчет температуры из шкалы Цельсия $t_C$  
в шкалу Фаренгейта $t_F$. Пересчет осуществляется по формуле 
\begin{equation*}
t_F = \tfrac{9}{5} \cdot t_C +32.
\end{equation*} 
Конкретно для примера температуры в виде твина для шкалы Цельсия \eqref{TwinTemp} получаем 
твин в шкале Фаренгейта 
\begin{equation*}
\mbf{T}_F = \tfrac{9}{5} \cdot \mbf{T}_C + 32  = 
\bigl[ \;[66.47, 67.73], \  [68.27, 69.53]\; \bigr].
\end{equation*}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   

Более сложные примеры операций с твинами, например, взятие минимума и максимума 
по включению, приводятся в главе 7 книги \cite{ModalIABook}. Там же обсуждается 
вычисление функций от твинов. 
  
Введение твинов, как нового типа данных, позволяет более гибко действовать 
на практике, учитывая возможную неточность в назначении концов интервалов возможных 
значений интересующих нас величин. Вместе с тем, при использовании твинов интерпретация 
результатов как обогащается, так  и осложняется, и требует от пользователя математической 
подготовки и более сложного математического аппарата, который в некоторых аспектах 
ещё не проработан. 
% требует осмысления и продуманного представления.
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
    
\section{Мультиинтервалы} 
\label{MultiIntervalSect} 

В ряде разделов науки и техники встречаются ситуации, когда исследуемая величина  
содержится в неодносвязной области. 

Согласно определению, приведённому в книге \cite{SSharyBook}, \emph{мультиинтервал} 
--- это объединение конечного числа несвязных интервалов числовой оси 
(Рис. \ref{MultiInterval}). \index{мультиинтервал}

\begin{figure}[ht]
	\centering
	\begin{picture}(70,30)
		\put(-90,-10){\includegraphics[width=0.7\textwidth]{pictures/Multifig.eps}}
		\put(-100,12){\vector(1,0){260}} \put(155,15){$\mbb{R}$} 
	\end{picture}	
	\caption{Мультиинтервал в $\mathbb{R}$.} %Рис. 1.11 из \cite{SharyBook}.}
	\label{MultiInterval} 
\end{figure}
 
 Между мультиинтервалами также могут быть определены арифметические операции 
 <<по представителям>>, аналогично тому, как это делается на множестве интервалов 
 \cite{Iakovlev1968}. 
   
В конце прошлого века было создано несколько программных реализаций мультиинтервальной 
арифметики. В частности, в отечественной программе-решателе  \texttt{UniCalc}, 
разработанной  в Российском НИИ искусственного интеллекта \cite{UniCalc}, присутствовала 
поддержка  мультинтервалов. Использованию мультиинтервалов в {\tt UniCalc} посвящен ряд 
публикаций,  например, \cite{JCT97}. Другая реализация мультинтервальных  арифметик того 
времени описана в публикации \cite{InCpp}. 
  
\paragraph{Пример мультиинтервалов и их преобразований.} 	
Приведём модельный пример, в котором появляются неодносвязные интервалы. 
  
\begin{example}
Для различных значений параметра $a$ рассмотрим задачу нахождения формального решения 
уравнения 
\begin{equation}
\label{ModuleMulti} 
	a \cdot |\mbf{x}| = [1.0, 1.5], 
\end{equation}
т.\,е. такого интервала $\mbf{x}$, что его подстановка в \eqref{ModuleMulti} 
и выполнение всех операций приводят к истинному равенству (см.~\S\ref{FormalSols},  
стр.~\pageref{FormalSols}). 
  	
Возьмём для определённости значения $a=0.5$ и $a=1.0$. Получаем соответствующие решения 
уравнения \eqref{ModuleMulti}: 
\begin{align}
	a=0.5: & \quad \mbf{X}_1= [-3.0,  -2.0] \cup [ 2.0, 3.0], \\[2mm] 	
	a=1.0: & \quad \mbf{X}_2= [-1.5, -1.0] \cup [ 1.0, 1.5].
\end{align} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[h!] 
\centering\small  
\setlength{\unitlength}{1mm}
\begin{picture}(100, 26)
	\put(10,0){\includegraphics[width=100mm]{pictures/InteMultiModule.eps}}
%			\put(8,23){{\color{blue} $a = 0.5$}} 
%			\put(25,23){{\color{red} $a = 1.0$}}
	\put(87,23){{\color{blue} $a = 0.5$}} 
	\put(70,23){{\color{red} $a = 1.0$}}
	\put(5,12){$y = 1.0$} 
	\put(5,17){$y = 1.5$} 
	\put(91,3.7){$x$} 
	\put(49.7,25){$y$}
\end{picture} 
\caption{Решение\, уравнения\, \eqref{ModuleMulti}}
\,с разными значениями параметра $a$.
\label{f:MultiIntervalModule}  
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
  
Мультиинтервалы $\mbf{X}_1, \mbf{X}_2$ показаны на Рис.~\ref{f:MultiIntervalModule} 
соответственно парами отрезков синего и красного цвета. При изменении коэффициента 
$a$ компоненты мультиинтервалов --- решений уравнения 
\eqref{ModuleMulti} меняют размер и положение на вещественной оси. 
\end{example}
  
\paragraph{Современное состояние теории мультиинтервалов.}
Относительно недавно появились новые публикации по теории мультиинтервалов. 
  
В частности, авторы статей \cite{IntervalUnions2017N}, \cite{IntervalUnions2017} 
используют для мультиинтервалов малоудачный термин \emph{interval unions}. В этих 
работах приводится система понятий, относящихся к мультиинтервалам, обсуждаются 
арифметика мультиинтервалов, даётся расширение интервальных методов Ньютона и 
Гаусса-Зейделя на мультиинтервалы. Рассмотрены различные способы предобуславливания, 
приводятся результаты ряда численных экспериментов со случайно поставленными 
условиями. Эти результаты также приведены в книге \cite{Domes2020}. 
  
Термин <<мультиинтервал>> явно лучше отражает математическую суть объекта и более 
удобен лингвистически, чем <<interval union>>, так что необходимости его изменения нет. 
  
Мультиинтервальная арифметика применяется редко ввиду серъёзных ограничений, которые 
возникают при алгебраических операциях с мультиинтервальными величинами и вычислительных 
сложностей. Тем не менее, сама по себе идея мультиинтервалов содержательна и полностью 
отметать её не стоит. Ряд научных и технических примеров  возникновения мультиинтервалов 
приводится в пособии  \cite{IntApplication2021}. 
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\section{Интервальные векторы и матрицы} 
\label{IVectMatrSect}
  
  
Интервальный вектор --- это упорядоченный набор из интервалов или, как ещё говорят, 
кртеж или $n$-ка интервалов. Этот набор может быть расположен горизонтально, и тогда 
это вектор-строка, а может быть расположен и вертикально, и тогда это вектор-столбец: 
\begin{equation*} 
\bigl(\mbf{a}_{1}, \mbf{a}_{2}, \cdots, \mbf{a}_{n}\bigr) 
\hspace{12mm} \text{ и } \hspace{12mm} 
\begin{pmatrix}
\mbf{a}_{1} \\  \mbf{a}_{2} \\  \vdots \\  \mbf{a}_{n} 
\end{pmatrix}.
\end{equation*} 
Фактически, мы придерживаемся того определения <<векторов>>, которое характерно 
для информатики и программирования. С другой стороны, интервальный вектор --- это 
прямое декартово произведение интервалов. Множество интервальных $n$-векторов, 
компоненты которых принадлежат $\mbb{IR}$, обозначаем через $\mbb{IR}^n$. 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{figure}[htb]
\centering\unitlength=1mm 
\begin{picture}(60,50)
    \put(0,0){\includegraphics[width=60mm]{pictures/IntervalBoxes.eps}} 
\end{picture} 
\caption{Интервальные векторы-брусы в $\mbb{R}^2$.} 
\label{InteBoxesPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   
Интервальные векторы называют также \textit{брусами}, поскольку их геометрическим 
образом являются прямоугольные параллелепипеды с гранями, параллельными координатным 
осям в $\mbb{R}^n$, см. Рис.~\ref{InteBoxesPic} и Рис.~\ref{Box3DPic}.\index{брус} 
Поскольку интервальные векторы-брусы --- подмножества в $\mbb{R}^n$,  для них 
определены обычные операции и отношения из теории множеств\index{интервальный вектор}  
--- принадлежность, включение, пересечение и объединение и т.\,п. Из-за специальной 
структуры интервальных векторов эти операции выполняются особенно просто, и это служит 
основой удобной техники преобразований и рассуждений с ними. Отметим также полезный 
термин \emph{подбрус}, которым называют любой брус, содержащийся в данном брусе. 
\index{подбрус} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{figure}[htb]
\centering\unitlength=1mm 
\begin{picture}(70,44)
    \put(0,1){\includegraphics[width=70mm]{pictures/Box3Dim.eps}} 
\end{picture} 
\caption{Интервальный вектор-брус в $\mbb{R}^3$.} 
\label{Box3DPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Интервальная матрица --- это матрица с интервальными элементами, т.\,е. прямоугольная 
таблица, заполненная интервалами.\index{интервальная матрица} Например, 
\begin{equation*} 
\mbf{A} \  
= \  
\left( 
\begin{array}{cccc} 
\mbf{a}_{11} & \mbf{a}_{12} & \cdots & \mbf{a}_{1n} \\[1pt] 
\mbf{a}_{21} & \mbf{a}_{22} & \cdots & \mbf{a}_{2n} \\ 
   \vdots    &    \vdots    & \ddots &  \vdots      \\
\mbf{a}_{m1} & \mbf{a}_{m2} & \cdots & \mbf{a}_{mn}  
\end{array} 
\right) 
\end{equation*} 
--- это интервальная $m\times n$-матрица. Множество всех таких интервальных матриц 
с элементами из $\mbb{IR}$ обозначается $\mbb{IR}^{m\times n}$. 
  
\begin{definition}
\label{Vertices}
\textsl{Вершинами} интервального вектора $\mbf{a}$ из $\mbb{IR}^n$ или $\mbb{KR}^n$ 
будем называть точечные $n$-векторы, $i$-ая компонента которых равна $\un{\mbf{a}}_i$ 
или $\ov{\mbf{a}}_i$. Множество вершин интервального вектора обозначаем как 
\begin{equation*}
\vert\mbf{a} \, := \, \bigl\{\,a\in\mbb{R}^n \mid
a_i\in\{\,\un{\mbf{a}}_i,\ov{\mbf{a}}_i\},\ i=1,2,\ldots,n\,\bigr\}.
\end{equation*}
\textsl{Вершинами} интервальной матрицы $\mbf{A} = (\,\mbf{a}_{ij})$ 
из $\mbb{IR}^{m\times n}$ или $\mbb{KR}^{m\times n}$ назовём точечные 
$m\times n$-матрицы, $ij$-ым элементом которых является $\un{\mbf{a}}_{ij}$ 
или $\ov{\mbf{a}}_{ij}$. Множество вершин интервальной матрицы обозначаем как 
\begin{equation*}
\vert\mbf{A} \, := \, \bigl\{\,A\in\mbb{R}^{m\times n} \mid A = (\,a_{ij}),
\; a_{ij}\in\{\,\un{\mbf{a}}_{ij},\ov{\mbf{a}}_{ij}\}\,\bigr\}.
\end{equation*}
\end{definition} 
   
\medskip 
Упорядочение по включению на множестве интервальных векторов и матриц с элементами 
как из $\mbb{IR}$, так и $\mbb{KR}$ определяется как прямое произведение порядков 
по включению на отдельных компонентах этих составных объектов, т.\,е. 
\begin{eqnarray*}
\mbf{a} = (\mbf{a}_{i})\;\subseteq\;\mbf{b} = (\mbf{b}_{i})
  & \Longleftrightarrow &  \mbf{a}_{i}\subseteq\,\mbf{b}_{i}
  \quad\mbox{ для всех $i$,}\\[4pt]
\mbf{A} = (\mbf{a}_{ij})\;\subseteq\;\mbf{B} = (\mbf{b}_{ij})
  & \Longleftrightarrow & \mbf{a}_{ij}\subseteq\,\mbf{b}_{ij}
  \quad\mbox{ для всех $i,j$}.
\end{eqnarray*}
Операции <<$\vee$>> и <<$\wedge$>> в применении к интервальным векторам и матрицам 
будут, следовательно, пониматься покомпонентно и поэлементно, так что, в частности, 
\begin{equation*}
\left(
\begin{array}{@{\,}c@{\,}}
\mbf{a}_{1}\\  \mbf{a}_{2}\\  \vdots\\  \mbf{a}_{n}
\end{array}
\right)
\vee
\left(
\begin{array}{@{\,}c@{\,}}
\mbf{b}_1\\  \mbf{b}_2\\  \vdots\\  \mbf{b}_n
\end{array}
\right)
:=
\left(
\begin{array}{@{\,}c@{\,}}
\mbf{a}_1\vee\mbf{b}_1\\  \mbf{a}_2\vee\mbf{b}_2\\ 
\vdots\\  \mbf{a}_n\vee\mbf{b}_n 
\end{array}
\right)
\  \text{ и }  \
\left(
\begin{array}{@{\,}c@{\,}}
\mbf{a}_1\\  \mbf{a}_2\\  \vdots\\  \mbf{a}_n 
\end{array}
\right)
\wedge
\left(
\begin{array}{@{\,}c@{\,}}
\mbf{b}_1\\  \mbf{b}_2\\ \vdots\\  \mbf{b}_n 
\end{array}
\right)
:=
\left(
\begin{array}{@{\,}c@{\,}}
\mbf{a}_1\wedge\mbf{b}_1\\  \mbf{a}_2\wedge\mbf{b}_2\\
\vdots\\  \mbf{a}_n\wedge\mbf{b}_n
\end{array}
\right).
\end{equation*}
Аналогично, в покомпонентном и поэлементном смысле будут пониматься отношения 
<<$\leq$>>, <<$<$>>, <<$\geq$>> и <<$>$>> между точечными и интервальными векторами 
и матрицами. 
  
Интервальные векторы и матрицы являются весьма специальным и довольно узким классом 
множеств в многомерных пространствах. С ними относительно удобно работать, с их 
помощью оценивают другие множества, возникающие при решении математических задач. 
В этой связи чрезвычайно важно следующее понятие. 
  
\begin{definition}
\index{оболочка интервальная}\index{интервальная оболочка}  
Пусть $S$ --- непустое ограниченное множество в $\mbb{R}^n$. Его \textsl{интервальной 
оболочкой}, обозначаемой $\,\ih S$, называется наименьший по включению интервальный 
$n$-вектор, содержащий $S$. \\[1pt] 
Аналогично --- для нупустого ограниченного множества в $\mbb{R}^{m\times n}$. 
\end{definition}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{figure}[htb]
\setlength{\unitlength}{1mm}
\hfill\begin{picture}(50,30)(-25,-7)
\put(-7,-3){\includegraphics[width=25.5mm]{pictures/CasualFigure.eps}}
\put(-25,0){\vector(1,0){52}} \put(-18,-7){\vector(0,1){30}}
\put(-7,-3){\color{blue}\framebox(25.5,16){}} 
\put(-17,-3){\small$0$}
\put(24,-3){\small$x_1$}
\put(-17,21){\small$x_2$}
\end{picture}\hfill\null
\caption{Интервальная оболочка множества в $\mbb{R}^2$.}
\label{InteHullPic} 
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Взятие интервальной оболочки множества, естественно, имеет смысл и в одномерном случае. 
Если мы берём интервальную оболочку от множества, которое является семейством интервалов, 
то это сводится к применению операции \eqref{InteMaxExpr} --- максимуму по включению 
или её многомерному обобщению. 
  
Для применений интервального анализа наиболее важной является возможность распространения 
на интервальные матрицы и векторы арифметических и аналитических операций.  
  
\begin{definition} 
\label{IMatrOpsDefi}
Сумма (разность) двух интервальных матриц одинакового размера есть интервальная матрица 
того же размера, образованная поэлементными суммами (разностями) операндов.\\* 
Если $\mbf{A} = (\,\mbf{a}_{ij})\in\mbb{KR}^{m\times l}$ и $\mbf{B} 
= (\,\mbf{b}_{ij})\in\mbb{KR}^{l\times n}$, то произведение матриц 
$\mbf{A}$ и $\mbf{B}$ есть матрица $\mbf{C} = (\,\mbf{c}_{ij})\in 
\mbb{KR}^{m\times n}$, такая что
\begin{equation*}
\mbf{c}_{ij} := \sum_{k=1}^l \mbf{a}_{ik}\mbf{b}_{kj}. 
\end{equation*}
\end{definition}
  
Как связаны результаты интервальных операций между матрицами с результатами операций 
между их представителями? Для сложения и вычитания справедливо равенство 
\begin{equation*} 
\mbf{A}\pm\mbf{B} = \bigl\{\, A\pm B \mid A\in\mbf{A}, B\in\mbf{B}\,\bigr\}, 
\end{equation*} 
аналогичное свойству \eqref{IAMainPrinciple}, которое было положено в основу 
классической интервальной арифметики. Но вот для умножения интервальных матриц 
такое же равенство невозможно, так как множество всевозможных произведений 
точечных матриц из заданных интервальных матриц может не быть интервальной матрицей. 
Нетрудно показать, что имеет место более слабое, чем  \eqref{IAMainPrinciple}, 
соотношение 
\begin{equation*} 
\mbf{A}\cdot\mbf{B} \,  = \, 
   \ih\,\{\, A\cdot B \mid A\in\mbf{A}, B\in\mbf{B}\,\}. 
\end{equation*} 
Это наибольшее, чего можно желать в сложившихся условиях. 
  
Помимо операций сложения, вычитания и умножения интервальных векторов и матриц 
согласованных размеров имеет смысл ввести также 
  
\begin{definition}
Произведением скалярного интервала $\mbf{a}\in\mbb{KR}$ на интервальную матрицу 
$\mbf{B} = (\,\mbf{b}_{ij})\in\mbb{KR}^{m\times n}$ назовём интервальную матрицу 
из $\mbb{KR}^{m\times n}$, обозначаемую $\mbf{aB}$, у которой $ij$-ый элемент 
равен $\mbf{ab}_{ij}$. Аналогичным образом определяется произведение 
интервальной матрицы $\mbf{A}$ на скалярный интервал $\mbf{b}$. 
\end{definition}
  
Для интервальных векторных и матричных операций, совершенно аналогично одномерному 
случаю, выполняется важное свойство \emph{монотонности по включению}. Более точно, 
для любых интервальных матриц $\mbf{A}$, $\mbf{A}'$, $\mbf{B}$, $\mbf{B}'$ 
соответствующих размеров и любой операции $\star\in\{\;+,\,-,\,\cdot\;\}$ 
справедливо 
\begin{equation*} 
\mbf{A}\subseteq\mbf{A}',\ \mbf{B}\subseteq\mbf{B}'
\quad\Rightarrow\quad\mbf{A}\star\mbf{B}\subseteq\mbf{A}'\star\mbf{B}'. 
\index{монотонность по включению}
\end{equation*}
Оно непосредственно следует из определений и монотонности по включению
интервальных арифметических операций в $\mbb{IR}$ и $\mbb{KR}$.
    
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
  
\section[Истинное значение измеряемой величины]% 
        {Истинное значение \\* измеряемой величины} 
\label{TrueValueSect} 
  
  
\textit{Истинным значением измеряемой величины} называется значение, идеально отражающее 
в рамках принятой нами модели (теории) рассматриваемое свойство объекта или явления. 
Подчеркнём, что измеряемая величина существует лишь в рамках принятой модели, то есть 
имеет смысл только до тех пор, пока модель признается адекватной объекту или явлению. 
Принципиальным положением классической метрологии является утверждение о существовании 
истинного значения. Но получение этого истинного значения на практике часто является 
невозможным, так как измерения могут искажаться неизбежными помехами, наши измерительные 
приборы могут быть несовершенны и т.\,п. 
  
Это определение не вполне соответствует  определениям термина <<истинное значение 
величины>>, данных в <<Международном словаре по метрологии: основные и общие понятия 
и  соответствующие термины>> (\cite{MetrolVocab}, пункты 1.19 и 2.11, <<истинное  
значение величины>>; оригинальный текст --- \cite{MetrolVocabOrig}) и <<Рекомендациях 
по межгосударственной стандартизации РМГ 29-2013 ГСИ>> (\cite{RMG29-2013}, пункт 5.4, 
<<истинное значение>>). Основным отличием является отход современных методик измерений, 
принятых в 90-е годы XX века, от понятия истинного значения величины по причине 
его якобы недоступности. Тем не менее, мы далее придерживаемся этого понятия и его 
классического определения, существенно опираемся на них и используем в своих 
построениях. 
  
Методически полезно понятие \emph{действительного значения} постоянной величины. Это 
значение величины, полученное экспериментальным путем и настолько близкое к истинному 
значению, что в поставленной измерительной задаче может быть полноценно использовано 
вместо него \cite{RMG29-2013}.         \index{действительное значение} 
  
В зависимости от смысла измеряемой постоянной величины её истинное значение может быть 
целого, вещественного или интервального типа. Принадлежность целому или вещественному 
типу понятна и вполне традиционна, а вот рассмотрение интервального типа данных для 
истинного значения измеряемой величины --- это новшество, которое требует пояснений.  
  
Интервальное значение используется в тех случаях, когда точечное значение наблюдаемой 
величины не являются физически осмысленными. В физических законах и в практике измерений 
измеряемая величина (или характеристика) по своей первоначальной физической сути часто 
имеет точечный характер. Таковыми являются, например, температура тела, концентрация 
вещества в химическом растворе. В таких случаях построение двустороннего интервала 
для измеряемой величины может рассматриваться как способ представления информации о ней. 
  
С другой стороны, измеряемая величина по своей физической сути может быть не точечной 
величиной, а объектом, имеющим протяжённость, к примеру, интервалом на числовой оси или 
даже областью-пятном в пространстве. Например, это происходит в случае, когда летящий 
самолёт облучается радиолокационным сигналом. Поверхность самолета является криволинейной 
и состоит из многих отражающих участков разной ориентации. Как следствие, отраженный 
радиолокационный сигнал (даже от плоского фронта облучающего сигнала) уже будет 
<<размытой точкой>> или <<пятном>> в пространстве. 
  
\begin{example} 
\setcounter{RadarExmp}{\value{ExmpNum}}
Рассмотрим задачу кругового обследования пространства обзорным радиолокатором. 
Для получения информации, например, об азимуте летящего самолета, антенна радиолокатора, 
вращаясь, облучает самолет импульсными зондирующими сигналами, которые посылаются через 
равные промежутки времени (Рис.~\ref{RadarIntervalPic}). Вследствие конечной ширины 
диаграммы направленности антенны (она изображена колоколообразной штриховой линией 
на Рис.~\ref{RadarIntervalPic}) и конечных размеров самолёта отраженный сигнал имеет 
заметную величину для нескольких импульсов, т.\,е. <<размазывается>>. На входе приемника 
радиолокатора он приобретает вид <<всплеска>>, т.\,е. <<пачки>> отраженных импульсов 
(вертикальные столбики на  Рис.~\ref{RadarIntervalPic}), принятых в моменты времени 
$t_1$, \ldots, $t_n$. Инженеры производят оценку временн\'{о}го интервала $[t_1,t_n]$ 
основания этой пачки над порогом чувствительности приемника (горизонтальная 
штрих-пунктирная линия на  Рис.~\ref{RadarIntervalPic}). 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb]
\centering\small
  \unitlength=1mm
  \begin{picture}(90,50)
    \put(2,0){\includegraphics[width=90mm]{pictures/RadarInterval.eps}} 
    \put(0,11.2){\mbox{порог}} 
    \put(-10,8.4){\mbox{чувствительности}} 
  \end{picture}
\caption{Как образуется интервал при радиолокационном} 
обследовании объекта. 
\label{RadarIntervalPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
Таким образом, результат измерения в данном случае уже сам представляется некоторым 
интервалом. Но для дальнейших расчетов на практике используется, как правило, средняя 
точка $t_{\text{mid}}$ этого интервала, что является, конечно, огрублением исходных 
данных измерения. Введение интервального типа данных для результатов измерений позволит 
более точно обрабатывать подобные ситуации. 
\end{example} 
  
Похожая ситуация --- в квантовой механике, где наблюдаемые величины описываются 
линейными операторами. При этом для пар физических величин (называемых <<квантовыми 
наблюдаемыми>>), которые выражаются некоммутирующими операторами, существуют 
фундаментальные ограничения на точность одновременного определения их величины. 
Они задаются так называемыми <<соотношениями неопределённостей>>. Примером такого 
ограничения служит известное соотношение неопределённостей Гейзенберга для координаты 
и импульса, связывающее среднеквадратичные отклонения $\Delta x$ координаты и $\Delta p$ 
импульса частицы:                   \index{соотношение неопределённостей} 
\begin{equation*} 
\Delta x\,\Delta p \geq \hbar/2, 
\end{equation*} 
где $\hbar$ --- приведённая постоянная Планка. 
   
Примеры невозможности одновременного точного определения наборов физических величин 
можно продолжить. Для волновых процессов соотношение, родственное соотношению 
неопределённостей Гейзенберга,  выглядит как 
\begin{equation*} 
\Delta\nu \,\Delta t \geq 1, 
\end{equation*} 
где $\Delta \nu$ 
--- полоса пропускания системы в частотной области,  $\Delta t$ --- длительность 
наблюдаемого колебательного процесса \cite{ZeldovichMyshkis}. 
  
В оптике существует \emph{инвариант Лагранжа-Гельмгольца} (известный также как 
<<инвариант Гюйгенса-Гельмгольца>>) связывает линейную протяженность (ширину) 
пучка $y$ и его угловую расходимость $\varphi$:  
\begin{equation*} 
y\,n\,\varphi \; = \;\const, 
\end{equation*} 
где $n$ --- оптическая плотность среды. Эта величина неизменна в любой среде 
\cite{Tudorovski1948}. Таким образом, невозможно добиться одновременно идеальной 
пространственной и угловой фокусировки волнового пучка. 
  
В механике известна \emph{теорема Лиувилля} о сохранении фазового объёма 
в фазовом пространстве гамильтоновой системы (см.~\cite{Gantmacher}, стр.~262). 
Рассмотрим траекторию малого пятна (множества точек) в фазовом пространстве, 
описываемого обобщёнными координатами $\Delta p_{i}$ и $\Delta q_{i}$. В процессе 
перемещения вдоль траектории пятно может растягиваться по одной координате, 
например $p_{i}$, но сжимается по другой координате $q_{i}$ так, что произведение 
$\Delta p_{i}\,\Delta q_{i}$ остаётся константой. Площадь пятна (фазовый объём) 
при этом не изменяется. 
   
Упомянутые выше эффекты необходимо учитывать при построении технических систем 
и измерительных приборов в волновой и корпускулярной оптике. Например, 
во времяпролётных масс-спектрометрах пространственно-временная фокусировка 
заряженных частиц возможна в плоскости, а не в малой области пространства. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section{Измерения и их результаты} 
\label{MeasuResultSect} 
      
Одним из основных понятий метрологии и теории обработки наблюдений является понятие 
<<измерения>> или <<наблюдения>>. Слово <<измерение>> (как и его аналоги) имеет 
много значений. Оно может обозначать как процесс измерения или наблюдения, так и 
его результат. Из контекста обычно бывает ясно, какое значение слова имеется в виду. 
  
В капитальной монографии М.Ф.\,Маликова \cite{Malikov} можно прочитать следующее 
классическое определение: <<Измерением мы называем познавательный процесс, 
заключающийся в сравнении путём физического эксперимента данной величины с некоторым 
её значением, принятым за единицу сравнения. Число, выражающее отношение измеряемой 
величины к единице, называется числовым значением измеряемой величины \ldots>>. 
Последующее развитие науки и техники чрезвычайно расширило и раздвинуло сферу измерений 
и модифицировало смысл самого понятия <<измерения>>, что привело к появлению других его 
определений,  дополняющих и углубляющих определение М.Ф.\,Маликова. Например, измерением 
часто называют нахождение значения физической величины опытным путём с помощью 
специальных технических средств. Мы не будем далее заниматься содержательным 
смыслом понятия измерения, считая его достаточно ясным и доступным в окружающем 
нас информационном пространстве. 
   
Далее принимается  
  
\begin{definition}       
\textsl{Измерением (замером, наблюдением)} будем называть измеренное значение величины. 
\index{измерение}\index{замер} 
\end{definition}
  
По способу получения результата измерения все процессы измерения разделяются 
в \cite{Malikov} на \emph{прямые}, \emph{косвенные} и \emph{совокупные}. 
  
<<При прямых измерениях объект исследования приводят в непосредственное взаимодействие 
со средством измерений и по показаниям последнего отсчитывают значение измеряемой 
величины. Порой показания прибора умножают на некоторый коэффициент, вводят 
соответствующие поправки к ним и т.\,п.>> (цитата по книге \cite{Rabinovich1978}; 
см. также аналогичное определение в \cite{Malikov}). \index{измерения прямые}  
  
<<При косвенных измерениях искомое значение измеряемой величины находят на основании 
известной зависимости между этой величиной и величинами-аргументами. Последние находят 
в результате прямых, а иногда косвенных, \ldots или совокупных измерений>> (цитата 
по книге \cite{Rabinovich1978}; см. также аналогичное определение в \cite{Malikov}). 
\index{измерения косвенные} 
  
К совокупным измерениям относятся измерения, состоящие из совокупности рядов прямых 
измерений, причём числовые значения искомых величин определяются из системы (совокупности)  
уравнений, параметрами которых являются значения величин, измеренных прямым способом  
(см. книги \cite{Malikov,Rabinovich1978}). \index{измерения совокупные}

Конечно, описанная выше классификация в значительной мере условна. Дальнейшее развитие 
математических технологий измерений привело, фактически, к слиянию второго и третьего 
пунктов этой классификации, так как совокупные измерения можно представить как косвенные 
для неявно заданных функциональных зависимостей. С другой стороны, в настоящее время 
практически все измерения так или иначе являются косвенными или совокупными, поскольку 
требуют для своего выполнения нетривиальной математической обработки.  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[ht]
\centering\small  
\unitlength=1mm
\begin{picture}(50,70)
    \put(0,0){\includegraphics[width=50mm]{pictures/InfoFlowDiagram.eps}} 
    \put(9,61){Первичные измерения} 
    \put(9,43.5){Вторичные измерения} 
    \put(2.6,27){Данные, получаемые в резуль-}
    \put(3.4,23){тате последующей обработки} 
    \put(10,10){Выходная величина}
    \put(7,6){измерительного прибора} 
\end{picture} 
\caption{Преобразование первичных результатов} 
измерений в окончательный результат. 
\label{PrimMeasrPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
    
Процитируем книгу В.И.\,Мудрова и В.Л.\,Кушко \cite{MudrovKushko}:     
<<В современных сложных приборных устройствах измерение, понимаемое как сравнение 
некоторой величины с другой, принимаемой за эталон, присутствует лишь на низших уровнях 
многоступенчатой иерархической структуры, какой является соответствующий прибор \ldots 
Все последующие величины, включая выходную величину (отсчёт) прибора, получаются 
в результате вычислений по определённому алгоритму, реализованному с помощью либо 
специальной схемы \ldots, либо счётно-решающего устройства>>. Этот важный тезис 
иллюстрирует Рис.~\ref{PrimMeasrPic}, заимствованный также из книги \cite{MudrovKushko}. 
Для дальнейшего принципиально важен следующий момент: результат измерения является 
\begin{itemize} 
\item[$\color{red}\bullet$] 
итогом какого-то физического эксперимента, \\ 
в котором получаются <<первичные измерения>>, и 
\item[$\color{red}\bullet$] 
последующего применения некоторого способа \\ 
математической обработки <<первичных измерений>>. 
\end{itemize} 
  
В соответствии с основной линией этой книги мы считаем, что на практике измерение 
(замер, наблюдение) может представлять собой вещественное число или интервал или же 
составленные из них многомерные объекты (вектор, матрицу, интервальный вектор, 
интервальную матрицу и т.\,п.). 
    
Вещественный тип данных для измерений является традиционным и вопросов не вызывает. 
Но как в результате измерений могут быть получены интервалы? Частичный ответ на этот 
вопрос уже дан ранее в \S\ref{TrueValueSect}: если истинное значение измеряемой величины 
по-существу интервально, то результат его измерения тоже имеет интервальный тип. 
В частности, в Примере \ref{TrueValueSect}.\arabic{RadarExmp} интервальный результат 
измерения азимута летящего самолёта можно получить, измерив диаметр пятна на экране 
радиолокатора. Но существенно интервальные результаты могут получаться и при 
измерении вещественных величин, и ниже мы рассмотрим несколько типичных примеров 
на эту тему.\footnote{Интересно отметить, что к интервальнозначным результатам 
измерений не имеют никакого отношения \emph{шкалы интервалов}, рассматриваемые 
в теоретической метрологии (см., в частности, \cite{Pfanzagl}).} 
     
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
  
\paragraph{Погрешности измерений и наблюдений.} 
Интервалы в результатах измерений могут возникать в результате коррекции исходных 
точечных результатов. 
  
Один из распространённых способов получения интервальных результатов в первичных 
измерениях --- это <<обинтерваливание>> точечных значений, когда к точечному 
\emph{базовому значению} $\mathring{x}$, которое считывается по показаниям 
измерительного прибора, прибавляется \emph{интервал погрешности} $\mbf{\epsilon}$: 
\begin{equation} 
\label{GeneralErrorModel} 
\index{базовое значение}
\index{интервал погрешности} 
\mbf{x} = \mathring{x} + \mbf{\epsilon}.  
\end{equation} 
Интервал погрешности, вообще говоря, может быть произвольным, но если он уравновешен, 
т.\,е. 
\begin{equation*} 
\mbf{\epsilon} = [-\epsilon, \epsilon] \quad\text{ для некоторого } \epsilon > 0, 
\end{equation*} 
то иногда для прямых измерений это можно трактовать, как отсутствие систематических 
погрешностей. 
  
Возможно также использование модели погрешности измерений, которая содержит в каждом 
наблюдении как относительную $\delta$, так и абсолютную $\epsilon$ составляющие 
погрешности: 
\begin{equation*}
\begin{array}{l}
x = \mathring{x} + \delta\mathring{x} + \epsilon,  \qquad 
|\delta| \leq \delta_{\max}, \ |\epsilon| \leq \epsilon_{\max},
\end{array}
\end{equation*} 
где $x$ --- измерение с погрешностью, $\mathring{x}$ --- неизвестное истинное 
значение наблюдаемой величины; $\delta$, $\epsilon$ --- погрешности измерений, 
ограниченные по модулю, соответственно, максимальными величинами $\delta_{\max}$  
и $\epsilon_{\max}$. Таким образом, компонента $\delta\mathring{x}$ пропорциональна 
измеряемой величине, а компонента $\epsilon$ ---  абсолютная погрешность, не зависит 
от измеряемой величины. 
%%%  переписать! рассказать про возможную обрезку интервальной погрешности 
%%%  какой-либо содержательной границей значений измеряемой величины 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[ht]
\centering\small  
\unitlength=1mm
\begin{picture}(64,53)
    \put(0,0){\includegraphics[width=64mm]{pictures/ErrorsGraph.eps}} 
    \put(60,21){$\mathring{x}$}    \put(30.5,49){$\mbf{x}$} 
    \put(34,21){$\epsilon_{\max}$} \put(30,29){$\epsilon_{\max}$} 
\end{picture} 
\caption{Вычисление интервала неопределённости измерения} 
         при наличии относительной и абсолютной погрешностей.            
\label{UncertPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
   
В этом случае интервал неопределённости $\mbf{x} = [\un{x}, \ov{x}]$ измерения 
$\mathring{x}$ рассчитывается следующим образом: 
\begin{equation*} 
\mbf{x} \ 
= \  \left\{ \ 
\begin{array}{ll} 
\left[\,\displaystyle\frac{\mathring{x} - \epsilon_{\max}}{1 - \delta_{\max}}, 
      \; \frac{\mathring{x} + \epsilon_{\max}}{1 +  \delta_{\max}}\,\right], 
   &  \text{если} \   \mathring{x} < -\epsilon_{\max},                    \\[4mm] 
\left[\,\displaystyle\frac{\mathring{x} - \epsilon_{\max}}{1 - \delta_{\max}},
      \; \frac{\mathring{x} + \epsilon_{\max}}{1 - \delta_{\max}}\,\right], 
   &  \text{если} \  -\epsilon_{\max}\leq\mathring{x}\leq\epsilon_{\max}, \\[4mm] 
\left[\,\displaystyle\frac{\mathring{x} - \epsilon_{\max}}{1 + \delta_{\max}},
      \; \frac{\mathring{x} + \epsilon_{\max}}{1 - \delta_{\max}}\,\right], 
   &  \text{если} \  \mathring{x} > \epsilon_{\max}. 
\end{array} 
\right. 
\end{equation*}
Наглядно интервал неопределённости измерения в зависимости от результата точечного 
измерения показан на Рис.~\ref{UncertPic}.  
   
Если измерение имеет только абсолютную погрешность, то его интервал неопределённости 
равен 
\begin{equation*} 
\mbf{x} = [\,\mathring{x} - \epsilon_{\max}, \mathring{x} + \epsilon_{\max}]. 
\end{equation*} 
Если измерение имеет только относительную погрешность, то его интервал 
неопределённости равен 
\begin{equation*} 
\mbf{x} = \left[\,\frac{\mathring{x}}{1 + \delta_{\max}},\,
                  \frac{\mathring{x}}{1 - \delta_{\max}}\,\right]. 
\end{equation*}    
  
\begin{example} 
Предположим, что в процессе измерения силы тока мы смотрим на шкалу амперметра 
и, соблюдая все необходимые технические условия, отсчитываем измеренное значение 
--- 5.4 Ампера. Класс точности используемого прибора --- 2, и это, по определению, 
максимально допустимое значение основной приведённой погрешности, выраженной 
в процентах. Следовательно, истинное значение измеряемого тока должно лежать 
в интервале 
\begin{equation*} 
\bigl[ 5.4 - 0.02\cdot5.4, 5.4 + 0.02\cdot5.4\bigr] \ \text{Ампер} \ 
   = \  [ 5.292, 5.508] \  \text{Ампер}.            \vspace{-4mm} 
\end{equation*} 
\end{example} 
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\paragraph{Погрешности квантования.}  
Это инструментальная погрешность, возникающая при преобразовании величины, принимающей 
непрерывный ряд значений, в цифровую форму, которая может принимать дискретный набор 
допустимых уровней. Обычно мгновенное значение преобразуемого аналогового сигнала 
заменяется ближайшим разрешенным уровнем цифрового сигнала, что естественно, даёт 
некоторую погрешность --- погрешность квантования. \index{погрешность квантования} 
В обыденной речи её часто называют также \emph{погрешностью оцифровки}. 
\index{погрешность оцифровки} 
  
Погрешности квантования присущи всем аналого-цифровым преобразователям, и 
потому являются в настоящее время совершенно обыденными. Но если мы допускаем 
интервальный тип данных, интервальные результаты измерений, то результат 
представления непрерывного сигнала $t$, не равный точно какому-либо допустимому 
уровню $t_0$, $t_1$, \ldots, $t_p$, может быть записан как интервал $[t_{i}, 
t_{i+1}]\ni t$, и это интервальное представление совершенно точно. Тогда 
погрешности квантования оказываются, по-видимости, под полным контролем. 
  
Интересный пример возникновения такого рода интервальности приводится 
в книге В.\,Крейновича с соавторами по статистике интервальных данных 
\cite{NguyenKreinWuXiang}. Он называется <<интервальностью, мотивируемой 
конфиденциальностью>>, т.\,е. стремлением скрыть свои точные данные путём 
представления их в виде <<размазанного>>  интервала, которому они реально 
принадлежат и который вполне достаточен для исследовательских целей. 
  
Потребность в сокрытии точных данных о человеке в самом деле возникает в некоторых 
социально-экономических обследованиях, в медицине и других областях человеческой жизни. 
Участвующие в сборе данных люди не против указать значение интересующего исследователей 
параметра, но слишком точное его значение может раскрыть личность человека, что часто 
нежелательно. По этой причине вместо точного значения можно указывать содержащий 
его интервал. Например, вместо точного возраста 26~лет указать, что возраст 
принадлежит интервалу $[21, 30]$ лет. 
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
   
\paragraph{Неопределённость измерения нуля.}
Согласно \cite{RMG29-2013} \emph{погрешностью нуля} называется погрешность средства 
измерений в контрольной точке, когда заданное значение измеряемой величины равно нулю. 
\index{погрешность нуля} Соответственно, неопределённость измерений нуля --- это 
неопределённость измерений в случае, когда заданное значение измеряемой величины 
равно нулю. 
  
Неопределённость измерений нуля связывается с нулевым показанием или показанием, близким 
к нулю, и охватывает интервал, для которого неизвестно, является ли измеряемая величина 
слишком малой, чтобы быть обнаруженной, или показание средства измерений вызвано только 
шумом. 
  
Опять таки, если в нашем распоряжении оказываются данные интервального типа, то 
описание неопределённости измерения нуля решительно упрощается. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
  
\paragraph{Нестационарность измеряемых величин.} 
Нередко измеряемая величина, которая в рассматриваемой модели явления рассматривается 
как постоянная, на практике таковой не является. Например,  
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[!htb]
\unitlength=1mm
\centering\small 
\begin{picture}(78,36) 
\put(0,0){\includegraphics[width=78mm]{pictures/NonStatInterval.eps}} 
\put(72,4){\text{время}}
\end{picture}
\caption{Нестационарность величины может описываться} 
интервальным результатом её измерения.  
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
     
Пример подобной ситуации рассматривается в книге в следующей главе --- 
Пример~\ref{BazhenovExmp} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\paragraph{Погрешности алгоритмов обработки данных.} 
Они могут оцениваться с помощью двусторонних (интервальных) оценок, так что 
для получения гарантированного результата интервальность в самом деле необходима. 
 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
  
\paragraph{Агрегирование результатов многократных наблюдений.} 
Во многих практических ситуациях измерение интересующей нас величины выполняется 
для надёжности многократно. Тем не менее, повторные измерения над одними и теми же 
явлениями не показывают разумное (в пределах точности измерений) совпадение результатов. 
Приняв все необходимы меры предосторожности, обеспечив постоянные условия измерения, 
мы всё равно не получаем удовлетворительно близких друг к другу результатов. Например, 
в промышленности нередка ситуация, когда измерительный прибор, как бы тщательно он 
не был отрегулирован, демонстрирует колебания в показаниях, которые не могут быть 
уменьшены ниже некоторого предела. 
  
В этих условиях необходимо как-то представить результат серии повторяющихся измерений.  
Традиционное решение вопроса --- это усреднение отдельных результатов, но по ряду причин 
оно может быть не вполне удовлетворительным. В частности, усреднение даёт одно число 
и теряет информацию о разбросе исходных данных. В этих условиях итогом, обобщённо 
представляющим несколько измерений, можно взять интервал от минимального до максимального 
из полученных результатов, т.\,е. агрегировать (объединить) результаты отдельных измерений. 
Математически,\index{агрегирование результатов} если результаты повторных измерений 
величины равны $x_1$, $x_2$, \ldots, $x_n$, то интервальным результатом следует взять 
\begin{equation*} 
\mbf{x} = \bigl[\,\min_{1\leq i\leq n} x_{i}, \max_{1\leq i\leq n} x_{i}\,\bigr].  
\end{equation*} 
Будем называть этот способ получения интервального результата измерения 
\emph{агрегированием}. 
  
Используя введённые выше (см. \S\ref{IVectMatrSect}) операции взятия интервальной 
оболочки множества и максимума по включению \eqref{InteMaxExpr} этот результат 
можно записать следующим равносильным образом: 
\begin{equation*} 
\mbf{x}\, = \,\ih\,\{ x_1, x_2, \ldots, x_n\} 
\end{equation*} 
или 
\begin{equation*} 
\mbf{x} = \bigvee_{1\leq i\leq n} x_{i}.  
\end{equation*} 
Эти представления хороши тем, что могут быть обобщены на более сложные случаи 
(см. \S\ref{NonCoverSampleSect}).  
  
%  доверительный интервал? 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
  
\smallskip   
Интервал измерения строится с целью оценить истинное значение измеряемой величины, 
и получаемые при этом приближения могут быть  качественно различными. Они могут 
включать (<<накрывать>>) истинное значение, но они также могут его и не содержать
(см. \S\ref{CoverMeasrSect}). Последнее может происходить, например, для измерений, 
которые являются выбросами (но не только для них). 
 
В заключение раздела упомянем также специальный вид измерений, которые по-существу 
требуют использования интервального типа данных. Это <<измерения для обнаружения 
присутствия>>, введённые И.Г.\,Хацкевичем в \cite{Khatskevich}. Их целью является 
установление факта присутствия интересующей нас величины в каком-то интервале 
значений. 
  
Процитируем \cite{Khatskevich}: <<В процессе работы с данными телеметрии, 
поступающими с космических аппаратов (КА), приходится иметь дело с измерениями, 
содержащими информацию о факте присутствия измеряемых величин в некоторых 
допустимых интервалах значений, или с показаниями приборов фиксирующих факты 
присутствия наблюдаемых объектов в полях зрения их чувствительных элементов. 
Примером могут служить данные, используемые для привязки ко времени телеметрической 
информации: последовательность дискретных значений показаний бортовых часов 
с указанием количества опросов временн\'{о}го телеметрического канала, содержащегося 
между ними. Другим примером служат оптические датчики с разнообразной геометрией 
поля зрения и разнообразным назначением. Включая их показания в обработку 
для определения параметров, характеризующих состояние системы (в данном случае 
--- положение на орбите КА или его ориентацию), целесообразно наиболее полно и 
без искажений использовать содержащиеся в них данные --- информацию о факте 
присутствия>>.  \index{измерение для обнаружения присутствия} 
    
<<Факт присутствия состоит в том, что в поле зрения прибора, определённым образом 
ориентированного в пространстве, отмечается присутствие предмета, и совершенно 
не указывается его положение относительно границ поля зрения. К факту присутствия 
может быть отнесено также попадание внутрь заданного временн\'{о}го интервала 
временн\'{о}й отметки о прохождении объектом некоторого оптического или 
радиолокационного барьера>>  \cite{MudrovKushko}.  
  
В работе \cite{Khatskevich} предлагается математический формализм для описания 
измерений для обнаружения присутствия, но, как замечают В.И.\,Мудpов и В.Л.\,Кушко 
в книге \cite{MudrovKushko}, такого рода показания приборов описываются 
совершенно так же, как предлагалось в основополагающей работе Л.В.\,Канторовича 
\cite{Kantorovich}, иными словами, с привлечением интервалов для описания 
двусторонних неравенств. Естественно, что введение интервального типа данных 
радикально поможет в этом описании, а различные математические формализмы 
для оперирования с такими измерениями --- измерениями для обнаружения 
присутствия --- можно стоить, например, на основе характеристической 
функции интервалов. 
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
  
\section{Погрешность измерений} 
  
    
На практике измерения и наблюдения, как правило, подвержены неизбежным внешним влияниям, 
выполняющие их средства измерений и приборы не вполне точны и т.\,п., что в целом 
приводит к отличию измеренного значения от истинного (идеального) значения величины. 
По отношению к неточным измерениям иногда используют термин <<зашумлённые>> (зашумлённые 
данные и т.\,п.), особенно, когда проводится целая серия таких измерений или наблюдений. 
Чтобы  количественно охарактеризовать неточности измерений, вводится понятие 
\emph{погрешности}. 
  
Погрешность измерения --- это отклонение результата измерения от истинного значения 
измеряемой величины. Математически погрешность равна алгебраической разности 
измеренного значения и истинного значения величины. Если это истинное значение $x^\ast$ 
и результат измерения $\tilde{x}$ --- вещественные числа, то погрешностью является 
разность $\tilde{x} - x^{\ast}$. Если истинное значение и результат измерения --- 
интервалы $\mbf{x}^{\ast}$ и $\tilde{\mbf{x}}$ соответственно, то погрешность 
$\mbf{\varDelta}$ определяется как алгебраичес\-кая разность 
\begin{equation} 
\label{AbsError} 
\mbf{\varDelta} \  = \  \tilde{\mbf{x}}\ominus\mbf{x}^{\ast}
\end{equation}
в полной интервальной арифметике Каухера, задаваемая посредством \eqref{AlgebrMinus}. 
Напомним, что обычное интервальное вычитание, которое обозначается традиционным 
знаком <<$-$>> и является интервальным расширением вычитания, не является операцией, 
алгебраически обратной сложению, и потому для нашей цели непригодно. Формула 
\eqref{AbsError} справедлива и в~том случае, когда истинное значение величины 
$x^{\ast}$ --- точечное, а результат её измерения $\tilde{\mbf{x}}$ интервальный. 
При этом в \eqref{AbsError} полагаем $\mbf{x}^{\ast} = [\,x^{\ast}, x^{\ast}]$. 
\index{погрешность} 
  
В русской и международной терминологии для обозначения погрешностей часто используется 
термин <<ошибка>>, <<error>>. Мы не придерживаемся этого словоупотребления потому, что 
в метрологии <<ошибкой>> обычно называют результаты измерений, выполненных без соблюдения 
необходимых условий на измерительные процедуры, с нарушениями технологических регламентов 
и т.\,п. В этом смысле ошибка измерения родственна выбросу или промаху (см. далее 
\S\ref{OutlierSect}), но имеет другой генезис.                  \index{ошибка} 
  
\textit{Абсолютной погрешностью} измерения назовём модуль (абсолютное значение) 
погрешности. Для интервальных измерений абсолютная погрешность равна модулю интервала 
разности $\tilde{x}\ominus\mbf{x}$, и, как легко видеть, она равна расстоянию 
\eqref{InteDist} между измеренным и истинным значениями величины. 
\index{абсолютная погрешность} 
  
\begin{example}
Рассмотрим для примера ситуацию, когда истинное значение измеряемой величины, скажем, 
массы какого-то груза, является интервалом $[3, 4]$ кг, а её измерение дало интервал 
$[3, 5]$ кг. Тогда его погрешность равна 
\begin{equation} 
\label{1stAbsErExmp}
[3, 5]\;\text{кг}\ominus[3, 4]\;\text{кг} = [0, 1] \ \text{кг}. 
\end{equation} 
Если в результате измерения мы получим вещественное значение $3.8$ кг, которое 
отождествляется с интервалом $[3.8, 3.8]$ кг, то его погрешность 
\begin{equation}
\label{2ndAbsErExmp}
[3.8, 3.8]\;\text{кг} \ominus [3, 4]\;\text{кг} = [0.8, -0.2]\;\text{кг} 
\end{equation} 
--- неправильный интервал. Может показаться, что он бессмыслен с физической точки 
зрения, но это поспешный вывод. Ситуация здесь совершенно аналогична, например, тому, 
как при измерении положительных физических величин (массы, плотности, давления и 
т.\,п.) мы получаем отрицательную погрешность, если измеренное значение приближает 
истинное значение снизу. 
  
Абсолютная погрешность измерения равна $1$ в случае \eqref{1stAbsErExmp} и $0.8$ 
в случае \eqref{2ndAbsErExmp}. 
\end{example}
  
\textit{Относительной погрешностью} измерения назовём отношение его абсолютной 
погрешности к модулю истинного значения величины: 
\begin{equation}
\delta \  = \  \frac{|\,\tilde{\mbf{x}}\ominus\mbf{x}^{\ast}|}{|\,\mbf{x}^{\ast}|} \  
          = \  \frac{\dist(\,\tilde{\mbf{x}}, \mbf{x}^{\ast})}{|\,\mbf{x}^{\ast}|}. 
\end{equation}
Как и в традиционном случае, в знаменателе этих дробей вместо редко доступного истинного 
значения величины можно использовать её действительное значение. 
\index{относительная погрешность} 
  
Точечная оценка истинного значения некоторой постоянной величины --- это число 
(вектор, матрица и т.\,п.), дающий приближение истинного значения в том или ином смысле. 
Аналогично, интервальная оценка истинного значения --- это интервал, дающий приближение 
истинного значения в том или ином смысле (находящаяся с ним в том или ином отношении 
--- принадлежности, расположения на определенном расстоянии, и т.д.). 
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section[Накрывающие и ненакрывающие измерения]% 
        {Накрывающие и ненакрывающие \\* измерения} 
\label{CoverMeasrSect} 
  
  
Если результат измерения --- точечная величина, то для неё возможны только два исхода 
проведения измерения: либо она получается равной истинному значению интересующей нас 
величины, либо не равной ей. Как говорят математики и программисты, исход измерения 
является <<булевозначным>>, <<да>> или <<нет>>. При этом ясно, что в случае измерения 
непрерывных физических величин, принадлежащих вещественному типу данных, равенство 
является исключительным событием и почти никогда не достигается. Если же оно 
по каким-то причинам произошло, то является неустойчивым к сколь угодно малым 
возмущениям или погрешностям в вычислительных алгоритмах. 
  
Принципиально другая ситуация возникает, если результат измерения может быть интервалом. 
Невырожденный интервал по своей сути является весьма представительным множеством на  
вещественной оси (имеющим ненулевую меру), и принадлежность ему истинного значения --- 
это уже не исключительное событие. Оно, как правило, устойчиво к малым возмущениям и 
погрешностям вычислений. Как следствие, для теории обработки интервальных данных 
фундаментальный характер имеет следующее определение: 
  
\begin{definition}
\textsl{Накрывающее измерение} (накрывающий замер) --- это интервальный результат 
измерения, который гарантированно содержит истинное значение измеряемой величины. \\ 
Измерение, для которого нельзя утверждать, что оно содержат истинное значение 
измеряемой величины, будем называть \textsl{ненакрывающим} 
{\rm(Рис.~\ref{PCoverMeasurPic}} и {\rm Рис.~\ref{ICoverMeasurPic})}. 
\index{накрывающее измерение}\index{ненакрывающее измерение} 
\end{definition}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[!htb]
\unitlength=1mm
\centering\small 
  \begin{picture}(110,30) 
    \put(0,7){\includegraphics[width=48mm]{pictures/PointCoverMesr.eps}} 
    \put(62,7){\includegraphics[width=48mm]{pictures/PointNonCovMesr.eps}} 
    \put(72,5){\mbox{\small\begin{tabular}{c}интервал \\[-1pt] измерения\end{tabular}}}
    \put(11,5){\mbox{\small\begin{tabular}{c}интервал \\[-1pt] измерения\end{tabular}}} 
    \put(42.4,14.5){\mbox{$\mbb{R}$}}  \put(104.4,14.5){\mbox{$\mbb{R}$}}        
    \put(14,24){\small\mbox{истинное значение}}    
    \put(82,24){\small\mbox{истинное значение}}    
  \end{picture}
\caption{Накрывающее (слева) и ненакрывающее (справа)} 
измерения точечного истинного значения величины.
\label{PCoverMeasurPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
Отметим, что с точки зрения формальной логики понятия накрывающего и ненакрывающего 
измерений являются <<противоположными>>, но не <<противоречащими>> (см., к примеру, 
\cite{Chelpanov}). Они находятся в так называемом подпротивном отношении друг к другу: 
в общем случае ненакрывающее измерение может не содержать истинное значение, а может 
и содержать его, но мы не знаем об этом наверняка. 
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[!ht]
\centering\small 
\unitlength=1mm
  \begin{picture}(110,30) 
    \put(0,5){\includegraphics[width=48mm]{pictures/InteCoverMesr.eps}} 
    \put(62,5){\includegraphics[width=48mm]{pictures/InteNonCovMesr.eps}} 
    \put(73,5){\mbox{\small\begin{tabular}{c}интервал\\[-1pt] измерения\end{tabular}}}
    \put(11,5){\mbox{\small\begin{tabular}{c}интервал\\[-1pt] измерения\end{tabular}}} 
    \put(43,11){\mbox{$\mbb{R}$}}  \put(105,11){\mbox{$\mbb{R}$}}        
    \put(14,24){\mbox{\small\begin{tabular}{c}истинное\\[-1pt] значение\end{tabular}}} 
    \put(87,24){\mbox{\small\begin{tabular}{c}истинное\\[-1pt] значение\end{tabular}}} 
  \end{picture}
\caption{Накрывающее (слева) и ненакрывающее (справа)} 
измерения интервального истинного значения величины. 
\label{ICoverMeasurPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
     
Итак, накрывающее измерение является гарантированной двусторонней <<вилкой>> значений 
измеряемой величины, тогда как для ненакрывающего измерения ничего подобного утверждать 
нельзя. Это придаёт накрывающим измерениям принципиально более высокий статус и 
позволяет строить на их основе более содержательные конструкции. 
      
Важность введённых выше понятий обусловлена тем, что накрывающее (охватывающее) измерение 
даёт не просто какое-то приближение к интересующему нас истинному значению физической 
величины, но двустороннюю оценку (<<вилку>>) этого значения, т.\,е. его гарантированные  
оценки снизу и сверху. Это обстоятельство позволяет привлечь для обработки накрывающих 
измерений более сильные средства, качественно другой математический аппарат (в частности, 
некоторые специфичные методы интервального анализа), и получить в результате уточнённые 
оценки для истинного значения также в виде двусторонней оценки. Для ненакрывающих 
измерений и выборок это подчас недостижимо. 
  
Тот факт, что интервальный результат измерения не является накрывающим, может быть 
вызван различными причинами. Прежде всего, это измерение может оказаться так называемым 
выбросом (промахом), см. \S\ref{OutlierSect}. Ещё одной частой причиной является то, что 
в процессе получения интервального результата из точечных показаний прибора мы можем  
недооценить погрешность прибора или методики измерения, т.\,е. взять её меньшей, чем 
реальная погрешность. Сюда же относится неадекватность выбранной модели объекта 
(рассматриваемая величина, к примеру, может не быть постоянной и дрейфовать во времени 
в процессе выполнения измерений). 
  
Проверка того, является ли данное измерение или выборка накрывающими, находится уже 
вне рамок математической теории интервальных измерений. Она возлагается на практику 
измерений, которая в каждом конкретном случае должна дать (или не давать) гарантии 
двусторонних оценок истинного значения измеряемой величины. Отметим также, что 
для традиционных точечных измерений аналога введённых понятий не существует, так как 
все точечные измерения, как правило, ненакрывающие. Накрывающее точечное измерение 
совпадает с истинным значением измеряемой величины, и потому подобное событие 
является исключительным и его почти невозможно проверить. Как следствие, вводить 
отдельное понятие для описания такой ситуации смысла нет. Но для интервальных 
измерений ситуация решительно меняется. 
  
Ценность свойства накрытия столь велика, что нередко для его достижения прибегают 
к специальным приёмам в процессе предобработки данных. Например, там, где это имеет 
смысл, можно несколько расширить полученные интервалы результатов измерений вокруг 
базового измеренного значения, чтобы новые расширенные интервалы были гарантированно 
накрывающими, т.\,е. содержащими истинные значения измеряемых величин (см. 
\S\ref{UncertAlterSect} и \S\ref{VaryUncertSect} в следующих главах). 
  
Так как проверка того факта, что измерение или выборка являются накрывающими, 
весьма непросто, можно привлекать косвенный признак накрытия --- свойство совместности 
выборки (см. \S\ref{SetCompatibitiy}). Эта подмена особенно привлекательна потому, 
что многие методы обработки интервальных данных позволяют точно указать величину 
расширения исследуемых интервалов, необходимую для достижения совместности выборки. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\section{Принцип соответствия} 
\label{CorresPrincpSect}
   
\textit{Принцип соответствия} в методологии науки --- это утверждение, что любая 
новая научная теория должна включать старую теорию и её результаты как частный 
предельный случай. 
  
<<В наиболее общем виде принцип соответствия может быть сформулирован следующим 
образом: теории, справедливость которых была экспериментально установлена для 
определённой группы явлений, с появлением новых теорий не отбрасываются, но сохраняют 
своё значение для прежней области явлений как предельная форма и частный случай новых 
теорий. Выводы новых теорий в той области, где была справедлива старая ``классическая'' 
теория, переходят в выводы классической теории. Математический аппарат новой теории, 
содержащий некоторый характеристический параметр, значения которого различны в старой 
и новой области явлений, при надлежащем значении характеристического параметра 
переходит в математический аппарат старой теории>> (цитата дана из книги 
\cite{CorrPrincBook}, стр.~6). Методологическое значение принципа соответствия 
состоит в том, что он может служить средством построения и коррекции новых 
теоретических систем, новых систем понятий, обеспечивая их непрерывную связь 
со старыми теориями. 
  
Принцип соответствия впервые был предложен Н.\,Бором в начале XX века для объяснения 
взаимного соотношения нарождавшейся квантовой механики и традиционной физики макрообъектов 
\cite{Bohr1928}. Он понадобился как средство для обеспечения <<непрерывной склейки>> новой 
теории с классической физикой, которая прекрасно описывает и объясняет огромное количество 
окружающих нас явлений и не может быть просто отвергнута на том основании, что она <<стара>>, 
<<немодна>> и т.\,п.\footnote{Н.\,Бор внёс огромный вклад в разработку философских вопросов 
квантовой механики. Помимо рассматриваемого нами ``принципа соответствия'' он является 
также автором ``принципа дополнительности'' (принципа комплементарности), одного 
из важнейших методологических и эвристических принципов квантовой механики и науки 
вообще.} Другой популярный пример: в специальной теории относительности для малых 
скоростей мы получаем те же уравнения движения, что и в классической механике Ньютона. 
\index{принцип соответствия} %  ссылка?  
  
В действительности, принцип соответствия имеет более широкое методологическое 
значение, будучи применим не только к физике и естествознанию, но и к математике 
(см. статью на эту тему в сборнике \cite{CorrPrincBook}). На современном этапе развития 
науки и техники сюда же следует отнести информационные технологии, статистику и анализ 
данных, а также другие современные области знаний. В применении к нашей ситуации, 
при обработке интервальных данных,  принцип соответствия требует, чтобы \emph{хорошие 
и разумные интервальные методы в пределе, при стремлении ширины интервалов к нулю, 
переходили в какие-то разумные методы обработки точечных данных}, коль скоро 
действительные числа являются предельным случаем интервалов. 
  
В сформулированном выше тезисе под <<методами обработки точечных данных>> мы имеем 
в виду, конечно, родственные по духу аппроксимационные методы, не опирающиеся на теорию 
вероятностей, так как вероятностные характеристики измерений и их погрешностей взять 
в данной ситуации неоткуда. \index{аппроксимационные методы} 
   
Далее мы будем использовать принцип соответствия, как инструмент проверки <<разумности>> 
и адекватности наших конструкций, понятий и методов обработки данных с интервальными 
неопределённостями, который позволяет отсекать заведомо <<неразумные>>. Комментарии 
о том, удовлетворяется ли принцип соответствия для тех или иных конкретных методов, 
будут делаться в посвящённых им местах текста. Отметим, что не все предложенные 
на данный момент методики обработки интервальных данных находятся в согласии 
с принципом соответствия. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\section{Выбросы и промахи} 
\label{OutlierSect}
  
\textit{Выбросами} или \textit{промахами} в метрологии называются такие измерения, 
результаты которых не привносят информацию об исследуемом объекте в рамках его 
принятой модели. \index{выброс}\index{промах}
  
Другое популярное определение выбросов или промахов состоит в том, что это результаты 
измерений, которые для данных условий резко отличаются от остальных результатов общей 
выборки. Выбросы нарушают некоторую однородность (согласованность, непротиворечивость), 
характерную для большинства наблюдений выборки по отношению к заданной математической 
модели (см.~\S\ref{RegrOutlSect}). 
  
Оба приведённых определения неформальны, так как, по-видимому, одного формального 
определения для этого важнейшего понятия дать нельзя.  
  
Как правило, выбросы (промахи) стремятся удалить из выборки на этапе её предварительной 
обработки (предобработки), то есть перед применением\index{предобработка} формальных 
математических методов, так как присутствие выбросов существенно искажает оценки 
истинных значений параметров. Выявление выбросов является нетривиальной и, как правило, 
трудно формализуемой процедурой, которая опирается на опыт и т.\,п. Для вероятностной 
статистики выявление выбросов является необходимой составной частью обработки данных, 
а некоторые процедуры даже рекомендованы в стандартах \cite{GOSTDirect}. 
  
Что считать выбросом (промахом) в случае интервальных результатов измерений? 
Прежде всего, не стоит связывать выбросы со свойством измерений быть накрывающими 
или ненакрывающими. Более точно, из того, что интервальное измерение не является 
накрывающим, не следует, что оно представляет выброс или промах. 
   
Отождествление выбросов (промахов) со свойством ненакрывания противоречит 
принципу соответствия, сформулированному в предыдущем параграфе. В самом деле, 
при стремлении ширины интервальных измерений к нулю они переходят в точечные 
измерения, которые, как правило, всегда ненакрывающие. Тем не менее, различение 
для них выбросов (промахов) от этого не исчезает. 
  
Если априори известно, что измерение, производимое данным инструментом при помощи 
некоторой определённой методики должно быть накрывающим, то, конечно, получение 
ненакрывающего результата служит явным признаком выброса. 
   
Более подробно мы рассмотрим выбросы и промахи, а также методики их выявления 
в соответствующих параграфах Глав~3 и 4, при обсуждении измерения постоянной 
величины и восстановления функциональных зависимостей. 
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\section{Выборки интервальных данных} 
\label{InteSampleSect} 
  
  
Серьёзные измерения редко проводятся однократно, всего один раз. Как правило, имеют 
дело с целыми сериями измерений, объединяемыми в более или менее однородные группы. 
Неоднократные измерения практикуют из-за того, что случайные погрешности при таких 
сериях измерений могут быть легче выделены и отсеяны. Кроме того, некоторые задачи 
обработки измерений (например, задача восстановления зависимостей) по-существу 
требуют для своего решения не одно измерение, а некоторую их совокупность, 
на основании которой только и может быть получен ответ. 
  
Как известно, выборка или выборочная совокупность в вероятностной статистике 
--- это часть генеральной совокупности элементов, которая охватывается экспериментом 
(наблюдением, опросом). Мы в нашей работе будем называть \textit{выборкой} просто 
совокупность результатов измерений. Абстрактное понятие генеральной совокупности, 
которая представляет собой совокупность всех мыслимых (но реально не существующих) 
наблюдений интересующей нас величины при заданных условиях эксперимента, в анализе 
интервальных данных не используется. \index{выборка}\index{генеральная совокупность} 
  
Отметим, что понятие <<генеральной совокупности>> является одним из наиболее тонких 
и неоднозначных понятий теоретико-вероятностной статистики, которое справедливо 
подвергалось критике за трудность его привязки к реальной практике (см. 
\cite{Alimov1980, AlimovKravtsov}). Тем не менее, в вероятностной статистике 
это понятие абсолютно необходимо для строгого обоснования почти всех основных 
конструкций. 
  
Обязательное требование к выборке в теоретико-вероятностной статистике --- её 
\emph{репрезентативность} или \emph{представительность}, т.\,е. свойство правильно 
отражать исследуемые свойства всей генеральной совокупности. Сформулированное 
определение очень общо и неконкретно, так что иногда его уточняют, например, 
следующим образом: <<выборка называется репрезентативной (представительной) 
если она достаточно хорошо представляет пропорции генеральной совокупности>> 
\cite{SmirnovDunBark}. \index{репрезентативность}\index{представительность} 
Иными словами, репрезентативная (представительная) выборка производится так, что 
она отражает специфику генеральной совокупности и по составу, и по индивидуальным 
характеристикам включаемых в неё объектов. Исследование выборки на репрезентативность 
отдельная непростая задача. 
  
Каждое измерение из интервальной выборки описывается своим отдельным интервалом 
неопределённости. Погрешности и неопределённости многомерных величин могут 
описываться интервальными векторами, которые могут быть <<брусами>>, т.\,е. 
прямыми произведениями интервалов по отдельным переменным, либо какими-либо другими 
конструкциями --- шарами некотрой нормы, порядковыми отрезками и т.\,п. 
  
% Дописать про "неопределённость измерения", в духе современных GUMов, а также связать 
% с погрешностью. Написать про <<интервал неопределённости измерения>>. 
  
Важным вопросом является перенесение понятия <<накрытия истинного значения>> 
на выборки. Простейший путь --- объявить <<накрывающей выборкой>> совокупность 
накрывающих измерений, тогда как выборки, не удовлетворяющие этому условию, т.\,е. такие, 
в которых присутствует хотя бы одно ненакрывающее измерение станут <<ненакрывающими>>. 
Но этот очевидный ход мысли будет чисто теоретическим, не учитывающим реальную практику 
измерений, где погрешности и выбросы (промахи) неотъемлемо присутствуют в данных. Кроме 
того, проверка самого свойства <<накрытия истинного значения>> является нетривиальной. 
В целом, чересчур жёстко определяемое понятие <<накрывающей выборки>> будет малополезным. 
  
Далее мы будем называть \textit{накрывающей выборкой}\index{накрывающая выборка} 
совокупность измерений, в которой доминирующая часть (большинство и т.\,п.) измерений 
(наблюдений) являются накрывающими. Напротив, выборка называется \textsl{ненакрывающей}, 
если преобладающая часть\index{ненакрывающая выборка} входящих в неё измерений --- 
ненакрывающие. Возможные альтернативные термины --- <<включающая выборка>>, а отрицание 
--- <<невключающая выборка>>.\footnote{Предлагаемые английские эквиваленты --- enclosing 
measurement, covering measurement.}\index{включающая выборка} 
  
Данное выше определение нестрого и использует расплывчатые понятия <<большинство>>, 
<<доминирующая часть>> и т.\,п., которые должны уточняться каждый раз в процессе 
своего применения. Тем не менее, эти определения реалистичны, и обычно они не приводят 
к большим неудобствам на практике.  
     
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section{Информационное множество} 
\label{InfoSetSect} 
  
  
В этом параграфе мы определим неформально одно из важнейших понятий теории обработки 
интервальных данных --- понятие \textit{информационного множества}, соответствующего 
интервальным данным измерений и модели объекта. 
  
Данные измерений, о которых говорилось в предшествующих пунктах, можно называть 
\emph{первичными}, так как чаще всего они ещё подвергаются дальнейшей обработке, чтобы 
была получена оценка постоянной величины, построена зависимость и т.\,п. Мы уже обсуждали 
эту особенность измерительных технологий в \S\ref{MeasuResultSect}. Напомним также, что 
необходимость математической обработки прямо оговаривается в определениях косвенных 
измерений и совокупных измерений. Таким образом, для определения окончательного 
результата измерения необходимо дополнить наши конструкции <<моделью обработки данных>> 
или <<способом обработки данных>>. Это математическая модель, формализующая требования 
к результату обработки измерения и оформленная в виде системы уравнений, неравенств, 
задачи оптимизации и т.\,п., которая определяет то, что должно считаться результатом 
обработки измерений, оценкой параметров и т.\,д. 
  
\begin{example}
Предположим, что мы решаем задачу восстановления функциональной зависимости некоторого 
заданного вида по данным измерений. Эта зависимость может восстанавливаться, например, 
методом наименьших квадратов, методом наименьших модулей или с помощью чебышёвского 
(минимаксного) сглаживания. Перечисленные методы представляют разные модели обработки 
данных. \index{метод наименьших квадратов}\index{метод наименьших модулей}  
\index{чебышёвское сглаживание} 
  
Для одних и тех же данных измерений, т.\,е. первичных данных, итоговый результат 
измерения будет существенно разным в зависимости от того, какая именно методика их 
обработки применяется --- метод наименьших квадратов (МНК), метод наименьших модулей 
или чебышёвское (минимаксное) приближение. Соответственно, мы получим три различных 
результата, которые в обычном случае неинтервальных данных, скорее всего, будут 
одноточечными множествами (но иногда могут состоять более чем из одной точки). 
\end{example}  
  
В рассмотренном примере результаты измерений, т.\,е. оценивания параметров функции, 
представляются точкой в пространстве параметров функции. Это же верно для результатов 
других типов измерений. Но если данные первичных измерений интервальны, то и результат 
их обработки, как правило, будет не одной точкой, а каким-то множеством точек. Мы 
будем называть его информационным множеством измерения. На практике это может быть, 
например, область значений функции при косвенных измерениях или же множество решений 
интервальной системы уравнений при совокупных измерениях и т.\,п.  
  
Иными словами, информационным множеством задачи измерения с интервальными данными 
является множество значений интересующих нас параметров задачи, которые совместны 
с данными измерений в рамках выбранной модели их обработки. Ниже 
в Главах~\ref{MeasrConstChap} и \ref{FuncFitChap} мы дадим конкретные 
определения информационных множеств, возникающих в задаче оценивания 
постоянной величины и в задаче восстановления линейной зависимости. 
    
Из сказанного следует, что информационное множество существенно зависит от выбранной 
модели обработки данных, и потому даже для одних и тех же данных может быть определено 
неединственным образом в зависимости от того, как мы эти данные обрабатываем и 
интерпретируем. Для интервальных данных информационное множество --- это множество 
значений параметров, удовлетворяющих математической системе отношений (как правило, 
интервальной системе уравнений, неравенств и т.\,п.), полученной в результате 
агрегирования информации о математической модели объекта, первичных данных измерений 
и модели их обработки.          \index{информационное множество} 
  
Отметим, что понятие <<информационного множества>> давно используется в теории 
дифференциальных игр и теории управления динамическими системами в условиях 
неопределённости. В этих дисциплинах оно обозначает совокупность возможных позиций 
системы или совокупности параметров её движения, совместных со свойствами системы, 
её описанием и информацией о её движении, накопленной к моменту обработки данных. 
Далее после работы \cite{Kurzhanski} термин <<информационное множество>> стал 
применяться для обозначения совокупности результатов измерений и наблюдений 
в условиях ограниченной  неопределённости, и мы следуем этой традиции. Ранее 
для этой же цели использовались (и иногда используются до сих пор) термины 
<<область возможных значений параметров модели>> \cite{VoschininSotirov}, 
<<множество допустимых значений параметров>>, <<область неопределённости параметров 
модели>> \cite{KantorSpivak,SpivakEtAl} и т.\,п. 
  
Аналогом <<информационного множества>> может отчасти служить понятие доверительного 
интервала  оцениваемой случайной величины в традиционной вероятностной статистике. 
В определение доверительного интервала входит дополнительный параметр --- уровень 
статистической значимости, без которого понятие становится бессодержательным из-за 
неограниченности носителей большинства вероятностных распределений, но смысл 
доверительного интервала примерно соответствует <<информационному множеству>>. 
\index{доверительный интервал} 
  
Далее для обозначения различных информационных множеств мы будем использовать прописную 
греческую букву $\varOmega$ (<<омега>>), добавляя к ней при необходимости параметры, 
обозначающие контекст задачи. Так как информационное множество может быть достаточно 
произвольным множеством в пространстве параметров и не обязательно является интервалом, 
интервальным вектором или интервальной матрицей, мы не выделяем  его символ жирным 
шрифтом. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\section[Двойственный характер интервальной неопределённости]% 
        {Двойственный характер \\* интервальной неопределённости} 
\label{DualUncertSect}
  
Интервальная неопределённость имеет двойственный характер, вытекающий из двойственного 
смысла, который может быть приписан интервалу, как множеству значений некоторой 
величины (см. \cite{SSharyIzvAN97, SSharyBook, SSharySurvey2002}). С одной стороны, 
интервал $[\un{a}, \ov{a}]$ может представлять множество \emph{всех} вещественных чисел, 
заключённых между своими концами $\un{a}$ и $\ov{a}$. С другой стороны, этот же 
интервал может быть вместилищем, двусторонними границами для какого-то \emph{одного} 
числа между $\un{a}$ и $\ov{a}$. Это различие, которое на первый взгляд может показаться 
почти схоластическим, обретает совершенно осязаемые черты при более детальном анализе 
постановок интервальных задач и привязке их к практике. 
  
Задачи с интервальными параметрами --- это некоторые математические формулировки, 
описывающие выполнение каких-то условий, которые содержат параметры, могущие принимать 
значения из заданных интервалов. И здесь интервалы параметров допускают двоякую 
трактовку. С одной стороны, интересующие нас свойства, условия или требования могут 
выполняться для всех значений из рассматриваемого интервала (интервального вектора-бруса, 
интервальной матрицы и т.\,д.). С другой стороны, эти свойства или условия могут 
выполняться лишь для некоторых (в крайнем случае --- для одного) значения из интервала 
(бруса и т.\,д.). Два этих случая соответствуют различному пониманию постановки 
задачи, где встречаются соответствующие интервалы. Дадим теперь более формальное 
описание этого различия. 
  
Интервалы вещественной оси $\mbb{R}$ --- это множества точек из $\mbb{R}$, которые 
мы обычно рассматриваем как удобные в описании и использовании множества возможных 
значений параметров различных задач. Как правило, можно считать, что задано какое-либо 
свойство $\mcl{P}(v)$, которое может выполняться или же не выполняться для точек $v$ 
из интервалов параметров. 
  
Например, свойство $\mcl{P}\,$ может иметь вид <<$v$ является решением данного 
уравнения>>, <<$v$ является решением рассматриваемой задачи>> с некоторыми 
параметрами, которые принимают значения из заданных интервалов, и т.\,д. 
При этом возможны следующие принципиально различные ситуации: 
\begin{list}{}{\itemsep=1pt\topsep=2pt\parsep=2pt}
\item[1)] 
либо свойство $\mcl{P}(v)$ выполняется для \emph{всех} точек $v$ из данного 
интервала $\mbf{v}$, 
\item[2)] 
либо свойство $\mcl{P}(v)$ выполняется лишь для \emph{некоторых} точек $v$ из интервала 
$\mbf{v}$, не обязательно всех, или даже только для одного значения из $\mbf{v}$. 
\end{list} 
Формально это отличие хорошо описывается с помощью логических кванторов --- специальных 
логических операций, которые уточняют и модифицируют смысл переменных. В математической 
логике наиболее популярны квантор всеобщности <<$\forall$>>, означающий <<для всех>>, 
и квантор существования <<$\exists$>>, означающий <<существует>> (см., например, 
\cite{DiscrMath, Kleene, Uspenskii}). Они помогают кратко и ёмко записать отмеченное 
различие между интервальными величинами:  
\begin{equation*}
%\label{UncerTypes}
\parbox{110mm}{%
\begin{list}{}{\itemsep=5pt\topsep=2pt\parsep=2pt}
\item[$-$] 
в первом случае мы пишем \ <<$(\forall v\in\mbf{v})\,\mcl{P}(v)\,$>> \\ 
\hspace*{4mm} и говорим об \emph{интервальной A-неопределённости},
\item[$-$] 
во втором случае мы пишем \ <<$(\exists v\in\mbf{v})\,\mcl{P}(v)\,$>> \\ 
\hspace*{4mm} и говорим об \emph{интервальной E-неопределённости}.
\end{list}}
\end{equation*} 
  
Таким образом, при работе с интервалами и постановке разнообразных интервальных 
задач мы должны различать два указанных выше типа интервальной неопределённости. 
В частности, на этом пути возникает важное понятие кванторных решений и AE-решений 
интервальных систем уравнений, неравенств и т.\,д. (см. \cite{SSharyIzvAN97, 
SSharyBook, SSharySurvey2002}), а также понятия слабого и сильного согласования 
интервальных измерительных данных и восстанавливаемых параметров объекта 
(см. \S\ref{GenIDataFitSect}). 
   
Очень удобна терминология, предложенная И.И.\,Ерёминым в книге \cite{IIEremin} 
(см. \S 21). Свойства, условия и пр., зависящие от интервального параметра 
с A-неопределённостью, т.\,е. выполняющиеся для всех значений параметра из заданного 
интервала, назовём \emph{сильными}. Свойства, условия и пр., зависящие от интервального 
параметра с E-неопределённостью, т.\,е. выполняющиеся для некоторых значений параметра 
из заданного интервала (может быть, даже одного), назовём  \emph{слабыми}. 
\index{сильное свойство}\index{слабое свойство} 
   
Ясно, что если некоторое свойство выполнено <<для всех>> значений параметра 
из какого-то множества, то оно тем более выполнено <<для некоторых>> значений 
из этого множества. Следовательно, множество решений задачи с сильными интервальными 
параметрами содержится во множестве решений задачи со слабыми интервальными 
параметрами на тех же интервалах. Этим часто пользуются на практике. 
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\section[Дальнейшая классификация интервальных данных]% 
        {Дальнейшая классификация \\*  интервальных данных} 
\label{InteWidClass}
      
Помимо классификации интервальных результатов измерений по происхождению, данной 
в \S\ref{MeasuResultSect}, существуют также другие подходы к их классификации. 
Они нужны для того, чтобы наилучшим образом выбрать метод их обработки, а также 
наиболее адекватно интерпретировать информацию, представляемую обрабатываемыми 
данными. 
  
\paragraph{Классификация по ширине интервалов.}  
Прежде всего, имеет смысл различать интервальные данные по ширине интервалов, 
т.\,е. по величине имеющейся у них неопределённости. 
  
\begin{itemize} 
\item[(I)] 
Если интервальные данные являются <<узкими>> (или даже очень узкими), почти совпадая 
с точечными величинами, то для них интервальная специфика выражена слабо или вовсе 
не выражена. В некоторых ситуациях для их обработки можно даже применять те подходы 
и алгоритмы, которые используются для неинтервальных (точечных) данных. С другой стороны, 
<<небольшая интервальность>> узких интервалов позволяет выполнять с ними упрощённые, но 
достаточно точные приёмы обработки, основанные на асимптотических разложениях, пренебрежении 
членами высокого порядка и т.\,п. В частности, для узких интервальных данных известный 
способ оценивания неопределённости косвенного измерения посредством линеаризации 
функциональной зависимости имеет смысл и приводит к хорошим результатам (см., 
к примеру, Главы~6 в книгах \cite{Rabinovich1978,Rabinovich2005}). 
\item[(II)] 
При увеличении ширины интервальных данных  их уже нельзя рассматривать как 
<<приближённо точечные>>, они становятся <<существенно интервальными>>. Тем не менее,  
они всё-таки несут некоторые черты, присущие точечным данным. Так, результаты точечных
измерений редко совпадают, т.\,е. почти никогда не пересекаются. По этой причине 
отсутствие пересечений существенно интервальных измерений выборки является признаком 
того, что интервальные данные всё ещё <<не слишком широки>> и не слишком сильно 
отличаются от точечных данных. 
\item[(III)]   
Наконец, при дальнейшем увеличении ширины интервальных измерений в выборке они начинают 
пересекаться друг с другом, и это служит признаком следующего качественного состояния 
--- когда интервальные данные являются <<широкими интервальными>>, т.\,е. интервальная 
неопределённость велика. Этот случай является специфически интервальным, к которому 
точечные методы обработки данных уже принципиально неприменимы. 
\end{itemize}  
  
Выписанная классификация интервальных данных <<по ширине>> мотивируется также 
теоретическими результатами по сложности интервальных вычислений. Известно, что 
подавляющее большинство задач Интервального Анализа являются труднорешаемыми 
(NP-трудными и NP-полными --- в математических терминах) в самом общем случае, если 
не накладываются никакие ограничения на ширину входных интервалов. Но если интервальные 
данные задачи --- узкие, ситуация меняется. В теории сложности интервальных вычислений 
известна теорема Лакеева--Крейновича, утверждающая, что для любой гладкой функции и 
<<достаточно узких>> интервалов изменения её аргументов существуют полиномиально 
сложные алгоритмы для сколь угодно точной оценки областей значений этой функций 
(см. \cite{LakeKreino}, а также книгу \cite{KreLakRohnKahl}, 
глава~16).\index{теорема Лакеева--Крейновича}  

\paragraph{Классификация по способу измерения.}  
В книге \cite{NguyenKreinWuXiang} вводится классификация интервальных данных 
по способу их получения --- с помощью одного или нескольких измерительных устройств. 
Она полезна и для наших целей. 
  
С учётом различных способов измерений и различной точности измерительных инструментов 
существенно по-разному нужно выполнять приём варьирования неопределённости в выборке 
(см. \S\ref{UncertAlterSect}). 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1. Что на входе? 
% 2. Что хотим получить на выходе? оценки точечные и интервальные 
\section{Оценки точечные и интервальные} 
  
Оценки величин, которые мы получаем с помощью традиционной статистики, как известно, 
могут быть \emph{точечными} или \emph{интервальными}. 
  
Точечные оценки, т.\,е. оценки в виде точки --- числа, вектора или матрицы --- 
соответствуют, как правило, тому типу данных, который используется в модели 
рассматриваемого объекта или явления и могут непосредственно использоваться 
при его дальнейшем исследовании, при прогнозировании его поведения и т.\,п. 
  
Интервальные оценки дают области возможных значений точечных оценок и нужны 
для характеризации их возможного разброса и изменчивости (которую мы будем также 
называть \emph{вариабельностью},\index{вариабельность} см. \S\ref{ConstVariabSect} 
и \S\ref{VariabilitySect}). В традиционной вероятностной статистике оценки 
параметров сами являются случайными величинами, а носители их вероятностных 
распределений могут быть неограниченными. По этой причине при определении интервальных 
оценок в вероятностной статистике обычно задают некоторый \emph{уровень значимости} 
или \emph{уровень доверительной вероятности}, с помощью которых выполняют усечение 
вероятностного распределения. Тем самым всегда обеспечивается ограниченность 
интервальных оценок и их практичность. 
%  поместить сюда рисунок! 
  
В интервальном анализе данных оценки величин также могут быть \emph{точечными} либо 
\emph{интервальными} или даже иметь форму каких-то других множеств. Точечная оценка 
несёт тот же смысл, что и в традиционной статистике, а интервальная оценка тоже даёт 
область возможных значений точечных оценок, характеризуя их возможный разброс и 
вариабельность. Многомерные интервальные оценки удобнее всего брать в форме брусов 
(см.~\S\ref{IVectMatrSect}). 
  
Но есть и существенные отличия от вероятностной статистики. Во-первых, задание уровня 
значимости не требуется, так как множества значений оценки, как правило, ограниченны. 
Во-вторых, интервальные оценки могут иметь различных смысл --- быть внутренними, 
внешними или какими-нибудь другими, сообразно чему их смысл различен 
(см.~\S\ref{ApprInfoSetSect}). В-третьих, в пределах внутренней интервальной оценки 
все значения равноценны и тоже могут служить точечными оценками рассматриваемой величины. 
Напротив, в традиционной вероятностной статистике точечные значения внутри интервальной 
оценки не вполне равноценны друг другу. 
  
%    2.1 Способы оценивания множеств совместных значений параметров
% 3. Как из входа получить выход? %(Технология/методика)
      
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\chapter{Измерение постоянной величины} 
\label{MeasrConstChap}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\section[Выборка измерений и интервалы их неопределённости]% 
        {Выборка измерений \\*  и интервалы их неопределённости} 
\label{MeasSetInte}   
  
Напомним, что \emph{постоянная величина} --- это величина, которая в рассматриваемом 
процессе сохраняет свое значение неизменным. К примеру, рост человека не меняется 
заметно в процессе его измерения, и потому может считаться постоянной величиной, 
хотя на протяжении жизни человека (или даже в течение суток) его рост, конечно же, 
несколько изменяется.                             \index{постоянная величина} 
  
Пусть имеется выборка измерений некоторой величины, 
\begin{equation}
\label{ISample} 
\mbf{x}_{1}, \mbf{x}_{2}, \ldots, \mbf{x}_{n}, 
\end{equation}                                 
или, кратко, $\{\,\mbf{x}_{k}\}_{k=1}^n$, где $k$ --- номер измерения, $\mbf{x}_k$ 
--- интервальный результат измерения, полученный, к примеру, какой-либо из процедур, 
описанных в предыдущих параграфах. Таким образом, согласно терминологии интервального 
анализа, рассматриваемая выборка --- это вектор интервалов, или интервальный вектор 
$\mbf{x} = (\mbf{x}_{1}, \mbf{x}_{2}, \ldots, \mbf{x}_{n})$. Число $n$ --- размерность 
вектора данных --- будем, как обычно, называть \emph{длиной выборки} (или объёмом 
выборки).\index{длина выборки} По интервальным результатам измерений или наблюдений 
требуется построить оценку для интересующей нас величины. 
\index{задача измерения постоянной величины} 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb]
\centering\small 
  \unitlength=1mm
  \begin{picture}(90,55)
    \put(0,0){\includegraphics[width=90mm]{pictures/ScatterPlot.eps}}
    \put(72,6){\mbox{\small номер измерения}} 
  \end{picture}
\caption{Диаграмма рассеяния интервальных} 
измерений постоянной величины
\label{ScatPlotPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
   
Для наглядного представления выборки часто рисуют образующие её интервалы в виде 
графика, например, как это изображено на Рис.~\ref{ScatPlotPic}. По статистической 
традиции подобные графики мы будем называть \emph{диаграммами рассеяния} (см. также 
Рис.~\ref{EncloConstPic} и Рис.~\ref{NEnclConstPic}). Кроме вертикального представления 
интервалов данных, когда по оси абсцисс отложены номера измерений, можно повернуть 
картинку и представлять интервалы данных горизонтально (см. Рис.~\ref{GraviConstPic}). 
Мы в равной мере будем пользоваться обоими способами. В статистике такие диаграммы 
называют, соответственно, \emph{столбчатыми} (или столбиковыми) и \emph{ленточными} 
\cite{EliseevaYuzbashev}. С более сложными примерами диаграмм рассеяния, возникающими 
в задаче восстановления зависимостей по интервальным данным, мы встретимся далее 
в Главе~4.                                     \index{диаграмма рассеяния} 
  
Значения $\r\mbf{x}_k$, $k = 1,2,\ldots,n$, показывают величины интервальной 
неопределённости отдельных измерений выборки. Вектор радиусов $\r\mbf{x} = 
(\r\mbf{x}_{1}, \r\mbf{x}_{2}, \ldots, \r\mbf{x}_{n})$ характеризует величину 
неопределённости всей выборки. Но часто не требуется такая детальность 
в представлении неопределённости выборки, а нужна какая-либо одна величина, 
которая выражает эту неопределённость агрегированным образом. В этом случае 
мы можем взять какую-нибудь норму вектора $\r\mbf{x}$, в зависимости от смысла, 
который желательно вложить в эту обобщающую характеристику. 
  
Напомним, что \emph{нормой} вектора называется математическое понятие, формализующее 
интуитивно ясные представления о <<размере объекта>>, его абстрактной <<величине>> 
т.\,п., вне зависимости от его формы, расположения и прочих второстепенных качеств. 
Обычно норма векторов вводится аксиоматически,\index{норма} как некоторая вещественная 
и неотрицательная функция на множестве векторов. По традиции она обозначается как 
$\|\cdot\|$ и удовлетворяет следующим условиям, называемым аксиомами векторной нормы 
\cite{GolubVanLoan,Watkins,HornJohn}: 
\begin{itemize} 
\item[(1)]  
$\|a\|\geq 0$ для любого вектора $a$, причём 
\item[(2)]  
$\|\alpha a\| = |\alpha|\cdot\|a\|$ для любого вектора $a$ и любого числа $\alpha$, 
\item[(3)]  
$\|a + b\|\leq \|a\| + \|b\|$ для любых векторов $a$ и $b$. 
\end{itemize}
  
На практике в качестве норм векторов чаще всего используют простые выражения от компонент 
вектора $a = (a_{1}, a_{2}, \ldots, a_{n})^{\top}\in\mbb{R}^n$, которые имеют ясный 
содержательный смысл (см., к примеру, \cite{Bakhvalov,GolubVanLoan,HornJohn}). В частности, 
в математике популярны 
\begin{align*} 
\|a\|_{1} = \sum_{i=1}^n \,|a_{i}| \ & \text{ --- 1-норма}, 
   \index{1-норма} \\[3mm]  
\|a\|_{2} \  = \  
   \sqrt{\;\sum_{k=1}^{n}\,a_{k}^{2}\,} \ & 
   \text{--- \begin{tabular}{l} 2-норма, которую называют \\[1pt]  
             также \emph{евклидовой нормой},\end{tabular}} 
   \index{2-норма} \\[2mm]  
\|a\|_{\infty} \  = \  \max_{1\leq k\leq n}\,|a_{k}| \ & 
   \text{ --- \begin{tabular}{l} $\infty$-норма или \emph{чебышёвская норма},\\[1pt]  
              называемая также \emph{максимум-нормой}. \end{tabular}}  
   \index{чебышёвская норма}\index{максимум-норма}\index{$\infty$-норма} 
\end{align*} 
  
В практической метрологии, где длина выборки может меняться от одного 
эксперимента к другому, часто необходимо обеспечить соизмеримость результатов 
их обработки вне зависимости от количества измерений. В то же время, значение 
1-нормы вектора или его 2-нормы существенно зависит от размерности вектора, 
т.\,е. от количества его компонент. Чебышёвская норма от размерности вектора 
напрямую не зависит. 
  
Чтобы нивелировать зависимость нормы вектора от размерности, полезны 
нормы, использующие усреднение, в частности, усреднённая 1-норма 
\begin{equation*} 
\|a\|_{1} \  = \  \frac{1}{n}\;\sum_{i=1}^n \,|a_{i}|, 
\end{equation*} 
а также усреднённая евклидова (среднеквадратичная) норма 
\begin{equation*} 
\|a\|_{2} \  = \  
   \sqrt{\;\frac{1}{n}\;\sum_{k=1}^{n}\,a_{k}^{2}\,}\,. 
\index{среднеквадратичная норма} 
\end{equation*} 
Среднеквадратичная норма чрезвычайно популярна в вероятностной статистике 
по причине её ясного теоретико-вероятностного смысла. 
 
Возвращаясь к агрегированной мере рассеяния выборки, можем предложить её, 
к примеру, в одном из следующих видов 
\begin{align*} 
\|\r\mbf{x}\|_{1} \  &= \  
   \frac{1}{n}\; \sum_{i=1}^n \,\bigl|\,\r\mbf{x}_{i}\bigr|,        \\[3mm]  
\|\r\mbf{x}\|_{2} \  &= \  
   \sqrt{\;\frac{1}{n}\;\sum_{k=1}^{n}\,(\,\r\mbf{x}_{k})^{2}\,},  \\[3mm]  
\|\r\mbf{x}\|_{\infty} \  &= \  \max_{1\leq k\leq n}\,\r\mbf{x}_{k}. 
\end{align*} 
Числовые значения этих характеристик выборки соразмерны друг с другом, так как 
существуют неравенства, ограничивающие значения какой-то одной определённой нормы 
двусторонними границами, выраженными через любую другую норму --- так называемые 
неравенства эквивалентности для норм (см., например, \cite{HornJohn}. Но их взаимное 
сопоставление значений отдельных норма может дать ценную информацию о неопределённости 
отдельных измерений в выборке. В частности, заметное различие значений чебышёвской 
нормы (т.\,е. $\|\r\mbf{x}\|_{\infty}$) и усреднённых первых норм (т.\,е. 
$\|\r\mbf{x}\|_{1}$ или $\|\r\mbf{x}\|_{2}$) свидетельствует о том, что неопределённости 
отдельных измерений непостоянны в зависимости от индекса и имеют разброс (см. 
Пример~\ref{TwoSamplExmp}). 
  
По аналогии с традиционной метрологией, будем называть измерения выборки 
\textit{равноширинными}, если неопределённость всех этих измерений одинакова, т.\,е. 
$\r\mbf{x}_k = r = \const$, $k = 1,\ldots,n$. Напротив, \textit{неравноширинными} 
(разноширинными) называем измерения, в которых величина неопределённости $\r\mbf{x}_k$ 
может меняться в зависимости от измерения выборки, $k = 1,\ldots, n$. Фактически, эти 
термины  означают  <<имеющие равную неопределённость>>  и  <<имеющие неодинаковые 
неопределённости>>. \index{равноширинные измерения}\index{неравноширинные измерения} 
Сделанное выше наблюдение о соотношении значений различных характеристик 
неопределённости выборки можно переформулировать следующим образом: заметное отличие 
$\|\r\mbf{x}\|_{1}$ или $\|\r\mbf{x}\|_{2}$ от $\|\r\mbf{x}\|_\infty$ говорит о том, 
что исследуемая выборка --- неравноширинная. 
    
Информационным множеством в случае оценивания единичной постоянной величины по выборке 
интервальных данных будет также интервал, который мы будем называть \emph{информационным 
интервалом}. Неформально говоря, это интервал, содержащий значения оцениваемой величины, 
которые <<совместны>> с измерениями выборки (<<согласуются>> с~данными этих измерений). 
Но конкретный смысл, вкладываемый в понятия <<совместные>> или <<согласующиеся>>, будет 
различен для разных ситуаций. В частности, он зависит от того, является ли выборка  
интервальных данных накрывающей или нет.        \index{информационный интервал} 
   
% Упомянуть, что это определение является частным случаем общего определения, 
% и ещё один его частный случай будет фигурировать в Главе 5. 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   

\section{Обработка накрывающих выборок} 
\label{CoverSampleProcSect} 
  
  
Пусть $\{\mbf{x}_{k}\}_{k=1}^{n}$ --- накрывающая выборка интервальных данных, 
т.\,е. такая, в которой доминирующая часть (большинство и т.\,п.) измерений 
являются накрывающими --- содержат истинное значение интересующей нас величины 
(см. \S\ref{InteSampleSect}). 
  
Если истинное значение величины содержится в интервалах измерений выборки  $\{\,\mbf{x}_{k}\}_{k=1}^n$ за исключением, возможно, некоторых немногих, то оно 
должно принадлежать также пересечению этих интервалов. Следовательно, уточнённым 
интервалом принадлежности истинного значения можно взять 
\begin{equation} 
\label{IXInterval} 
\mbf{I}\; = \;\bigcap_{1\leq k\leq n} \mbf{x}_{k}. 
\end{equation} 
Это и будет информационное множество $\mbf{I}$ оценки измеряемой физичес\-кой величины 
(см. Рис.~\ref{EncloConstPic}). Его можно называть для краткости \emph{информационным 
интервалом}. Явные выражения для его левой (нижней) и правой (верхней) границ даются 
следующими формулами:                            \index{информационный интервал}
\begin{equation}
\label{LoUpBounds} 
\un{\mbf{I}}\, = \,\max_{1\leq k\leq n} \,\un{\mbf{x}}_{k}, 
\hspace{23mm} 
\ov{\mbf{I}}\, = \,\min_{1\leq k\leq n} \,\ov{\mbf{x}}_{k}. 
\end{equation}                                     
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb]
\centering\small 
  \unitlength=1mm
  \begin{picture}(90,52)
    \put(0,0){\includegraphics[width=90mm]{pictures/ConstMeasrEncl.eps}}
    \put(75,5){\mbox{\small Номер измерения}} 
  \end{picture}
\caption{Обработка накрывающей выборки\\* интервальных измерений величины.} 
\label{EncloConstPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
     
В силу сделанного допущения о том, что выборка накрывает истинное значение величины, 
имеем  $\un{\mbf{I}}\leq\ov{\mbf{I}}$. При этом интересен предельный случай совместной 
выборки, когда $\un{\mbf{I}} = \ov{\mbf{I}} = x^{\ast}$. Тогда выборка совместна, но мы, 
образно говоря, находимся на пределе её совместности, и информационный интервал $\mbf{I}$ 
вырождается при этом в точку. 
   
Если известен некоторый априорный интервал возможных значений оцениваемой постоянной  
величины $\mbf{I}_\text{апр} = [\un{\mbf{I}}_\text{апр}, \ov{\mbf{I}}_\text{апр}]$, 
который должен гарантированно содержать её, то границы результирующего интервала 
\eqref{IXInterval} могут быть уточнены пересечением 
\begin{equation}
\label{ImpIXInterval}
\mbf{I} = \mbf{I} \,\cap \,\mbf{I}_{\text{апр}}. 
\end{equation} 
Отметим, что априорный интервал $\mbf{I}_\text{апр}$ может задавать одностороннее 
ограничение, если он имеет вид $[\un{\mbf{I}}_\text{апр}, +\infty]$ или 
$[-\infty, \ov{\mbf{I}}_\text{апр}]$, т.\,е. является полубесконечным интервалом 
из арифметики Кэхэна (см. \S\ref{KahanArithmSect}).   
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
На практике часто необходимо работать не с интервалами интересующей нас величины --- 
\eqref{IXInterval} или \eqref{ImpIXInterval}, а с некоторой точечной оценкой $\hat{x}$. 
Все точки информационного интервала вполне равноценны друг другу, так что эту точечную 
оценку $\hat{x}$ можно выбирать достаточно произвольно (см. Рис.~\ref{EncloConstPic}). 
Тем не менее, имеет смысл взять из интервала некоторое точечное значение, которое 
представляет его наилучшим образом. В качестве такой величины можно использовать, 
к примеру, его \textit{центральную оценку} $x_{\text{c}}$, 
\begin{equation}
\label{MidEstim}
x_{c} \; = \;\m\mbf{I}\; = \;\tfrac{1}{2}\,\bigl(\un{\mbf{I}} + \ov{\mbf{I}}\bigr). 
\end{equation} 
Напомним, что в силу \eqref{MidOptimal} середина интервала обладает определённой 
оптимальностью, являясь точкой, которая наименее удалёна от других точек этого 
интервала.             \index{центральная оценка}  
  
Другой возможный сценарий обработки данных накрывающей выборки может состоять в том, 
что вместо пересечения интервальных измерений мы используем обобщающую её операцию 
<<$\wedge$>>, т.\,е. взятие минимума всех интервальных результатов измерений 
относительно упорядочения по включению, которое задаётся Определением~\ref{IncluDefi}: 
\begin{equation}
\label{IncluMin} 
\mbf{I} \  = \   
\bigwedge_{1\leq k\leq n} \mbf{x}_{k} \   = \  
\Bigl[\,\max_{1\leq k\leq n} \un{\mbf{x}}_{k}, 
    \min_{1\leq k\leq n} \ov{\mbf{x}}_{k}\,\Bigr].  
\end{equation} 
Здесь по существу требуется использование полной интервальной арифметики Каухера, 
так как интервал \eqref{IncluMin} может оказаться неправильным. Соответственно, 
точечной оценкой измеряемой величины целесообразно взять 
\begin{equation}
\label{WidOptEst} 
x_\text{c} \  = \  \m\mbf{I} \  
   = \  \tfrac{1}{2}\Bigl(\,\max_{1\leq k\leq n} \un{\mbf{x}}_{k} 
                          + \min_{1\leq k\leq n} \ov{\mbf{x}}_{k}\,\Bigr), 
\end{equation} 
т.\,е. середину интервала, который получается как минимум по включению всех интервалов 
выборки (см. \eqref{InteMinExpr}). Если выборка совместна, то \eqref{WidOptEst} совпадает 
с \eqref{MidEstim}. Если же выборка несовместна, то результатом \eqref{IncluMin} является 
неправильный интервал $\mbf{I}$, $\r\mbf{I} < 0$. Соответственно, информационное 
множество результатов измерений по обрабатываемой выборке пусто. 
  
Но даже когда интервал \eqref{IncluMin} неправилен, его середина \eqref{WidOptEst} 
--- это точка, обладающая определёнными условиями оптимальности. Она первой появляется 
в непустом пересечении интервалов выборки, если мы станем равномерно уширять их, 
увеличивая неопределённость измерений (см. ниже \S\ref{UncertAlterSect}). 
  
В самом деле, пусть радиусы всех интервалов выборки увеличились на $s$, $s\geq 0$, 
тогда как середины остались неизменными. Вместо радиусов $\r\mbf{x}_{k}$ мы получили 
$\r\mbf{x}_{k} + s$, $k = 1,2,\ldots,n$. Кроме того, все нижние концы интервальных 
измерений стали теперь $\un{\mbf{x}}_{k} - s$, а верхние концы --- $\ov{\mbf{x}}_{k} + s$, 
$k = 1,2,\ldots,n$. Как следствие, $\max_{1\leq k\leq n} \un{\mbf{x}}_{k}$  уменьшается 
на $s$, а $\min_{1\leq  k\leq n} \ov{\mbf{x}}_{k}$ увеличивается на $s$, а радиус 
получающегося интервала \eqref{IncluMin} теперь равен $\r\mbf{I} + s$. Поэтому, если 
взять $s$ таким, чтобы $s\geq|\r\mbf{I}|$, то получившийся интервал станет правильным, 
и точка $x_\text{c}$ будет лежать в нём. Можно также сказать, что в точке \eqref{WidOptEst} 
минимизируется равномерное уширение интервалов данных рассматриваемой выборки, 
необходимое для достижения её совместности. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   

\begin{example} 
\label{SiOExmpPart2} 
Ранее в Примере~\ref{SiOExmpPart1} (с.~\pageref{SiOExmpPart1}) рассмотрена задача 
оценивания массовой доли оксида кремния в стандартном образце материала по выборка 
интервальных измерений $W$. Анализ выборки $W$ обнаружил, что она несовместна, но 
её максимальная совместная подвыборка $W_1$ включает в себя все измерения $W$ 
за исключением одного --- измерения $\mbf w_6$ (Рис.~\ref{SiOEstimatesPic}). 
  
В случае, если причиной несовместности исходной выборки послужили грубые ошибки 
при регистрации измерения $\mbf w_6$, искомую оценку массовой доли оксида кремния 
$\hat{\mbf{w}}$ логично строить по подвыборке $W_1$.
  
Следуя \eqref{IXInterval}, то есть вычисляя $\hat{\mbf{w}}$ пересечением измерений 
подвыборки $W_1$, получим  
\begin{gather*}
\hat{\mbf{w}}\; = \bigcap_{\mbf{w}_k \in W_1} \mbf{w}_k \  
   = \  [3.392, 3.466] \  \% \ \text{м.д.}, \\[3mm] 
w_c\; = \;\m\hat{\mbf{w}} = 3.429\;\% \ \text{м.д.}, 
   \quad \varrho\; = \;\r\hat{\mbf{w}} = 0.037\;\% \ \text{м.д.}
\end{gather*} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
     
\begin{figure}[ht]
\centering 
%    \begin{subfigure}{.61\textwidth}
\subfile{pictures/SiOW1Estim}
\caption{Оценка массовой доли оксида кремния\\ по подвыборке $W_1.$
%$\m \hat{\mbf{w}}$ (\ref{EstimMidInterval}), $\hat{w}_{\text{мнк}}(W)$ 
%(\ref{EstimLSM}), $\hat{w}_{\text{мнк}}(W_1)$ (\ref{EstimLSM1})
}
\label{SiOEstimatesPic}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \color{red}
% С.И.Кумкову: Выяснить происхождение информации о неопределённости исходных наблюдений
% ($\epsilon=0.086$). От этого зависит, какую МНК-оценку оставить в тексте и на рисунке. 

% \color{teal}
% С.И.Кумков: по словам <<метрологини>> аттестованное значение $\epsilon=0.086$ 
% определяется не по измерениям выборки, а <<внешним>> по отношению к данным образом --- 
% выводом из характеристик метода, условий измерений с учетом приборной погрешности и 
% человеческого фактора. Документ, который регламентирует этот вывод, С.И. Кумков 
% попробует найти.

% Метрологиня: для стандартных образцов (о которых в нашей статье говорится) можно 
% привести ссылку на ГОСТ 8.531-2002 и ГОСТ Р 8.694-2010.
% \color{black}

% Для сравнения оценка массовой доли оксида кремния в стандартном образце 
% по методу наименьших квадратов и оценки границ доверительного интервала 
% $\pm2s$ равны соответственно 

% по подвыборке $W_1$:
% \begin{gather*} 
% \hat{w}_{\text{мнк}}(W_1) = 3.4220 \text{\% м.д.}, \\[2pt] 
% s(W_1) = 0.0380 \text{\% м.д.},\\[2pt] 
% \hat{w}_{\text{мнк}}(W_1) - 2s(W_1) = 3.3460 \text{\% м.д.}, \\[2pt] 
% \hat{w}_{\text{мнк}}(W_1) + 2s(W_1) = 3.4980 \text{\% м.д.};
% \end{gather*} 
% по полной выборке $W$:
% \begin{gather*} 
% \hat{w}_{\text{мнк}}(W) = 3.4483 \text{\% м.д.}, \\[2pt] 
% s(W) = 0.0777 \text{\% м.д.},\\[2pt] 
% \hat{w}_{\text{мнк}}(W) - 2s(W) = 3.2928 \text{\% м.д.}, \\[2pt] 
% \hat{w}_{\text{мнк}}(W) + 2s(W) = 3.6037 \text{\% м.д.}. 
% \end{gather*} 

Альтернативный путь построения оценки $\hat{\mbf{w}}$, опирающийся на предположение 
о недооценке погрешности $\epsilon$ всех измерений выборки $W$, рассмотрен 
в Примере~\ref{SiOExmpPart3} (с. \pageref{SiOExmpPart3}). 
\end{example} 
  
      
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Совместность выборки} 
\label{SetCompatibitiy} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\subsection{Определение и общие соображения} 
  
    
Сказанное в предшествующем параграфе мотивирует необходимость введения внутреннего 
свойства интервальной выборки, которое характеризует согласование её данных между 
собой: 
  
\begin{definition} 
Выборка $\{\,\mbf{x}_{k}\}_{k=1}^n$ называется \textsl{совместной}, если пересечение 
всех интервалов составляющих её измерений непусто, т.\,е.   
\index{совместная  выборка}\index{несовместная выборка} 
\begin{equation*} 
\bigcap_{1\leq k\leq n} \mbf{x}_{k} \;\neq\;\varnothing. 
\end{equation*} 
В противном случае, если пересечение всех интервалов $\mbf{x}_{k}$, $k = 1,\ldots,n$, 
является пустым, то выборка называется \textsl{несовместной}. 
\end{definition}
  
Свойство совместности характеризует саму выборку и, строго говоря, не связано напрямую 
с её свойством быть накрывающей выборкой, т.\,е. с включением ею истинного значения 
измеряемой величины (см. \S\ref{CoverMeasrSect}). Выборка может быть совместной, но 
ненакрывающей. Но если выборка накрывающая, то она, по всей видимости, должна быть 
совместной. Эквивалентная формулировка этого свойства: если выборка несовместна, то она, 
скорее всего, не является накрывающей.  Основываясь на этих соображениях, в практической 
обработке результатов измерений трудный анализ накрытия выборкой истинного значения 
можно заменить анализом её совместности, так как это удобнее и нагляднее (хотя и 
не вполне строго). 
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
     
\begin{figure}[htb]
\centering\small 
  \unitlength=1mm
  \begin{picture}(95,95)
    \put(0,0){\includegraphics[width=95mm]{pictures/CompCovDiag.png}}
    \put(31,92){\mbox{накрытие}} 
    \put(82,44){\mbox{совместность}} 
    \put(8,71){\mbox{$x^{\ast}$}} 
    \put(8,22){\mbox{$x^{\ast}$}} 
    \put(87,69){\mbox{$x^{\ast}$}} 
    \put(87,30){\mbox{$x^{\ast}$}} 
    \put(5,84){\mbox{a)}} 
    \put(5,40){\mbox{б)}}
    \put(49,84){\mbox{в)}} 
    \put(49,40){\mbox{г)}} 
\end{picture}
\caption{Диаграмма <<совместность-накрытие>>}
интервальной выборки.
\label{CompCovDiag} 
\end{figure} 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
Рис.~\ref{CompCovDiag} наглядно иллюстрирует возможные варианты комбинаций 
свойств накрытия и совместности выборки. Выборки a) и в) на этом рисунке --- 
накрывающие, а выборки б) и г) --- ненакрывающие. Кроме того, выборки в) и г) 
на Рис.~\ref{CompCovDiag} --- совместные, а выборки а) и б) --- несовместные. 
  
Если обрабатываемая выборка несовместна, то это может вызываться следующими причинами: 
\begin{list}{}{\leftmargin=14mm\itemsep=5pt\topsep=4pt\parsep=2pt} 
\item[(а)] 
неверно заданным значением величины неопределённости измерений $\r\mbf{x}_k$ для каких-то 
$k\in\{ 1,2,\ldots,n \}$, которое занижено в сравнении с фактическим значением 
неопределённости; 
\item[(б)] 
наличием в этой выборке выбросов (промахов), т.\,е. сбойных измерений; 
\item[(в)] 
невыполнением условий на измеряемую постоянную величину (её непостоянство и т.\,п.). 
\end{list} 
  
В случае (а) обработка выборки описана далее в пункте <<Коррекция величины 
неопределённости измерений выборки>> из \S\ref{UncertAlterSect}. 
  
В случае (б) выполняется дополнительный содержательный анализ обрабатываемой выборки. 
Например, если несовместность рассматриваемой выборки возникает из-за небольшого числа 
измерений (в пределе --- всего одного), удаление которых из выборки делает её совместной, 
то эти измерения следует признать подозрительными на выбросы. Другой приём --- выявление 
распадения выборки на совместные подвыборки (см. \S\ref{CompatibilityGraph}). 
  
В случае (в) можно пересмотреть модель явления и/или воспользоваться другими методиками 
обработки данных. Например, при наличии достаточных оснований от задачи определения 
постоянной величины можно перейти к решению задачи восстановления зависимости 
(см. Главу~\ref{FuncFitChap}) и т.\,п. 
     
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%     
   
\begin{example} 
\label{BazhenovExmp} 
Рассмотрим пример из практики с существенно неточным измерением значений постоянной 
физической величины, когда она должна устанавливаться в течение значительного времени. 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{figure}[ht]
\centering
%\includegraphics[width=0.8\textwidth]{pictures/TemperatureTestFill.eps}
\includegraphics[width=0.8\textwidth]{pictures/TempBlank.eps}
\caption{Измерения температуры помещения датчиками A и B в течение суток. Разница 
показаний датчиков лежит в пределах их декларированной погрешности. Данные датчиков 
совместны весь период измерений, множество $\mrm{A}\cap\mrm{B}$ всегда непусто.} 
\label{SensorTestPic} 
\end{figure}  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  

В лаборатории лазерной диагностики плазмы и взаимодействия плазмы с поверхностью 
Физико-технического института им.~А.Ф.\,Иоффе РАН (г.~Санкт-Петербург) создана 
и эксплуатируется установка \cite{PTE2021} для термовакуумных испытаний опытных 
образцов устройств для исследования плазмы токамаков. Одним из входных параметров 
испытаний является температура. Например, при исследованиях покрытий зеркал 
изучается зависимость 
\begin{equation*}
\delta = \delta(T, p),
\end{equation*}
где $\delta$ --- толщина покрытия образца, $T$ --- температура отжига образца, 
$p$ --- вектор других параметров. 
    
Для повышения надёжности, температура образца измеряется двумя платиновыми датчиками 
типа Pt100. Датчики идентичны, но скорость установления температуры конкретного датчика 
зависит от набора физических свойств конструкционных материалов (теплопроводность, 
теплоёмкость, плотность, коэффициент отражения излучения и другие) и параметров 
проведения электрических измерений (рабочий ток, постоянная интегрирования). 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
  
\begin{figure}[ht] 
\centering
\includegraphics[width=1\textwidth]{pictures/TempData.eps}
\caption{Измерения температуры двумя датчиками. На верхнем рисунке --- данные 
за весь период проведения эксперимента, около 2-х суток. Серым залит временной 
диапазон среднего графика. На среднем рисунке --- данные в окрестности стационарного 
режима, серая заливка --- временной диапазон нижнего графика. На нижнем рисунке 
--- данные за 2~часа проведения испытаний. Совместность показаний обоих датчиков, 
т.\,е. $\mrm{A}\cap\mrm{B}\neq\varnothing$, имеет место около 1 часа.} 
%после выхода на стационарный режим.}
\label{2SensorsPic} 
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
Для проверки готовности установки к работе необходимо убедиться в совместности показаний 
датчиков. Для этого предварительно проводят достаточно долговременные измерения 
в относительно стабильных условиях. На Рис.~\ref{SensorTestPic} приведены показания 
температуры двумя датчиками в течение суток. Разница показаний датчиков находится 
в пределах декларированной производителем погрешности датчиков, $\pm 0.35^{\circ}$C. 
Суточные колебания температуры отражают характер её вариации в конкретном лабораторном 
помещении, 25--29$^{\circ}$C. 
  
Рабочая температура проведения исследований существенно выше и достигает нескольких 
сотен $^{\circ}$C. В результате некоторого различия теплофизических и электрических 
параметров у разных датчиков, температуры, фиксируемые датчиками, реально никогда 
не бывают одинаковыми в процессе установления теплового равновесия. Характерная 
тенденция при этом такова: один из датчиков показывает более высокую скорость увеличения 
температуры при ее повышении, и более высокую скорость ее уменьшения в противоположной 
ситуации. Пример  результатов измерений температуры в процессе работы приведён 
на Рис.~\ref{2SensorsPic}. 
  
Верхний фрагмент рисунка показывает процесс изменения температуры в течение всего 
временн\'{о}го промежутка исследований. При выходе установки на температурный режим 
разница показаний датчиков составляет несколько десятков $^{\circ}$C. По достижении 
участка технологических испытаний показания стабильны и отличаются на несколько 
$^{\circ}$C. При снижении температуры численные значения показаний датчиков меняются 
местами: более <<холодный>> ранее датчик показывает более высокую температуру вплоть 
до окончательного приближения к комнатной. 
  
На нижнем фрагменте Рис.~\ref{2SensorsPic} представлен  интервал измерений после выхода 
на стационарный режим,  с 14 до 16 час. В этот период показания датчиков близки. Заливкой 
цветом показаны области  показаний датчиков, соответствующие паспортной погрешности 
измерений. На различных отрезках времени имеются примеры как совместных, так и 
несовместных показаний датчиков. 
\end{example} 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\begin{figure}[!ht]
\centering\small
  \unitlength=1mm
  \begin{picture}(110,87)
    \put(-5,0){\includegraphics[width=110mm]{pictures/GraviConstPic.eps}}
  \end{picture}
\caption{Результаты измерений гравитационной} 
константы согласно \cite{GraviConstItaly}. 
\label{GraviConstPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{example} 
Одной из самых известных и самых важных физических констант является гравитационная 
константа $G$, которая фигурирует в законе всемирного тяготения. История её измерений 
продолжается уже более двух столетий, и полученные на этом пути результаты нетривиальны. 
  
Измерения гравитационной константы, проведённые в последние десятилетия в различных 
исследовательских центрах и различными методами, выражаются доверительными интервалами, 
не пересекающимися друг с другом (Рис.~\ref{GraviConstPic}). Таким образом, если 
из полученных интервальных данных составить выборку, то она будет несовместной. 
  
Конкретные числовые данные, отображённые на Рис.~\ref{GraviConstPic}, и ссылки 
на источники их получения можно найти как в обзорных статьях на эту тему 
\cite{GraviConstChina, GraviConstItaly}, так и в электронном приложении 
\cite{IntervalAnalysisExamples} к настоящей книге. В этом приложении наряду 
с данными о гравитационной константе рассмотрены также другие примеры аналогичного 
свойства, которые относятся, в частности, к измерению константы Хаббла, времени 
жизни нейтрона, массе $t$-кварка и т.\,п.     %%  а попроще примеры имеются? 
\end{example}
  
Технология измерения постоянной величины по результатам интервальных наблюдений 
существенно зависит от того, является выборка накрывающей или ненакрывающей 
(см. \S\ref{CoverSampleProcSect} и \S\ref{NonCoverSampleSect}). 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\subsection{Граф совместности выборки} 
\label{CompatibilityGraph} 
  
Взаимное отношение интервальных измерений в выборке в смысле их совместности друг 
с другом удобно описывать \emph{графом совместности выборки}, \index{граф совместности 
выборки} а для исследования структуры выборки прибегать к развитым инструментам теории 
графов \cite{Zykov,Roberts}. 

Напомним, что \emph{граф} представляет собой математическую абстракцию системы 
произвольной природы, в которой между составляющими её объектами возникают парные 
связи. Граф есть совокупность двух множеств --- множества объектов, называемого 
\emph{множеством вершин}, и множества их пар, именуемого \emph{множеством рёбер}. 
Наглядно граф представляется диаграммой, где вершины отображаются в виде точек, 
а рёбра --- в виде отрезков (или дуг), соединяющих пары вершин. 
  
В графе совместности выборки $\mbf{X} = (\mbf{x}_{1}, \mbf{x}_{2}, \ldots, \mbf{x}_{n} )$ 
множество вершин образовано интервалами $\mbf{x}_{i}$ выборки $\mbf{X}$, а множество 
рёбер составлено из пар интервалов $(\mbf{x}_{j},\mbf{x}_{k})$, которые имеют непустое 
пересечение, то есть совместны друг с другом (Рис.~\ref{CompatGraphPic}). Графы, 
определённые таким способом по набору интервалов, в теории графов принято называть 
\emph{интервальными графами} или \emph{графами интервалов}. Они неплохо изучены и находят 
много приложений (см., к примеру, \cite{Roberts}). Кроме того, интервальные графы обладают 
рядом полезных свойств, которые упрощают решение различных задач.\index{интервальный граф} 
\index{граф интервалов}  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\begin{figure}[!ht]
\centering 
    \begin{subfigure}{.61\textwidth}
        \centering
        \subfile{pictures/SimpSamp}
    \end{subfigure} 
%%%%%%%%%%%%%%%%%%%%%%%        
    \hfill 
    \begin{subfigure}{.36\textwidth}
        \centering
        \subfile{pictures/SimpCompGraph}
        \begin{minipage}{.1cm}
        \vfill
        \end{minipage}
    \end{subfigure} 
%%%%%%%%%%%%%%%%%%%%%%%        
    \caption{Выборка и её граф совместности.}
    \label{CompatGraphPic}
\end{figure}
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
Поскольку в совместной выборке любые два интервала дают непустое пересечение, то 
в соответствующем ей графе совместности рёбрами соединены все возможные пары вершин. 
В теории графов такие графы называются \emph{полными}.\index{полный граф} Очевидно, что 
графы совместности для несовместных выборок полными не являются. В них отсутствуют рёбра, 
соединяющие пары вершин, соответствующие непересекающимся интервалам (в примере на 
Рис.~\ref{CompatGraphPic} --- пары интервалов $(\mbf{x}_{1},\mbf{x}_{2}), (\mbf{x}_{1}, 
\mbf{x}_{3}), (\mbf{x}_{1},\mbf{x}_{5})$ и т.п.).

Предметом интереса в несовместных выборках являются максимальные совместные подвыборки. 
Термин <<максимальная>> здесь означает, что такую подвыборку невозможно расширить 
какими-либо дополнительными интервальными измерениями выборки, при сохранении свойства 
её совместности. При этом несовместная выборка может содержать несколько максимальных 
совместных подвыборок различной или одинаковой длины. 

Нетрудно заметить, что в графе совместности выборки интервальных измерений совместным 
подвыборкам соответствуют полные подграфы, а в случае, когда совместная подвыборка 
максимальна, --- максимальный полный подграф. В теории графов максимальный полный 
подграф имеет специальное название --- \index{клика}.\index{клика} 
  
В выборке интервальных измерений, показанной на Рис.~\ref{CompatGraphPic}, можно выделить 
три максимальных совместных подвыборки и три соответствующие клики в графе совместности: 
$\mbf{X}_1 = (\mbf{x}_{1},\mbf{x}_{4}), \mbf{X}_2 = (\mbf{x}_{3},\mbf{x}_{4}, \mbf{x}_{5})$ 
и $\mbf{X}_3 = (\mbf{x}_{2},\mbf{x}_{3})$. 
  
Следует заметить, что граф совместности выборки может содержать и одиночные 
вершины, не соединённые ребрами с какими-то другими. Такие изолированные вершины также
представляют собой максимальные полные подграфы (клики) и соответствуют интервальным
измерениям выборки, не имеющим пересечений с прочими интервалами. Такие вершины и 
измерения могут рассматриваться как кандидаты в выбросы.

Задача о поиске максимальных совместных подвыборок для выборки интервальных измерений 
равносильна задаче построения всех максимальных клик в графе совместности выборки. 
Для решения задачи построения списка максимальных клик в графе, можно воспользоваться, 
либо классическим алгоритмом Брона -- Кербоша и его усовершенствованиями 
\cite{Christofides, BronKerbosch, EppsteinEtAl}, пригодными для произвольных графов, 
либо вычислительно более эффективными алгоритмами, учитывающими специфику интервальных 
графов \cite{Habib}. 

\begin{example} \label{SiOExmpPart1}
Анализируется выборка $\mbf{W} = ( \mbf{w}_1, \dots, \mbf{w}_7 )$ интервальных измерений 
массовой доли оксида кремния в стандартном образце состава титаномагнетитовой руды 
(Табл.~\ref{TabSiO}), которая представляет собой укороченный вариант данных из работы 
\cite{KumkovIgnatenkova}. Для краткости единицы измерения массовой доли далее будем 
обозначать <<\% м.д.>>. 
  
Интервальные измерения $\mbf{w}_k$ получены из базовых точечных значений $\mathring{w}_k$ 
добавлением интервала погрешности $[-\epsilon, \epsilon]$, где $\epsilon = 0.086$. 
На Рис.~\ref{SiOGraphPic} показаны диаграмма рассеяния выборки $\mbf{W}$ и граф 
её совместности. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
    
\begin{table}[htb]
  \centering
  \caption{Измерения массовой доли оксида кремния \\в стандартном образце материала}
  \label{TabSiO}
  \begin{tabular}{cccc} 
  \hline 
  \makecell{Номер\\ измерения, $k$} & $\mathring{w}_k$ & $\mbf{w}_k$ \\ \hline 
   1 & 3.421 & [3.335, 3.507]\\
   2 & 3.393 & [3.307, 3.479]\\
   3 & 3.404 & [3.318, 3.490]\\
   4 & 3.380 & [3.294, 3.466]\\
   5 & 3.456 & [3.370, 3.542]\\
   6 & 3.606 & [3.520, 3.692]\\
   7 & 3.478 & [3.392, 3.564]\\ \hline
  \end{tabular}
\end{table}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[ht]
    \centering 
    \begin{subfigure}{.61\textwidth}
        \centering
        \subfile{pictures/SiOSamp}
    \end{subfigure} 
%%%%%%%%%%%%%%%%%%%%%%%        
    \hfill 
    \begin{subfigure}{.37\textwidth}
        \centering
        \subfile{pictures/SiOCompGraph}
        \begin{minipage}{.1cm}
        \vfill
        \end{minipage}
    \end{subfigure} 
%%%%%%%%%%%%%%%%%%%%%%%        
    \caption{Выборка измерений массовой доли оксида кремния\\
             и соответствующий ей граф совместности.}
    \label{SiOGraphPic}
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Цель анализа состоит в построении оценки массовой доли оксида кремния в стандартном 
образце. Однако проверка выборки на совместность даёт отрицательный результат. Поэтому 
одним из направлений дальнейшего её анализа может быть более детальное изучение 
структуры совместности данных путем выделения максимальных совместных подвыборок, 
которые соответствуют максимальным кликам в графе совместности выборки. 

Алгоритм поиска максимальных клик в графе совместности выборки $W$ обнаруживает 
две максимальные клики, которые соответствуют подвыборкам 
\begin{equation*}
\mbf{W}_1 = (\mbf{w}_1, \mbf{w}_2, \mbf{w}_3, \mbf{w}_4,\mbf{w}_5,\mbf{w}_7) 
\qquad\text{ и }\qquad 
\mbf{W}_2 = (\mbf{w}_5,\mbf{w}_6,\mbf{w}_7). 
\end{equation*}
На Рис.~\ref{SiOCliquesPic} жирными линиями показаны максимальные клики $\mbf{W}_1$ 
и $\mbf{W}_2$ в графе совместности выборки $\mbf{W}$, а также диаграммы рассеяния 
соответствующих им максимальных совместных подвыборок. 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\begin{figure}[ht]
\centering 
%%%%%%%%%%%%%%%%%%%%%%% а) граф       
    \begin{subfigure}{.37\textwidth}
        \centering
        \subfile{pictures/SiOW1CompGraph}
        \begin{minipage}{.1cm}
        \vfill
        \end{minipage}
    \end{subfigure} 
%%%%%%%%%%%%%%%%%%%%%%% а) измерения
    \hfill 
    \begin{subfigure}{.61\textwidth}
        \centering
        \subfile{pictures/SiOW1Samp}
    \end{subfigure} 
    \vfill
%%%%%%%%%%%%%%%%%%%%%%% б) граф       
    \begin{subfigure}{.37\textwidth}
        \centering
        \subfile{pictures/SiOW2CompGraph}
        \begin{minipage}{.1cm}
        \vfill
        \end{minipage}
    \end{subfigure} 
%%%%%%%%%%%%%%%%%%%%%%% б) измерения
    \hfill 
    \begin{subfigure}{.61\textwidth}
        \centering
        \subfile{pictures/SiOW2Samp}
    \end{subfigure} 
%%%%%%%%%%%%%%%%%%%%%%%   

\caption{Максимальные клики $W_1$ (сверху),  $W_2$ (снизу) и соответствующие им 
совместные подвыборки измерений}
\label{SiOCliquesPic}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Состав подвыборок $\mbf{W}_1$  и $\mbf{W}_2$ указывает на то, что несовместность 
исходной выборки в целом может обуславливаться свойствами измерения $\mbf{w}_6$, 
несовместного с большинством прочих. Причинами существенных отличий $\mbf{w}_6$ могут 
быть грубая ошибка при проведении этого измерения либо недооценка погрешности измерений 
$\epsilon$. В первом случае построение целевой оценки величины имеет смысл продолжить 
по данным подвыборки $\mbf{W}_1$, то есть исключив выброс из рассмотрения. Во втором 
случае представление о степени недооценки погрешности может дать приём варьирования 
уровня неопределённости интервалов выборки (см. \S\ref{UncertAlterSect}). Дальнейший 
ход анализа по каждому из указанных путей будет описан в Примере~\ref{SiOExmpPart2} 
(с.~\pageref{SiOExmpPart2}) и Примере~\ref{SiOExmpPart3} (с.~\pageref{SiOExmpPart3}) 
соответственно. 
\end{example} 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\subsection{Индекс Жаккара} 
  
Индекс Жаккара --- известная мера схожести множеств \ldots
\index{индекс Жаккара} 

Для описания выборок, помимо оценок их размеров, желательно иметь дополнительную информацию о мере сходства элементов выборки. В п.~\ref{InteWidClass} был рассмотрен вопрос о классификации выборок в зависимости от соотношения ширин интервалов в выборке по отношению к их полной вариабельности.
При определении накрывающих выборок в п.~\ref{CoverMeasrSect} отмечалось, что понятие невозможно определить строго, поскольку жесткие требования к <<накрытию>> приводят к исключению из рассмотрения большинства практических ситуаций.

В различных областях анализа данных, биологии, информатике, в науках о Земле используют 
различные меры сходства множеств \cite{Jaccard}. Мера сходства бинарная: $S(A, B) \rightarrow 
[0, 1] $ --- это вещественная функция между объектами $A, B$. \index{IoU --- Intersection 
over Union} Формально  принадлежность к мерам сходства определяется набором свойств:
\begin{itemize}
    \item[---] ограниченность $0 \leq S(A, B) \leq 1 $;
    \item[---] симметрия $S(A, B) = S(B,A)  \leq 1$;
    \item[---] рефлексивность $S(A, B)=1  \  \Leftrightarrow \  A=B $;
    \item[---] транзитивность 
               $A\subseteq B\subseteq C \  \Rightarrow \  S(A, B) \geq S(A, C)$.
\end{itemize}
Существуют и иные системы аксиом сходства. \index{индекс Жаккара} \index{мера сходства} 
\label{JaccardMeasure} В компьютерных приложениях (обработка изображений, машинное обучение) 
меру сходства множеств обозначают как \emph{IoU} (\emph{Intersection over Union}). 
В математике для подобных конструкций часто используется термин \emph{индекс Жаккара}, 
по имени математика, предложившего подобную меру. 
  
В процессе развития интервального анализа были введены различные определения и конструкции 
оценки меры совместности интервальных объектов. Вместе с тем в практике обработки данных 
часто необходимо оперировать относительными величинами. В частности, это нужно в связи 
с необходимостью сопоставления допусков и размеров деталей, погрешности измерителей 
и значений измеряемых величин и т.\,п. \cite{Kabir2017}. 
  
Представим обобщение меры Жаккара на выборки интервалов \cite{Jaccard2022}. В качестве 
числовой характеристики степени совпадения двух интервалов $\mbf{x}, \mbf{y}$ рассмотрим  
величину 
\begin{equation}
\label{Rwid}
\Ji (\mbf{x}, \mbf{y}) := 
\frac{\w (\mbf{x} \wedge \mbf{y} )}{\w (\mbf{x} \vee \mbf{y})}.
\end{equation}
В выражении \eqref{Rwid} используется ширина интервала (см. стр. \pageref{InteWid}), 
а вместо операций пересечения и объединения множеств --- операции взятия  минимума 
($\wedge$) \eqref{InteMinExpr} и максимума ($\vee$) \eqref{InteMaxExpr} по включению 
двух величин в полной интервальной арифметике Каухера. В наименовании $\Ji (\mbf{x}, 
\mbf{y})$ буква $\textit{J}$ указывает на фамилию <<Jaccard>>, а $\textit{i}$ --- 
на интервальность его применения. В общем случае минимум по включению в выражении 
\eqref{Rwid} может быть неправильным интервалом. 
  
Рассмотренная мера обобщает обычное понятие меры совместности на различные типы 
взаимной совместности интервалов. Если пересечение интервалов $\mbf{x}, \mbf{y} $ 
пусто, иначе  $\mbf{x} \, \cap \, \mbf{y} = \varnothing$, то $\mbf{x} \, \wedge \, 
\mbf{y}$ --- неправильный интервал и числитель формулы \eqref{Rwid} имеет отрицательное 
значение. В предельном случае вещественных вырожденных интервалов $x \neq y$ имеем
\begin{equation*}
\Ji(x, y) = -1.
\end{equation*}
В целом получаем
\begin{equation}\label{Rwidrange}
-1 \leq \Ji (\mbf{x}, \mbf{y}) \leq 1.
\end{equation}
Таким образом, величина $\Ji$ непрерывно описывает ситуации от полной несовместности 
вещественных значений $x\neq y$ до полного перекрытия интервалов $\mbf{x} = \mbf{y}$. 
Следует заметить, что в отличие от случая вещественных величин, для которых для индекса 
Жаккара имеется только два значения, $0$ и $1$, формула \eqref{Rwid} даёт характеризацию 
различных отношений сходства интервалов с помощью непрерывного ряда значений. 
  
Мера совместности, введенная  для двух интервалов  в форме \eqref{Rwid}, допускает 
естественное обобщение в случае интервальной выборки. Пусть имеется интервальная 
выборка  $\mbf{X} = \{ \mbf{x}_i \}, \ i=1,2, \ \ldots, \ n$. Определим меру 
$\Ji(\mbf{X})$ как 
\begin{align} 
\label{RwidSetKR}
\Ji (\mbf{X}) = 
\frac{\w\bigl(\,\bigwedge_{i} \mbf{x}_{i}\bigr)}{\w\bigl(\,\bigvee_{i}\mbf{x}_i\bigr)}. 
\end{align}
Важно, что выражение \eqref{RwidSetKR} переходит в случае интервальной выборки 
из двух элементов в выражение \eqref{Rwid}. %Таким образом, принцип соотвествия выполнен.
  
\begin{example}{Пример вычисления меры совместности для накрывающей выборки.}
Пусть имеется интервальная выборка из четырех элементов \eqref{ModeExampleData}, 
рассмотренная при вычислении интервальной моды в п.~\ref{ModeSampleSect} 
\begin{equation*}
\mbf{X}   = \{ 
[1, 4],  [5, 9],  [1,5, 4,5],   [6, 9]   \}.
\end{equation*}	
Диаграмма рассеяния выборки $\mbf{X}$ приведена на рис. \ref{f:ModelData2hor}.
Выберем из нее накрывающую подвыборку
\begin{equation*}
\mbf{X}_{c}   = \{  
[5, 9],   [6, 9]  \}.
\end{equation*}	
Для выборки $\mbf{X}_{c}$  имеем, по формуле \eqref{RwidSetKR},
\begin{equation*}
\Ji(\mbf{X}_{c}) = \frac{9-6}{9-5} = 0.75.
\end{equation*}
Значение $\Ji(\mbf{X}_{c})$, близкое единице, демонстрирует высокую меру 
сходства элементов выборки $\mbf{X}$. 
\end{example}
  
В статье \cite{Jaccard2022} приводятся примеры различных применений обобщения 
индекса Жаккара. Также в \cite{Jaccard2022} предложен ряд модификаций формулы 
\eqref{RwidSetKR} на случай несимметричного вхождения интервалов. Для использования 
\textsl{Ji} в функционалах качества, предложен положительно определённый вариант 
индекса Жаккара. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\section{Мода интервальной выборки} 
\label{ModeSampleSect} 
  
  
В традиционной статистике важной характеристикой выборки является её \emph{мода} 
--- значение из выборки, которое встречается наиболее часто. Если же рассматривается 
случайная величина с непрерывным вероятностным распределением, то её мода --- это 
точка (или точки), в которых плотность вероятности имеет локальный максимум. Выборки 
и распределения с одной модой называются, как известно, \emph{унимодальными}, а 
с двумя и более модами --- \emph{мультимодальными} (бимодальными т.\,д.). 
\index{унимодальная выборка} \index{мультимодальная выборка} 
  
Мода лучше, чем среднее значение (математическое ожидание) характеризует выборки 
с большим разбросом значений. Кроме того, мода, как характеристика <<средней 
величины>>,  может применяться при обработке данных, имеющих нечисловую природу. 
    
Имеет смысл  распространить понятие моды на обработку интервальных данных, где она  
будет обозначать интервал тех значений, которые наиболее часты, т.\,е. встречаются 
в интервалах обрабатываемых данных наиболее часто. Фактически, это означает, что 
точки из моды интервальной выборки накрываются наибольшим числом интервалов этой 
выборки. Ясно, что по самому своему определению понятие моды имеет наибольшее значение 
(и наибольший смысл) лишь для накрывающих выборок. Иначе, если выборка ненакрывающая, 
то смысл <<частоты>> тех или иных значений в пределах рассматриваемых интервалов этой 
выборки в значительной мере теряется, хотя и не обесценивается. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
   
\begin{table}[!hp]
\centering
\caption{Алгоритм для нахождения моды\\ 
            интервальной выборки} 
\label{ModeAlgo} 
\vspace{2pt} 
\color{MyGreen} 
\fboxsep=3mm
\fboxrule=0.5pt
\fbox{\color{black} 
\begin{minipage}{100mm}
\centering 
\begin{tabbing}
A\= AAA\= AAA\= AAAA\= \hspace{4em}\= \kill
\>\>\>\>\> \hspace{9pt}\textsf{Вход}                                      \\[2mm] 
\> Интервальная выборка $\mbf{X} = \{\mbf{x}_{i}\}_{i=1}^n$ длины $n$.    \\[5mm] 
\>\>\>\>\> \hspace{6pt}\textsf{Выход}                                     \\[2mm]
\> Мода $\mode\mbf{X}$  выборки $\mbf{X}$ и её частота $\mu$.             \\[5mm] 
\>\>\>\>\> \textsf{Алгоритм}                                              \\[2mm] 
\> $\mbf{I}\, \gets \,\bigcap_{i=1}^n \mbf{x}_{i}$\,;                     \\[2mm]  
\> \texttt{IF} \ $\mbf{I}\neq\varnothing$ \  \texttt{THEN}                \\[1mm]  
\>\> $\mode\mbf{X} \gets \mbf{I}\,$;                                      \\[1mm]  
\>\> $\mu \gets n$                                                        \\[1mm]  
\> \texttt{ELSE}                                                          \\[1mm]  
\>\> помещаем все концы $\un{\mbf{x}}_{1}$, $\ov{\mbf{x}}_{1}$, 
     $\un{\mbf{x}}_{2}$, $\ov{\mbf{x}}_{2}$, \ldots, 
     $\un{\mbf{x}}_{n}$, $\ov{\mbf{x}}_{n}$                               \\[2pt] 
\>\> \quad интервалов рассматриваемой выборки $\mbf{X}$ в один            \\[2pt]  
\>\> \quad массив $Y = (\,y_{1}, y_{2}, \ldots, y_{2n})$\,;               \\[1mm] 
\>\> упорядочиваем элементы в $Y$ по возрастанию значений;        \quad   \\[2mm]
\>\> порождаем интервалы $\mbf{z}_{i} = [y_{i}, y_{i+1}]$, 
                                           $i = 1,2,\ldots,2n-1$  \quad   \\[2pt] 
\>\> (назовём их \emph{элементарными подинтервалами измерений});  \quad   \\[2mm]       
\>\> для каждого $\mbf{z}_{i}$ подсчитываем число $\mu_i$ интервалов      \\[2pt] 
\>\>\quad из выборки $\mbf{X}$, включающих интервал $\mbf{z}_{i}\,$;      \\[2mm] 
\>\> вычисляем $\displaystyle\;\mu\gets\max_{1\leq i\leq 2n-1}\mu_{i}\,$; \\[1mm]   
\>\> выбираем номера $k$ интервалов $\mbf{z}_k$, для которых $\mu_k$      \\[2pt] 
\>\> \quad равно максимальному, т.\,е. $\,\mu_{k} = \mu$, и формируем     \\[2pt]
\>\> \quad из таких $k$ 
                 множество $K = \{k\}\subseteq\{ 1,2,\ldots,2n-1\}\,$;    \\[2mm] 
\>\> $\mode\mbf{X}\, \gets \;\bigcup_{k\in K} \mbf{z}_{k}$                \\[3mm] 
\> \texttt{END IF} 
\end{tabbing} 
\end{minipage}} 
\end{table} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
Следуя работе \cite{HuCHuZH}, введём 
  
\begin{definition} 
\textsl{Модой} интервальной выборки назовём совокупность интервалов пересечения  
максимальных совместных подвыборок рассматриваемой выборки. Максимальная длина 
совместных подвыборок данной выборки называется \textsl{частотой моды}. 
\index{частота моды}   \index{мода выборки} 
\end{definition} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb]
\centering\small 
  \unitlength=1mm
  \begin{picture}(90,42) 
    \put(0,1){\includegraphics[width=90mm]{pictures/ModeAlgorithm.eps}}
    \put(4.5,38){\mbox{\small\begin{tabular}{l} 
                             Номер \\ измерения
                             \end{tabular}}} 
  \end{picture}
\caption{Вычисление моды интервальной выборки.} 
\label{ModeAlgoPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
    
Так как сама выборка, очевидно, является своей подвыборкой, то в случае её совместности 
мода совпадает с пересечением всех интервалов выборки. Если же выборка несовместна, 
то в ней может найтись несколько совместных подвыборок максимальной длины, и их 
пересечения нужно рассматривать в совокупности друг с другом. Как следствие, мода 
может быть мультиинтервалом (см.~\S\ref{MultiIntervalSect}). Это совершенно аналогично 
ситуации с обычными неинтервальными (точечными) данными, где выборка или распределение 
могут иметь несколько мод. Мы также будем использовать термины \emph{унимодальный}, 
\emph{бимодальный}, \emph{мультимодальный} по отношению к интервальным выборкам. 
  
Для вычисления моды интервальной выборки можно применить графовые алгоритмы, 
которые рассматривались в предшествующем \S\ref{CompatibilityGraph} и предназначены 
для нахождения максимальной совместной подвыборки. Мода является пересечением 
интервалов такой максимальной подвыборки, и если максимальных подвыборок имеется 
более одной, то мода будет объединением их пересечений, т.\,е. мультиинтервалом. 
Но можно предложить и более простой алгоритм вычисления моды интервальной выборки, 
который не опирается на нахождение максимальных совместных подвыборок. 
  
Псевдокод специализированного алгоритма для нахождения моды выборки интервальных 
измерений и её частоты приведён в Табл.~\ref{ModeAlgo}. Отметим также, что мода 
интервальной выборки --- это интервал или мультиинтервал, который не обязан совпадать 
с каким-либо из интервалов обрабатываемой выборки. 
  
\begin{example} 
\label{IModeExmp} 
Рассмотрим пример вычисления моды интервальной выборки из 4 элементов 
\begin{equation}
\label{ModeExampleData} 
\mbf{X}   = \{\;  [1, 4],  [5, 9],  [1.5, 4.5],   [6, 9] \;  \}.
\end{equation}	
Её диаграмма рассеивания приведена на Рис.~\ref{f:ModelData2hor}. 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
   
\begin{figure}[htb]
\centering\small  
\setlength{\unitlength}{1mm} 
\begin{picture}(100,54) 
\put(0,0){\includegraphics[width=100mm]{pictures/ModeData.png}}
\put(5,50){\mbox{Номер }} 
\put(5,47){\mbox{измерения}} 
\put(0,12){\mbox{1}} 
\put(0,21){\mbox{2}} 
\put(0,31){\mbox{3}} 
\put(0,40){\mbox{4}} 
\put(12,0){\mbox{1}} 
\put(16,0){\mbox{1.5}} 
\put(40,0){\mbox{4}} 
\put(44,0){\mbox{4.5}} 
\put(50,0){\mbox{5}} 
\put(59,0){\mbox{6}} 
\put(87,0){\mbox{9}} 
\put(95,5){$x$}
\put(14,6){$\mbf{z}_1$} 
\put(28,6){$\mbf{z}_2$} 
\put(73,6){$\mbf{z}_6$} 
\end{picture}
\caption{Диаграмма рассеяния интервальной выборки \eqref{ModeExampleData}} 
и элементы массива $\mbf{z}$. 
\label{f:ModelData2hor} 
\end{figure}
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


В соответствии с алгоритмом Табл.~\ref{ModeAlgo}, проверим совместность $\mbf{X}$. 
Пересечение элементов выборки пусто 
\begin{equation*} 
\mbf{I}\, = \,\bigcap_{i=1}^n \mbf{x}_{i} = \varnothing.
\end{equation*} 
Таким образом, необходимо выполнить шаги алгоритма Табл.~\ref{ModeAlgo} после 
ключевого слова \texttt{ELSE}. 

Сформируем массив  интервалов $\mbf{z}$ из концов интервалов $\mbf{X}$ 
\begin{equation}
\label{ModeExamplecarray} 
\mbf{z}   = 
\{ \  [1, 1.5], [1.5, 4],  [4, 4.5],  [4.5, 5], [5, 6],  [6, 9], [9, 9] \  \}. 
\end{equation}	
%Мощность $N$ массива $\mbf{z}$ равна $6$, и она меньше $2n-1 = 7$ из-за совпадения  правых концов интервалов $[5, 9]$ и $[6, 9]$. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{figure}[htb]
\centering\small  
\setlength{\unitlength}{1mm} 
\begin{picture}(100,33)
	\put(0,0){\includegraphics[width=100mm]{pictures/MuArray.png}}
\put(-3,32){$\mu_i$} 
\put(-3,12){\mbox{1}} 
\put(-3,22){\mbox{2}} 
\put(16,-1){\mbox{1.5}} 
\put(40,-1){\mbox{4}} 
\put(60,-1){\mbox{6}} 
\put(87,-1){\mbox{9}} 
\put(28,6){$\mbf{z}_2$} 
\put(73,6){$\mbf{z}_6$} 
\put(95,4){$x$}
\end{picture}	
\caption{Значения частот $\mu_i$ и интервальная мода $\mode\mbf{X}$}  
выборки \eqref{ModeExampleData}, а также элементы $\mbf{z}_k$, $k\in K$,  
массива $\mbf{z}$. 
\label{f:MuArray2} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
Для каждого интервала $\mbf{z}_{i}$ подсчитываем число $\mu_i$ интервалов из выборки 
$\mbf{X}$, включающих $\mbf{z}_{i}$, получаем массив значений $\mu_i$ в виде 
\begin{equation}
\label{muarray} 
 \{ 1,   2,   1,   0,   1,   2,   2 \}. 
\end{equation} 
%Как и в массиве \eqref{ModeExamplecarray}, в \eqref{muarray} подчеркнуты значения 
%для подинтервалов, которых нет в исходной выборке.
Mаксимальные $\mu_i$, равные $2$, достигаются для индексного множества 
\begin{equation*} 
K = \{ 2, 6, 7 \},
\end{equation*} 
так что частота моды равна $\mu =2$. Как итог, мода является мультиинтервалом 
\begin{equation}
\label{ModeIni} 
\mode\mbf{X}\, = \; \bigcup_{k \in K} \mbf{z}_{k}\, = [1.5, 4]\cup [6, 9] \cup[9, 9] 
 = [1.5, 4]\cup [6, 9]. 
\end{equation} 
  
На Рис.~\ref{f:MuArray2} значения частот $\mu_i$ из массива \eqref{muarray} показаны 
синим цветом, а интервальная мода $\mode\mbf{X}$ \eqref{ModeIni} --- красным цветом. 
\end{example}
  
Внимательный анализ данных Примера~\ref{IModeExmp} показывает, что выборка 
\eqref{ModeExampleData} содержит две максимальные совместные подвыборки, состоящие 
из измерений $1$ и $3$ и измерений $2$ и $4$. Их пересечения позволяет другим способом 
получить ту же моду интервальной выборки~\eqref{ModeExampleData} --- мультиинтервал 
$[1.5, 4]\cup[6, 9]$. 
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\section[Выборки унимодальные и мультимодальные]%
        {Выборки унимодальные \\* и мультимодальные} 
\label{UniMultiModSect} 
    
  
Тот факт, что выборка не является унимодальной, указывает на её неоднородность и может 
служить признаком сложной внутренней структуры описываемого ею явления. Получается, 
что из всего диапазона охватываемых выборкой значений выделяются тогда два или более 
изолированных друг от друга участка, доминирующих над близкими к ним значениями 
по частоте. Если это доминирование велико, то исследуемая величина может, к примеру, 
не быть постоянной, а является <<смесью>> нескольких близких постоянных величин. 
Интересное и содержательное обсуждение различных причин отсутствия унимодальности 
выборок и распределений на примере медицинских данных можно найти, в частности, 
в работе \cite{EAMurphy}. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[!htb]
\centering\small 
  \unitlength=1mm
  \begin{picture}(55,40) 
    \put(0,2){\includegraphics[width=55mm]{pictures/MultiMod.eps}}
    \put(52,1.5){$x$} %\put(-3,36){$p(x)$}  
  \end{picture}
\caption{Плотность вероятности бимодального распределения.} 
\label{BiModePic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
При обработке неинтервальных (точечных) данных распределения вида, показанного 
на Рис.~\ref{BiModePic}, обоснованно считаются уже не унимодальными, так как имеют 
более одного явно выраженного пика, хотя и разной высоты. Но в конструкциях 
предшествующего параграфа возможное различие частот подвыборок, на которые распадается 
исходная выборка, никак не учитывалось, и это является их существенным недостатком. 
Соответственно, необходимо распространить наши рассуждения на ситуации, когда интервальная 
выборка имеет несколько пиков частоты с разными значениями. Далее моду с наибольшей 
частотой будем называть \emph{основной модой} (или \emph{главной модой}). 
\index{основная мода} Прочие моды выборки будут называться \emph{неосновными}. 
\index{неосновная мода}\index{главная мода}  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Для нахождения мод мультимодальной выборки можно предложить дальнейшее развитие алгоритма, 
рассмотренного в предшествующем \S\ref{ModeSampleSect}. Эта процедура последовательно 
локализует моды выборки и запоминает соответствующие им множества индексов элементарных 
подинтервалов измерений $(\mbf{z}_{i})$. 
  
Выполнение процедуры начинается с нахождения массива $M$ частот $\mu_i$ по алгоритму 
Табл.~\ref{ModeAlgo} для заданной интервальной выборки. Вся последующая работа ведётся 
с полученным массивом. На шаге с номером $j$, $j = 1,2,\ldots$, в массиве $M$ зануляем 
значения, индексы которых соответствуют его основной моде. После этого находим максимум 
элементов в обновлённом массиве $M = (\mu_{i})$ и соответствующие ему значения индексов 
элементов массива (их может быть более одного). Для хранения этих индексов заводится 
специальный массив $K_j$. Этот процесс повторяется до полного зануления элементов массива 
$M$ или по иному критерию, определяемому постановкой конкретной задачи. Например, можно 
поставить цель найти только несколько первых мод. В результате строится последовательность 
массивов $K_j$, $j = 1,2,\ldots\,$, в которых хранятся индексы различных мод выборки, 
основной и неосновных. Наконец, по найденным наборам индексов максимумов $K_j$ строим 
массив мод исходной выборки. 
  
При конкретной реализации алгоритма может потребоваться его дополнительная настройка. 
Она может включать обработку шумов массива частот $M$, а также степень детальности 
описания мод. Специфика подобного усложнения алгоритма лежит уже вне тематики учёта 
интервальности данных. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section{Медиана интервальной выборки} 
   
   
Популярной характеристикой выборок в традиционной статистике является \emph{медиана}.  
\index{медиана} В работе А.В.\,Пролубникова предложен простой алгоритм вычисления 
медианы интервальной выборки 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\section{Обработка ненакрывающих выборок} 
\label{NonCoverSampleSect} 
  
Если выборка --- ненакрывающая, так что мы не можем гарантировать, что доминирующая 
часть её интервальных измерений содержит истинное значения измеряемой величины, то 
приведённые в предыдущих параграфах рассуждения и приёмы частично теряют свой смысл. 
Тогда при получении оценки постоянной величины необходимо руководствоваться другими 
соображениями. 
  
Поскольку кроме информации, представленной выборкой, в нашем распоряжении ничего нет, 
то следует бережно отнестись ко всем измерениям и считать, что каждое из них несёт 
существенную информацию об измеряемой величине, которая не должна быть потеряна. 
Уточнение пересечением здесь уже неуместно, и информационное множество для истинного 
значения величины имеет смысл взять в виде объединения всех интервалов выборки, 
т.\,е. как 
\begin{equation} 
\label{UnionInterval} 
\bigcup_{1\leq k\leq n} \mbf{x}_{k}. 
\end{equation} 
  
Это множество может не быть единым интервалом на вещественной оси (подобное часто 
случается, к примеру, если выборка несовместна). Разумно тогда воспользоваться 
вместо объединения обобщающей его операцией <<$\vee$>> (см.~\eqref{InteMaxExpr}), 
т.\,е. взятием максимума по включению, и вместо \eqref{UnionInterval} взять 
информационный интервал в виде 
\begin{equation} 
\label{UNInterval} 
\mbf{J}\; = \;\bigvee_{1\leq k\leq n} \mbf{x}_{k} \ 
= \  \Bigl[\,\min_{1\leq k\leq n} \un{\mbf{x}}_{k}, 
             \max_{1\leq k\leq n} \ov{\mbf{x}}_{k}\,\Bigr]. 
\end{equation} 
Этот способ обработки несовместной выборки рассмотрен, к примеру, в работе 
\cite{Artbauer}. Точечной оценкой измеряемой величины может служить середина 
полученного интервала, т.\,е. 
\begin{equation} \label{midUNInterval} 
x_\text{c} \  = \  \m\mbf{J} \   
= \  \tfrac{1}{2} \Bigl(\,\min_{1\leq k\leq n} \un{\mbf{x}}_{k} + 
                          \max_{1\leq k\leq n} \ov{\mbf{x}}_{k}\,\Bigr). 
\end{equation} 
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb]
\centering\small 
  \unitlength=1mm
  \begin{picture}(90,52)
    \put(0,0){\includegraphics[width=90mm]{pictures/ConstMeasNEncl.eps}}
    \put(75,5){\mbox{\small Номер измерения}} 
  \end{picture}
\caption{Обработка ненакрывающей выборки} 
интервальных измерений величины.
\label{NEnclConstPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
Как и ранее, нам может быть известен некоторый априорный интервал возможных значений 
оцениваемой постоянной величины $\mbf{J}_\text{апр} = [\un{\mbf{J}}_\text{апр}, 
\ov{\mbf{J}}_\text{апр}]$, который должен гарантированно содержать её. Его могут 
задавать внешние физические (химические, биологические, экономические и т.\,п.) условия 
или ограничения. Тогда границы результирующего интервала \eqref{UNInterval} могут быть 
уточнены пересечением 
\begin{equation}
\label{ImpUNInterval}
\mbf{J} = \mbf{J} \,\cap \,\mbf{J}_{\text{апр}}. 
\end{equation}                                     
В данной ситуации это уточнение имеет даже б\'{о}льший смысл, чем в случае накрывающей 
выборки. 
  
\begin{example} 
\label{ConstExmp} 
Рассмотрим неопределённость измерения нуля цифрового измерителя напряжения. 
Это явление может быть причиной возникновения интервальной неопределённости 
результатов измерений и ранее упоминалось в \S\ref{MeasuResultSect}. 
% в пункте <<Неопределённость измерения нуля>>. 

Пусть в качестве измерителя используется микросхема аналоговой памяти DRS4 для записи 
коротких сигналов \cite{DRS4}. Перед проведением основных измерений, необходимо 
определить неопределённость измерения нуля. Для этого на вход измерителя подают 
нулевое значение напряжения и получают выборку замеров. 

При дальнейшей обработке данных полученной таким образом выборки возможны различные 
варианты, соотносящиеся с шириной интервалов по отношению к разбросу средних значений, 
как это обсуждается в \S\ref{InteWidClass} в пункте <<Классификация по ширине 
интервалов>>. Дело в том, что измерение электрического сигнала может производиться 
с различной точностью, причём точность измерения может варьироваться  в весьма 
широких пределах. 
  
В цифровых измерителях напряжения для грубых измерений типичными являются измерители 
с 8 двоичными разрядами, что соответствует амплитудному разрешению, равному $1/2^8 
\cdot 100\% \simeq 0.4\% $. Для более точных измерений разрядность измерителя 
зависит от частоты проводимых измерений и варьируется от 10 ($\simeq 0.1 \%$) 
до 24 ($\simeq 10^{-5}\,\%$) двоичных разрядов.
% При этом следует обращать внимание на то, что формальная разрядность  
% измерителя может не соответствовать реальной точности. 
  
В \S\ref{MeasuResultSect}, где рассматривались измерения и их результаты, введено 
понятие модели погрешности измерений. В нашем конкретном случае можно в качестве 
модели измерения \eqref{GeneralErrorModel} принять выражение
\begin{equation} 
\mbf{x} = \mathring{x} + \mbf{\epsilon}, 
\end{equation}
где $\mathring{x}$ --- значение, выданное измерителем, а интервал погрешности взять 
уравновешенным интервалом  
\begin{equation}
\label{ErrorNOB}
\mbf{\epsilon} = [-\epsilon, \epsilon], \qquad \epsilon = \frac{1}{2^{\mathit{N\!O\!B}}},
\end{equation}
где $\mathit{N \!O\!B}$ (number of bits) --- разрядность измерителя.
При этом предполагается, что систематические погрешности отсутствуют или неизвестны. 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
     
\begin{figure}[htb]
\centering\small 
\unitlength=1mm
\begin{picture}(100,52) 
	\put(2,4){\includegraphics[width=0.75\textwidth]{pictures/ExampNonCovDataHi.png}} 	
\put(0,50){\mbox{\small Напряжение, В}} 
\put(88,43){\mbox{\small $ \max_{1\leq k\leq n} \ov{\mbf{x}}_{k}$}} 
\put(90,25){\mbox{\small $x_\text{c}  $}}	
\put(88,8){\mbox{\small $ \min_{1\leq k\leq n} \un{\mbf{x}}_{k}$}} 
\put(88,3){\mbox{\small Номер}} 
\put(88,0){\mbox{\small измерения}} 
\put(80,1){\mbox{\small 100}} 
\put(40,1){\mbox{\small 50}}
\put(-5,43){\mbox{\small 0.007}} 
\put(-5,8){\mbox{\small 0.001}} 
\end{picture}
\caption{Диаграмма рассеяния интервальных   измерений}
неопределённости нуля. Значение $\mathit{N\!O\!B} = 14$. 
\label{DRS4ZeroLine100cell1} 
\end{figure}  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
Пусть паспортная разрядность цифрового измерителя равна 14-ти двоичным разрядам.
На Рис.~\ref{DRS4ZeroLine100cell1} представлены данные для 100 измерений неопределённости 
нуля  $\left\lbrace \mathring{x}_k\right\rbrace _{k=1}^{100}$.
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[htb]
\centering\small 
\unitlength=1mm
\begin{picture}(100,55)
	\put(17,0){\includegraphics[width=0.6\textwidth]{pictures/HistExampNonCovDataHi.png}}
	\put(0,50){\mbox{\small Число}} 
	\put(0,47){\mbox{\small измерений}}
	\put(0,44){\mbox{\small в столбце}}
	\put(0,41){\mbox{\small гистограммы}} 
	\put(84,3){\mbox{\small Напряжение, В}} 
\end{picture}
\caption{Гистограмма интервальных измерений $\left\lbrace \mbf{x}_k\right\rbrace 
_{k=1}^{100}$ нуля.} Значение $\mathit{N \!O\!B} = 14$.
\label{HISTZeroLine} 
\end{figure}  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
По характеру данных, представленных на Рис.~\ref{DRS4ZeroLine100cell1}, видно, 
что для конкретной точности измерителя ширины интервалов отдельных измерений 
по модели \eqref{ErrorNOB} малы в сравнении с полным диапазоном значений в выборке. 

Как обсуждается в \S\ref{InteWidClass}, в такой ситуации надо обратить 
внимание на  характер пересечений пар результатов отдельных замеров $ \mbf{x}_i \cap 
\mbf{x}_j$. Из Рис.~\ref{DRS4ZeroLine100cell1} видно, что число непустых пересечений 
относительно невелико. В этом случае можно применять подходы и алгоритмы, которые 
используются для неинтервальных (точечных) данных. Весьма информативно построение 
гистограммы для выборки базовых значений $\left\lbrace\mathring{x}_k 
\right\rbrace _{k=1}^{100}$ или же для интервальных результатов $\left\lbrace\mbf{x}_k 
\right\rbrace _{k=1}^{100}$. 

Гистограмма для выборки  $\left\lbrace \mbf{x}_k\right\rbrace _{k=1}^{100}$  
на Рис.~\ref{HISTZeroLine} демонстрирует несимметричное распределение величины 
неопределённости нуля, и она непохожа на популярные теоретико-вероятностные 
распределения. В этой ситуации, чтобы не привносить в обработку данных необоснованные 
модельные представления, имеет смысл взять в качестве интервальной оценки искомой 
величины максимальный по включению интервал, содержащий все измерения. Эта оценка 
наиболее проста, учитывают всю доступную нам информацию об искомой величине и 
не обусловлена никакими дополнительными ограничениями. По этим причинам она может 
быть взята в качестве содержательного приближения измеряемой величины. 
     
Согласно формуле \eqref{UNInterval} получаем оценку информационного множества 
неопределённости нуля 
\begin{equation*} 
%\label{UNInterval} 
\mbf{J}\; = \;\bigvee_{1\leq k\leq n} \mbf{x}_{k} \ 
  = \  \Bigl[\,\min_{1\leq k\leq n} \un{\mbf{x}}_{k}, 
\max_{1\leq k\leq n} \ov{\mbf{x}}_{k}\,\Bigr] 
  = \left[ 0.73 \cdot 10^{-3}, 6.90 \cdot 10^{-3} \right].
\end{equation*} 
Точечная оценка нуля \eqref{midUNInterval} равна 
\begin{equation*}
x_\text{c} \  = \  \m\mbf{J} \   
= \  \tfrac{1}{2} \Bigl(\,\min_{1\leq k\leq n} \un{\mbf{x}}_{k} + 
\max_{1\leq k\leq n} \ov{\mbf{x}}_{k}\,\Bigr) = 3.82 \cdot 10^{-3}. 
\end{equation*} 

В целом вид диаграммы рассеяния на Рис.~\ref{DRS4ZeroLine100cell1} и гистограммы 
Рис.~\ref{HISTZeroLine}, свидетельствует о переоценке точности представления 
результатов измерения или недоучёте систематических погрешностей. Иначе говоря, 
модель ошибки \eqref{ErrorNOB}, включающая только погрешность квантования цифрового 
измерителя, не описывает корректно данные, и суммарная ошибка в каждом измерении 
реально больше. В таком случае, можно применить к выборке процедуру варьирования 
неопределённости, описанную ниже, в \S\ref{UncertAlterSect}, и добиться совместности 
данных (получить накрывающую выборку). 
\end{example}
  
Наконец, если ненакрывающая выборка интервальных измерений имеет большую длину, то 
имеет смысл взять среднее арифметическое образующих её интервалов, т.\,е. 
\begin{equation*} 
\label{AverIntval} 
\mbf{I} \  = \  \frac{1}{n}\;\sum_{k=1}^n \mbf{x}_{k}. 
\end{equation*} 
Известно, что усреднение помогает компенсировать случайные погрешности, и этот 
довод вполне применим к рассматриваемой ситуации. Середина полученного интервала 
\eqref{AverIntval} может служить точечной оценкой измеряемой величины. 
  
Нетрудно убедиться в том, что оба рассмотренных выше приёма обработки ненакрывающей 
выборки при стремлении ширины интервальных данных к нулю переходят в осмысленные методы  
оценивания постоянной величины по точечным данным. В частности, она полагается равной 
среднему арифметическому измерений выборки во втором случае. То есть, эти методы 
удовлетворяют <<принципу соответствия>>, рассмотренному ранее в \S\ref{CorresPrincpSect}. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\section{Вариабельность оценки} 
\label{ConstVariabSect}  
  
Рассмотрим теперь характеристики разброса оценок постоянной величины, полученных 
по интервальной выборке. Её наиболее естественной мерой, если информационный интервал 
непуст, является его \textit{радиус}, т.\,е. величина 
\begin{equation*}
\varrho\; = \;\r\mbf{I}\; = \;\tfrac{1}{2}\,\bigl(\ov{\mbf{I}} - \un{\mbf{I}}\bigr). 
\end{equation*} 
Фактически, это максимальное отклонение границ информационного интервала от центральной 
оценки. 
  
При анализе данных имеет также смысл знать отклонения точечных или интервальных 
измерений выборки от итоговой точечной оценки $\hat{x}$. Они дают возможность судить 
о степени разброса измерений относительно полученной оценки, что помогает при анализе 
<<качества>> выборки и выявлении выбросов (промахов). Эти отклонения $\Delta_k$ 
для первичных интервальных измерений рассчитываются как 
\begin{equation}
\label{MeasurDiffsInt} 
\Delta_k = \dist(\mbf{x}_{k}, \hat{x}),  \qquad  k = 1,\ldots,n, 
\end{equation} 
где dist --- расстояние между интервалами, определённое как \eqref{IntvalDist}. 
В совокупности они образуют вектор $\Delta = (\Delta_{k})_{k=1}^n$, который будем 
называть \emph{вектором рассеяния} выборки (или \emph{вектором разбросов}). 
\index{вектор рассеяния} \emph{вектор разбросов} 
    
В некоторых случаях имеет смысл отсчитывать отклонения от базовых точечных измерений, 
вокруг которых строятся далее интервальные результаты, т.\,е. рассматривать в качестве 
вектора рассеяния $\Delta$ выборки измерений вектор с компонентами 
\begin{equation}
\label{MeasurDiffs} 
\Delta_k = | \mathring{x}_k - \hat{x}|,  \qquad  k = 1,\ldots,n. 
\end{equation}  
  
Норма вектора $\Delta = (\Delta_{1},\dots,\Delta_{n})$ по своему назначению может 
служить, к примеру, аналогом выборочной дисперсии оценки из традиционной вероятностной 
статистики и т.\,п. 
  
\begin{example} 
\label{TwoSamplExmp} 
Для иллюстрации информации, которую несут различные нормы при оценивании 
вариабельности, рассмотрим обработку двух накрывающих выборок. 
   
Первая выборка состоит из одинаковых  измерений 
\begin{equation*}\label{EqualMeasSet}
\mbf{X}_1 = 
   \bigl(\,[1, \ 3],\, [1, \ 3],\,[1, \ 3],\, [1, \ 3] \,\bigr),
\end{equation*}
во второй выборке все значения разные
\begin{equation*}\label{NonEqualMeasSet}
\mbf{X}_2 = 
   \bigl(\, [1.5,\, 3.5], \, [0.5,\, 2.5],\, [1.5,\, 3.5],\, [2,\, 4] \,\bigr). 
\end{equation*} 
Ленточные диаграммы рассеяния измерений выборок представлены на Рис.~\ref{ModeNormEx}. 
Проведём их обработку по рассмотренной выше методике. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  

\begin{figure}[h!]
	\centering\small 
	\unitlength=1mm
	\begin{picture}(36,46) 
		\put(0,1){\includegraphics[width=36mm]{pictures/ModeNormExample1.eps}}
		\put(6,38){\mbox{\small\begin{tabular}{l} 
					номер \\ измерения
		\end{tabular}}} 
		\put(32,7){\mbox{$\mbf{x}_i$}}	
		\put(16,31){\mbox{$x_c$}}
		\put(-1,42){a)}
	\end{picture}
	\hspace{20mm} 
	\begin{picture}(36,46) 
		\put(0,1){\includegraphics[width=36mm]{pictures/ModeNormExample2.eps}}
		\put(6,38){\mbox{\small\begin{tabular}{l} 
					номер \\ измерения
		\end{tabular}}} 
		\put(32,7){\mbox{$\mbf{x}_i$}}
		\put(18,31){\mbox{$x_c$}}	
		\put(-1,42){б)}
	\end{picture}
	\caption{Диаграммы рассеяния интервальных выборок\\
    a) $\mbf{X}_1$ и б) $\mbf{X}_2$.	} 
	\label{ModeNormEx} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
Вычислим границы информационного интервала по формулам \eqref{LoUpBounds} 
для выборки $\mbf{X}_1$: 
\begin{equation*}
\label{LoUpBounds1} 
\un{\mbf{I}} \, = \,\max_{1\leq k\leq n} \,\un{\mbf{x}}_{k} = 1, 
\hspace{23mm} 
\ov{\mbf{I}} \, = \,\min_{1\leq k\leq n} \,\ov{\mbf{x}}_{k} = 3. 
\end{equation*}                    
Радиус информационного интервала равен
\begin{equation*}
\varrho\; = \;\r\mbf{I} \  = \ 
  \tfrac{1}{2}\,\bigl(\ov{\mbf{I}} - \un{\mbf{I}} \bigr) = 1. 
\end{equation*} 
Точечная оценка измеряемой величины по выборке $\mbf{X}_1$ равна 
\begin{equation*}
x_\text{c} \  = \  \m\mbf{I} \   
= \  \tfrac{1}{2} \Bigl(\,\min_{1\leq k\leq n} \un{\mbf{x}}_{k} + 
\max_{1\leq k\leq n} \ov{\mbf{x}}_{k}\,\Bigr) = 2. 
\end{equation*} 
Вектор рассеяния $\Delta$ для первичных интервальных измерений на основе 
расстояния между точечной оценкой и интервалами измерений вычисляем по формуле 
\eqref{MeasurDiffsInt}
\begin{equation*}
\Delta = \ \bigl( \ 1, \ 1, \ 1, \ 1 \  \bigr).
\end{equation*}
Для различных норм  вектора рассеяния  $\Delta = 
(\Delta_{1},\dots,\Delta_{n})$  получаем в соответствии с предложенными в \S\ref{MeasSetInte} определениями норм, учитывающими объём выборки, численные значения
\begin{equation*}
\|\Delta\|_1 = \|\Delta\|_2   =  \|\Delta\|_{\infty}= 1. 
\end{equation*}

Аналогично проведём вычисления для выборки $\mbf{X}_2$. Границы информационного интервала равны 
\begin{equation*}
	\label{LoUpBounds2} 
	\un{\mbf{I}} \, = \,\max_{1\leq k\leq n} \,\un{\mbf{x}}_{k} = 0.5, 
	\hspace{23mm} 
	\ov{\mbf{I}} \, = \,\min_{1\leq k\leq n} \,\ov{\mbf{x}}_{k} = 4. 
\end{equation*}                    
Радиус информационного интервала равен
\begin{equation*}
	\varrho\; = \;\r\mbf{I} \  = \ 
	\tfrac{1}{2}\,\bigl(\ov{\mbf{I}} - \un{\mbf{I}} \bigr) = 1.75.
\end{equation*} 
Точечная оценка измеряемой величины по выборке $\mbf{X}_2$ равна 
\begin{equation*}
	x_\text{c} \  = \  \m\mbf{I} \   
	= \  \tfrac{1}{2} \Bigl(\,\min_{1\leq k\leq n} \un{\mbf{x}}_{k} + 
	\max_{1\leq k\leq n} \ov{\mbf{x}}_{k}\,\Bigr) = 2.25. 
\end{equation*} 
Вектор рассеяния теперь равен
\begin{equation*}
\Delta = \  \bigl( \ 1.25, \ 1.75, \ 1.25, \ 1.75 \  \bigr).
\end{equation*}

В отличие от выборки $\mbf{X}_1$, различные нормы теперь несут различную информацию о мере вариабельности выборки:
\begin{equation*}
	 \| \Delta \|_1 = 1.5 <  \| \Delta \|_2 = 1.52 < \| \Delta \|_{\infty}= 1.75. 
\end{equation*}	
  
По поводу вычисления величины вариабельности справедливо такое математическое наблюдение. 
Как известно, возведение в степень, большую единицы, --- нелинейная функция, при действии 
которой относительно б\'{о}льшие аргументы получают б\'{о}льшие приращения. Кроме того, 
этот эффект нарастает при увеличении показателя степени. Как следствие, чем выше степень, 
в которую возводятся элементы вектора отклонений при вычислении его нормы, тем более 
значительную роль в окончательном результате --- величине вариабельности выборки --- 
играет максимальный элемент вектора. Для оценки вариабельности выборки через норму 
$\| \Delta \|_1$ влияние максимального элемента вектора отклонения минимально, а 
в предельном случае оценка вариабельности через норму $\| \Delta \|_{\infty}$ равна 
максимальному отклонению. 
\end{example} 
  
Таким образом, вычисление векторов рассеяния и их различных норм позволяют 
получать аналоги мер рассеяния (разброса) из теоретико-вероятностной статистики, 
а также просто (но и грубо) оценивать вариабельность выборок. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\section[Критические соображения П.Е.\,Эльясберга и их разбор]% 
        {Критические соображения\\* П.Е.\,Эльясберга и их разбор} 
  
  
П.Е.\,Эльясберг в известной книге \cite{Eliasberg83} рассматривает и комментирует 
(в \S 2.10) технику определения постоянной величины по интервальным результатам 
измерений, которая совпадает с описанной выше в  \S\ref{CoverSampleProcSect}. 
В \cite{Eliasberg83} разбирается пример обработки выборки из четырёх измерений 
$\tilde{q}_1$, $\tilde{q}_2$, $\tilde{q}_3$ и $\tilde{q}_4$, которые имеют погрешности 
$\delta_i$, $i\in\{1,2,3,4\}$. Соответственно, мы имеем интервальные результаты 
измерений, обозначенные 
\begin{equation*} 
[A_{1}, B_{1}], \qquad [A_{2}, B_{2}], \qquad [A_{3}, B_{3}],  \qquad [A_{4}, B_{4}], 
\end{equation*} 
так что $A_{i} = \tilde{q}_{i} - \delta_{i}$, $B_{i} = \tilde{q}_{i} + \delta_{i}$, 
$i\in\{1,2,3,4\}$. Изложение иллюстрируется рисунком, аналогичным нашему 
Рис.~\ref{EncloConstPic}. Восхищаясь вначале простотой и привлекательностью интервальной 
методики, П.Е.\,Эльясберг затем подвергает её развёрнутой критике. Так как аргументы 
П.Е.\,Эльясберга довольно типичны для суждений об интервальных методах обработки 
данных, мы воспроизводим полностью соответствующую цитату из книги \cite{Eliasberg83} 
в следующем абзаце. 
  
<<Однако в действительности всё не так хорошо, как это кажется на первый взгляд. Дело 
в том, что предлагаемый алгоритм чрезвычайно чувствителен к аномальным измерениям. 
Действительно, достаточно появиться хотя бы одному из них, отклоняющемуся от истинного 
значения $q$ больше чем на $\pm\delta_i$, чтобы возникла опасность отказа всего алгоритма. 
Так, в рассмотренном примере аномальное измерение $\tilde{q}_4$ делает невозможным 
нахождение отрезка, общего для всех интервалов $[A_{1}, B_{1}]$, $[A_{2}, B_{2}]$, 
$[A_{3}, B_{3}]$ и $[A_{4}, B_{4}]$. А если таких измерений несколько? Может возникнуть 
невероятная путаница! Насколько нам известно, пока не найдено эффективного выхода из этого 
положения. Конечно, можно просто увеличить величины $\delta_i$, но это приведёт к резкому 
ухудшению точности получаемой оценки. И вообще этот алгоритм слишком сильно зависит от 
величин $\delta_i$, назначаемых в значительной мере произвольно. Достаточно ошибиться 
в их выборе, в ту или иную сторону, чтобы он либо вообще не работал, либо давал слишком 
грубую оценку. В связи с этим он на практике почти не используется>>. 
  
Эти рассуждения П.Е.\,Эльясберга об интервальном методе оценивания постоянной величины 
слабо обоснованны и довольно поверхностны. Фактически, они являются следствием неполноты 
рассматриваемой им версии интервального метода, а также некоторых априорных требований 
к методам обработки данных, которые не универсальны и не могут быть автоматически 
применены ко всем методам. Ну а то, что в 1981 году, к моменту написания книги 
\cite{Eliasberg83}, рассмотренный П.Е.\,Эльясбергом алгоритм <<на практике почти 
не использовался>>, следует отнести на счёт незнания его полной версии (описанной 
выше в \S\ref{CoverSampleProcSect} и  \S\ref{NonCoverSampleSect}), а также общей 
неразвитости интервального анализа той эпохи. 
  
Прежде всего, нельзя не отметить недопустимую гиперболизацию последствий увеличения 
погрешности: <<\ldots можно просто увеличить величины $\delta_i$, но это приведёт 
к резкому ухудшению точности получаемой оценки>>. Написано красиво, но неадекватно. 
  
Почему именно <<резкому>>? Как следует из формул \eqref{UNInterval} и \eqref{IncluMin},  
точность получаемой оценки ухудшится ровно настолько, насколько мы увеличим величины 
$\delta_i$, и если мы не будем делать этого <<резко>>, то и точность оценки резко 
не ухудшится. Поэтому приведённая выше устрашающая фраза П.Е.\,Эльясберга просто 
не соответствует истинному характеру ухудшения точности оценки. Она имела бы смысл, 
если бы результирующая погрешность процедуры являлась быстрорастущей функцией от 
погрешностей $\delta_i$, но это не так. Результат имеет погрешность, не превосходящую 
всего лишь $\max \delta_{i}$. 
   
То же самое относится к последней мысли цитированного отрывка: достаточно ошибиться 
в выборе $\delta_i$ в ту или иную сторону, <<чтобы алгоритм либо вообще не работал, 
либо давал слишком грубую оценку>>. Насчёт <<не работал>> мы поговорим ниже, а вот 
<<слишком грубая оценка>> попросту невозможна, если мы не увеличиваем  $\delta_i$ 
слишком сильно. 
  
Рассмотрим теперь обвинение в том, что <<\ldots алгоритм чрезвычайно чувствителен 
к аномальным измерениям>>. Здесь следует начать с того, что П.Е.\,Эльясберг 
не различает накрывающие и ненакрывающие интервальные измерения, накрывающие 
и ненакрывающие выборки, т.\,е. все те тонкие моменты, которые были разобраны 
в  \S\ref{CoverMeasrSect}. Кроме того, приёмы обработки выборок, описанные нами выше 
в \S\ref{CoverSampleProcSect} и \S\ref{NonCoverSampleSect}, дополняют другу друга и 
образуют одну общую методику, которую надлежит применять в общем случае целиком, 
как единую технологию. П.Е.\,Эльясберг в книге \cite{Eliasberg83}, фактически, 
рассматривает только половину этой методики, изложенную в \S\ref{CoverSampleProcSect}, 
и относительно неё он делает свои суровые выводы. Но ведь вторая половина, 
из  \S\ref{NonCoverSampleSect}, тоже вносит свой вклад в обработку данных, включаясь 
в нужный момент, и без неё интервальная технология неполна. Более точно, оценка 
величины получается даже тогда, когда выборка ненакрывающая и даже несовместная, 
тогда как П.Е.\,Эльясберг отсутствие совместности выборки называет <<отказом>>, 
говоря, что при этом алгоритм <<вообще не работает>> и т.\,п. Это, конечно, в корне 
неверно. Никаких отказов общий алгоритм (опирающийся на использование неправильных 
интервалов) не допускает, и оценка постоянной величины будет получена в нём всегда. 
  
Далее, почему свойство алгоритма обработки данных <<быть чувствительным к аномальным 
измерениям>> является, согласно П.Е.\,Эльясбергу, его недостатком? (Усилительный эпитет 
<<чрезвычайно>>, употреблённый в книге \cite{Eliasberg83}, мы опускаем как лирику, 
не относящуюся к сути дела.) Алгоритм обработки данных честно докладывает нам 
о появлении аномального измерения, а это объявляется недостатком? Хорошо видно, 
что П.Е.\,Эльясберг в своём суждении опирается на неявное допущение, что <<хороший>> 
алгоритм должен <<быть нечувствительным к аномальным измерениям>>. Откуда оно появилось? 
И имеет ли смысл вообще? Да, это требование иногда имеет смысл, но оно не безусловно, 
и должно быть отнесено к тем качествам методик обработки данных, которые при определённых 
условиях абсолютно необходимы, а в других условиях могут оказаться крайне нежелательными. 
  
Строго говоря, отмеченное П.Е.\,Эльясбергом свойство <<чувствительности к аномальным 
измерениям>> является общей чертой методов приближения и сглаживания данных, которые 
опираются на идею равномерного (чебышёвского, минимаксного) приближения, когда мы 
стремимся минимизировать максимальное отклонение результата от обрабатываемых данных 
\cite{Akhiezer,Remez}. Эти методы давно (полтора века) с успехом используются в различных 
приложениях, обсуждаемая их особенность хорошо известна и уже мало кого должна смущать. 
Более того, в некоторых технических приложениях эта особенность методов равномерного 
приближения является критической, которая требует применение только их и никаких других. 
  
Если же мы хотим, чтобы алгоритм обработки данных не был <<чувствителен к аномальным 
измерениям>>, то он неизбежно должен опираться на идею усреднения результатов, когда 
аномальность в одних измерениях компенсируется малыми погрешностями в других. Но тогда 
с таким же пафосом можно обвинить алгоритмы, опирающиеся на усреднение измерений в том, 
что они <<скрывают>> или <<маскируют>> появление аномальных измерений и делают их 
труднообнаруживаемыми! 
  
Нашим рассуждениям можно придать несколько другую точку зрения. <<Аномальные измерения>>, 
о которых говорит П.Е.\,Эльясберг в \cite{Eliasberg83}, есть не что иное, как выбросы 
или промахи, которые мы обсуждали в \S\ref{OutlierSect} и которые будут рассмотрены в 
\S\ref{RegrOutlSect}. Они могут иметь совершенно разную природу, и их выделяют из общего 
массива измерений, как правило, неформально, на специальных этапах предобработки или 
постобработки данных, с опорой на знание специфики именно данного конкретного случая. 
Требовать от формального алгоритма выявления выбросов можно, лишь подавая ему на вход 
эту неформальную информацию (или априори встраивая её в алгоритм). Учитывая, что такая 
информация не может быть универсальной и пригодной для всех возможных случаев, нельзя 
надеяться, что какой-то один алгоритм сможет обеспечить выявление выбросов (<<аномальных 
измерений>>) во всех практических ситуациях. 
  
Наконец, нельзя пройти мимо заявления П.Е.\,Эльясберга о том, что величины $\delta_i$ 
назначаются <<в значительной мере произвольно>>. Оно лишь отчасти соответствует 
действительности, так как в большинстве практических ситуаций мы всегда можем хоть 
как-то оценивать точность выполняемых измерений, и совершенно произвольной она быть 
не может. Кроме того, для всякой выборки мы можем также оценивать величину погрешности 
измерений, необходимую для достижения совместности выборки (см. следующий параграф). 
Это позволяет осуществлять контроль точности измерений с другой стороны, тоже 
ограничивая произвол выбора $\delta_i$.  
  
В заключение темы стоит отметить, что значительная часть рассуждений и выводов 
П.Е.\,Эльясберга в книге \cite{Eliasberg83} по поводу интервальных методов обработки 
данных очень схожа с парадоксами интервальной статистики, которые подробно разбираются 
и объясняются далее в \S\ref{IStatParadoxesSect}. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section[Приём варьирования величины неопределённости]%
        {Приём варьирования \\*  величины неопределённости} 
\label{UncertAlterSect} 

Выше мы видели, что величина реальной неопределённости измерения, т.\,е. радиуса 
интервала измерения, определяется непросто и подчас неоднозначно. С другой стороны, 
она сильно влияет на свойства как отдельного измерения, так и выборки интервальных 
измерений. Совместность выборки и свойство накрытия истинного значения для большинства 
способов получения интервальных измерений (см. \S\ref{MeasuResultSect}) существенно 
зависят от правильно назначенной величины неопределённости --- радиуса интервальных 
измерений. 
  
Фундаментальный факт, касающийся интервальных измерений состоит в том, что если 
некоторое $\Delta$ является величиной неопределённости накрывающего интервального 
измерения или выборки, то и любое $\Delta'$, удовлетворяющее $\Delta' \geq\Delta$, 
также может служить величиной неопределённости.  
   
Сказанное выше приводит к мысли о том, что при обработке интервальных данных величиной 
неопределённости можно управлять, виртуально варьируя её, с целью исследования 
интервальных измерений и их выборок. С другой стороны, такое варьирование величины 
неопределённости может помочь в построении оценок с нужными свойствами. В этом 
и состоит суть <<приёма варьирования неопределённости>>, вынесенного в заголовок 
этого параграфа.       \index{приём варьирования неопределённости} 
  
Если выборка интервальных измерений несовместна, то, увеличивая одновременно величину 
неопределённости всех измерений, мы всегда сможем добиться того, чтобы выборка 
сделалась совместной, т.\,е. чтобы пересечение \eqref{IXInterval} стало непустым, 
а интервал \eqref{IncluMin} --- правильным. Кроме того, точка (или точки), которая 
первой появляется в непустом пересечении \eqref{IXInterval} при расширении интервальных 
измерений, и тем самым требует наименьшего увеличения неопределённости измерений 
для достижения  совместности выборки, является <<наименее несовместной>>. 
Её разумно брать в качестве оценки величины (или оценки параметров зависимости). 
  
Наоборот, если некоторая выборка интервальных измерений совместна, то можно равномерно 
сужать образующие её интервалы до возникновения несовместности. Последняя точка 
(или точки), которая остаётся в непустом информационном множестве перед его исчезновением, 
очевидно, имеет наибольший <<запас совместности>>. И снова эта точка является наилучшим 
кандидатом на оценку рассматриваемой величины, т.\,к. обладает наибольшей совместностью. 
    
В реальности часто встречаются ситуации, когда измерения выборки неравнозначны друг 
другу по качеству, к примеру, являются существенно неравноширинными. Тогда одновременное 
изменение величины неопределённости для всех измерений на одно и то же значение может 
оказаться неразумным. Неравноценность различных измерений можно учесть с помощью задания 
положительного весового вектора $w = (w_{1}, w_{2}, \ldots, w_{n})$,  $w_{k} > 0$, 
размерность которого равна длине исследуемой выборки. Тогда изменение величины 
неопределённости $k$-го измерения --- $\r\mbf{x}_k$, должно быть пропорциональным 
$w_k$, т.\,е. для любых $k, l\in\{1,2,\ldots,n\}$ справедливо 
\begin{equation*} 
\frac{\text{изменение} \ \r\mbf{x}_k}{\text{изменение} \ \r\mbf{x}_l} = \frac{w_k}{w_l}. 
\end{equation*} 
В частности, назначая некоторым измерениям большие, а другим --- очень малые веса, можно 
смоделировать ситуацию, когда для некоторых из измерений выборки может назначаться запрет 
на варьирование величины их неопределённости. Основанием для этого может служить, например, 
присутствие в выборке хорошо проверенных измерений, не оставляющих у  исследователя сомнений 
в их надёжности. 
  
Идея варьирования величины неопределённости интервальных измерений оформилась в 80-е 
годы XX века (Н.М.\,Оскорбин \cite{Oskorbin1983} и др.), и далее неоднократно 
переоткрывалась различными исследователями.  %% Kearfott с китайцем ?  Warwick Tucker ?  
Следует чётко осознавать виртуальный статус экспериментов с варьированием  уровней
неопределённости измерений. Очевидно, что получаемые в экспериментах оценки величин 
неопределённости и соответствующие им оценки искомых величин носят в известной мере 
условный характер, то есть не могут приниматься автоматически, без содержательной 
проверки. Скорее, они должны рассматриваться как полезные характеристики совместности 
данных и используемых моделей. В совокупности с прочей априорной информацией и 
прикладными знаниями они позволяют выдвигать те или иные предположения по поводу 
анализа обрабатываемых данных и вырабатывать его конкретные шаги.  
      
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   
\paragraph{Коррекция величины неопределённости измерений выборки.}   
Из предшествующих рассуждений вытекает одна из возможных процедур коррекции величины 
неопределённости измерений. В случае обнаружения несовместности обрабатываемой 
выборки производим варьирование (увеличение) исходной величины её неопределённости 
до такого <<критического>> значения $r^\ast$, при котором информационный интервал 
\eqref{IXInterval} становится непустым. Далее выполняем анализ этой величины: 
\begin{itemize}
\item[$\color{blue}\bullet$] 
если $r^\ast$ не превышает некоторое априорное приемлемое значение $r^{\text{апр}}$, 
т.\,е. $r^{\ast} \leq r^{\text{апр}}$, то рабочее значение неопределённости выборки 
корректируется до этого практически приемлемого значения $r^{\ast}$ и обработка 
выборки повторяется; 
\item[$\color{blue}\bullet$]  
если значение $r^{\ast}$ является практически неприемлемым, то из несовместности 
исследуемой выборки следует, что она, скорее всего, содержит выбросы и её нужно 
подвергнуть дополнительному содержательному анализу. 
\end{itemize}  
Содержательный анализ выборки может состоять, в частности, в выявлении её распадения 
на совместные подвыборки при некоторой приемлемой величине неопределённости измерений
и пр. 

\begin{example} 
\label{SiOExmpPart3} 
Описанная в Примере~\ref{SiOExmpPart1} (с.~\pageref{SiOExmpPart1}) выборка интервальных 
измерений массовой доли оксида кремния в стандартном образце материала несовместна. 
Предположение о том, что причиной несовместности является выброс в шестом измерении, 
отработано в Примере~\ref{SiOExmpPart2} (с. \pageref{SiOExmpPart2}), где построена 
оценка содержания оксида кремния в стандартном образце по максимальной совместной 
подвыборке, не содержащей выброса $\mbf{w}_6$. 

Еще одной заслуживающей внимания версией причины несовместности исходной выборки 
может быть недооценка фактической величины погрешности измерений, верхняя граница  
$\epsilon$ которой  согласно исходным данным равна $0.086\%$~м.~д. Прибегая к приёму
варьирования неопределённости интервальных измерений, отыщем минимальное значение 
$\epsilon^{*}$, обеспечивающее совместность выборки.

Учитывая единое происхождение, одинаковые условия измерений содержания оксида 
кремния и уравновешенность интервалов их погрешностей, в данном случае варьирование 
неопределённости сводится к её синхронному увеличению для каждого измерения 
до появления общей точки в пересечении интервалов измерений 
$\mbf{w}_k,$ $k = 1, \dots, 7$ (достижения непустоты информационного множества). 
Такой точкой оказывается $\tilde{w} = 3.493$\% м.д. при достижении верхней границей 
погрешности $\epsilon$ значения $\tilde{\epsilon} = 0.113$\% м.д. Диаграмма рассеяния 
исходных измерений и измерений с увеличенными интервалами погрешности 
$[-\tilde{\epsilon}, \tilde{\epsilon}]$ приведена на Рис.~\ref{SiOVarUncertPic}.

%% Оригинальный текст С.И.Кумкова
%Очевидна несовместность выборки в целом при заданном максимальном уровне погрешности 
%0.086\% массовой доли оксида кремния. Несовместность обусловлена пустотой пересечения 
%интервалов 4-го и 6-го измерений. Оценка фактического минимально допустимого (для 
%совместности выборки) уровня погрешности измерений составляет 0.113\% м.д. Данная 
%оценка находится процедурой варьирования ограничения на максимальный уровень 
%погрешности. Первая точка, появляющаяся в информационном множестве, --- 
%$\hat{w} = 3.493$\% м.д. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
     
\begin{figure}[!htb]
\centering 
\subfile{pictures/SiOSampExpand}
\caption{Измерения массовой доли оксида кремния\\ с разными уровнями неопределённости.}
\label{SiOVarUncertPic}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Следует подчеркнуть, что при $\epsilon = \tilde{\epsilon}$ попытка вычисления оценки 
$\hat{\mbf{w}}$ содержания оксида кремния в стандартном образце по формуле 
\eqref{IXInterval}, даёт интервал $\hat{\mbf{w}} = [\tilde{w}, \tilde{w}] = [3.493, 3.493]$ 
нулевой ширины, т.е. оценку с нулевой неопределённостью. В большинстве 
случаев оценка с нулевой неопределённостью является практически невозможной, а это 
означает, что для получения правдоподобного и практически приемлемого результата 
оценивания значение $\epsilon$ должно быть заметно больше найденного теоретического 
минимума $\tilde{\epsilon}$.

В данной конкретной задаче оценивания столь значимая недооценка погрешности 
измерений маловероятна и более правдоподобным представляется предположение о наличии 
выброса в данных. Если тщательная проверка подтвердит это предположение, то в качестве 
финального решения задачи оценивания следует выбрать оценку, полученную после удаления 
выброса.
\end{example} 
  
\begin{example} \label{VarExmp}
Ранее, в Примере~\ref{ConstExmp}, была рассмотрена задача нахождения неопределённости 
нуля в цифровом измерителе напряжения. Мы получили интервальную оценку искомой величины, 
построили гистограмму данных. 
    
Рассмотрим теперь, что было бы при более низкой точности цифрового измерителя, 
чем в Примере~\ref{ConstExmp}.  Пусть она равна 9 двоичным разрядам. 
На Рис.~\ref{DRS4ZeroLine100cell2} представлены интервальные результаты измерения 
нуля с теми же  данными $\left\{\mathring{x}_k\right\}_{k=1}^{100}$. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
  
\begin{figure}[htb]
\centering\small 
\unitlength=1mm
	\begin{picture}(100,50)
	\put(1,2){\includegraphics[width=0.75\textwidth]{pictures/ExampNonCovDataLo.png}}
	\put(5,44){\mbox{Напряжение, В}} 
	\put(88,40){\mbox{$\max_{1\leq k\leq n} \ov{\mbf{x}}_{k}$}} 
	\put(90,24){\mbox{$x_\text{c}$}}	
	\put(88,7){\mbox{$\min_{1\leq k\leq n} \un{\mbf{x}}_{k}$}} 
	\put(88,2){\mbox{Номер}} 
	\put(88,-1){\mbox{измерения}} 
	\put(42,0){\mbox{$50$}}
	\put(81,0){\mbox{$100$}} 	
	\put(-5,41){\mbox{$0.009$}} 
	\put(-7,7){\mbox{$-0.001$}} 
	\end{picture}
\caption{Диаграмма рассеяния интервальных измерений}
неопределённости нуля. Значение $\mathit{N\!O\!B} = 9$. 
\label{DRS4ZeroLine100cell2} 
\end{figure}  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
В случае измерений с большей интервальной неопределённостью характер попарной 
совместности измерений выборки существенно изменяется. Для многих пар замеров 
соответствующее пересечение $\mbf{x}_i \cap \mbf{x}_j$ становится уже непустым. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\begin{figure}[h!]
\centering\small 
\unitlength=1mm
\begin{picture}(100,55)
%	\put(17,0){\includegraphics[width=0.6\textwidth]{pictures/HISTZeroLineResolution=9.png}}
	\put(17,0){\includegraphics[width=0.6\textwidth]{pictures/HistExampNonCovDataLo.png}}
	\put(0,50){\mbox{\small Число}} 
	\put(0,47){\mbox{\small измерений}}
	\put(0,44){\mbox{\small в столбце}}
	\put(0,41){\mbox{\small гистограммы}} 
	\put(84,3){\mbox{\small Напряжение, В}} 
\end{picture}
\caption{Гистограмма интервальных измерений 
$\left\{\mbf{x}_k\right\}_{k=1}^{100}$ нуля.} Значение $\mathit{N\!O\!B} = 9$. 
\label{HISTZeroLine2} 
\end{figure}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Характер изменения статуса пересечения данных демонстрируется гистограммой 
на Рис.~\ref{HISTZeroLine2}. Ширина столбца на ней соответствует погрешности 
измерения. Распределение результатов измерения неопределённости нуля цифрового 
измерителя напряжения становится более равномерным. Согласно формуле 
\eqref{UNInterval} получаем оценку информационного множества неопределённости нуля 
\begin{equation*} 
\mbf{J}\; = \;\bigvee_{1\leq k\leq n} \mbf{x}_{k} \ 
= \  \Bigl[\,\min_{1\leq k\leq n} \un{\mbf{x}}_{k}, 
  \max_{1\leq k\leq n} \ov{\mbf{x}}_{k}\,\Bigr] 
= \left[ -1.16 \cdot 10^{-3}, 8.79 \cdot 10^{-3} \right]. 
\end{equation*} 
Точечная оценка измеряемой величины \eqref{midUNInterval} не изменилась:
\begin{equation*}
x_\text{c} \  = \  \m\mbf{J} \   
= \  \tfrac{1}{2} \Bigl(\,\min_{1\leq k\leq n} \un{\mbf{x}}_{k} + 
\max_{1\leq k\leq n} \ov{\mbf{x}}_{k}\,\Bigr) = 3.82 \cdot 10^{-3}. 
\end{equation*} 
  
Гистограмма на Рис.~\ref{HISTZeroLine2} демонстрирует гораздо более высокую степень 
однородности интервальной выборки Рис.~\ref{DRS4ZeroLine100cell2}.   
\end{example} 
  
Примеры \ref{ConstExmp} и \ref{VarExmp}, рассмотренные в совокупности, обнаруживают 
интересный эффект. Именно, результаты измерений с б\'{о}льшей интервальной 
неопределённостью выглядят, на первый взгляд более надёжными, чем результаты более 
точных измерений. В случае решения задач восстановления зависимостей по интервальным 
данным (Глава~\ref{FuncFitChap}) аналогичное явление носит название парадокса 
Е.З.\,Демиденко, см.~\S\ref{DemidParadoxSect},\index{парадокс Демиденко} суть 
которого кратко выражается фразами <<чем грубее --- тем лучше>> 
или <<чем точнее --- тем хуже>>.  
  
Тем не менее, сделанный вывод поспешен и может оказаться неверным. Оценка 
информационного множества $\mbf{J}$ заметно расширилась для измерений с большей 
интервальной неопределённостью. Причиной этого могло стать то обстоятельство, 
что огрублённая дискретность измерений в существенной степени поглотила неучтённую 
систематическую погрешность, сделала её менее заметной. 
  
Итак, более грубые измерения или, что эквивалентно, удерживание меньшего количества 
разрядов (значащих цифр) в результатах измерений, не способствуют адекватной 
интерпретации результатов обработки выборки. При таком огрублении утрачивается часть 
информации, объективно содержащейся в выборке $\left\{\mathring{x}_k\right\}_{k=1}^{100}$. 
Чтобы организовать процесс контролируемого огрубления и найти его подходящий уровень, 
полезно применить рассмотренную выше процедуру варьирования неопределённости. 
Она даёт количественную оценку увеличения радиусов отдельных замеров, необходимую 
для достижения модифицированной выборкой свойства накрытия.  
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\chapter{Задача восстановления зависимостей} 
\label{FuncFitChap}
  
  
В этой главе даются определения новых понятий и терминов, которые возникают в связи 
с восстановлением функциональных зависимостей по данным их измерений и наблюдений, 
имеющим интервальную неопределённость. Кроме того, мы рассмотрим необходимый 
математический аппарат, основные идеи и типичные приёмы восстановления зависимостей 
по интервальным данным, а также проблемы, возникакющие при их применении. Более 
подробно исследуется случай простейшей линейной зависимости, но большинство 
построений и рассуждений легко переносятся на общий нелинейный случай. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section[Постановка задачи и необходимые понятия]% 
        {Постановка задачи \\* и необходимые понятия} 
  
Предположим, что величина $y$ является функцией некоторого заданного вида от 
независимых переменных $x_1$, $x_2$, \ldots, $x_m$, т.\,е. 
\begin{equation}
\label{ParamFunc} 
y = f(x_{1}, \ldots, x_{m}, \beta_1, \ldots, \beta_l), 
\end{equation} 
где $\beta_1, \ldots, \beta_l$ --- некоторые параметры. В кратком виде  
\begin{equation*}
y = f(x, \beta), 
\end{equation*} 
где $x = (x_{1}, \ldots, x_{m})$ --- вектор независимых переменных, $\beta = (\beta_1, 
\ldots, \beta_l)$ --- вектор параметров функции. Имея набор значений переменных $x$ 
и $y$, нам нужно найти $\beta_1, \ldots, \beta_l$, которые соответствуют конкретной 
функции $f$ из параметрического семейства \eqref{ParamFunc}. Эту задачу мы будем 
называть \emph{задачей восстановления зависимости}, и она будет основным предметом 
рассмотрения в этой главе.                \index{задача восстановления зависимости} 
  
\begin{example} 
Предположим, что интересующая нас функциональная зависимость является алгебраическим 
полиномом третьей степени от одной переменной, вида 
\begin{equation*} 
y = a_{3}x^3 + a_{2}x^2 + a_{1}x + a_{0}. 
\end{equation*} 
Тогда в выписанной выше постановке задачи восстановления зависимостей $x$ и $y$ --- 
вещественные скаляры, а вектор параметров $\beta$ является вектором $(a_{0}, a_{1}, 
a_{2}, a_{3})\in\mbb{R}^4$. 
\end{example}
  
Широко используются также другие названия --- <<задача идентификации параметров>>,  
<<задача подгонки данных>>, <<задача подгонки кривой>>,\footnote{Соответствующие 
англоязычные термины --- identification problem, data fitting problem, curve fitting 
problem.} <<задача сглаживания данных>> и т.\,п. В вероятностной статистике 
рассматриваемую нами задачу называют <<задачей построения регрессии>> или <<задачей 
регрессионного анализа>>, а изучающая её математическая дисциплина называется 
регрессионным анализом (см., к примеру, \cite{DraperSmith,HCramer}). Наконец, стоит 
отметить ещё одно название нашей задачи --- <<задача построения эмпирических формул>>. 
Оно было особенно популярно вплоть до середины XX века, но часто используется и сейчас 
(см., к примеру, \cite{DemidMaronShuval}).  
  
\index{задача подгонки  кривой}\index{задача регрессионного анализа}%    
\index{задача сглаживания данных}\index{задача подгонки данных}% 
  
Исходя из контекста или предметной области, где рассматривается поставленная задача, 
для переменных в функциональной зависимости \eqref{ParamFunc} используют также различные 
другие термины. Независимые переменные $x_1$, $x_2$, \ldots, $x_m$ часто называют 
\emph{экзогенными}, \emph{предикторными} или просто \emph{входными} переменными. 
Зависимая переменная $y$ называется также \emph{эндогенной}, \emph{критериальной} или 
\emph{выходной} переменной. 
\index{переменная предикторная}\index{переменная критериальная}%  
\index{переменная экзогенная}\index{переменная эндогенная}% 
\index{переменная входная}\index{переменная выходная}%  
  
Важнейший частный случай рассматриваемой задачи --- определение параметров линейной 
функции вида 
\begin{equation} 
\label{LinFunc}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_m x_m , 
\end{equation} 
в которой свободный член $\beta_0$ и коэффициенты $\beta_1$, \ldots, $\beta_m$ должны 
быть определены из ряда измерений значений $x_1$, $x_2$, \ldots, $x_m$ и $y$. 
  
Результаты измерений, как правило, неточны, и всюду ниже мы предполагаем что они имеют 
\emph{интервальную неопределённость}. Напомним, что это важнейший  подвид  ограниченной 
неопределённости (см.  \S\ref{InteStatistiSect}), когда нам известны лишь некоторые 
интервалы возможных значений измеряемых величин. Для восстановления зависимости 
выполняются ряд измерений независимых и зависимой переменных, и результатом $i$-го 
измерения  являются интервалы 
\begin{equation*} 
\mbf{x}^{(i)}_{1}, \  \mbf{x}^{(i)}_{2}, \  \ldots, \  
   \mbf{x}^{(i)}_{m}, \  \mbf{y}^{(i)}. 
\end{equation*} 
В целом имеется $n$ измерений, так что индекс $i$ может принимать значения 
из множества натуральных чисел $\{ 1,2,\ldots,n \}$. 
  
Далее для удобства построений и выкладок обозначим номер измерения $i$ не верхним, 
а нижним индексом, который мы поставим первым при обозначении результатов измерений. 
Таким образом, полный набор данных для восстановления зависимости будет иметь вид 
\begin{equation} 
\label{EmpInData} 
\begin{array}{ccccc} 
\mbf{x}_{11}, & \mbf{x}_{12}, & \ldots & \mbf{x}_{1m}, & \mbf{y}_{1}, \\
\mbf{x}_{21}, & \mbf{x}_{22}, & \ldots & \mbf{x}_{2m}, & \mbf{y}_{2}, \\
  \vdots      &   \vdots      & \ddots &   \vdots      &  \vdots      \\
\mbf{x}_{n1}, & \mbf{x}_{n2}, & \ldots & \mbf{x}_{nm}, & \mbf{y}_{n}. 
\end{array}
\end{equation} 
Если ищется функция вида \eqref{ParamFunc}, то необходимо найти или как-то оценить  
параметры $\beta_1$, \ldots, $\beta_l)$, при которых функция $y = f(x,\beta)$ 
наилучшим образом <<соответствует>> данным \eqref{EmpInData}, приближает их и т.\,п. 
Аналогично, если ищется линейная функция, нам необходимо найти или как-то оценить 
свободный член и коэффициенты $\beta_j$, $j = 0,1,\ldots,m$, для которых линейная 
функция \eqref{LinFunc} наилучшим образом <<приближала>> бы интервальные данные 
измерений \eqref{EmpInData}, лучше всего <<соответствовала>> или <<согласовывалась>> 
бы с ними и пр. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb] 
\centering\small 
\unitlength=1mm 
\begin{picture}(80,53) 
\put(0,2){\includegraphics[width=80mm]{pictures/UncertainBoxes.eps}} 
\end{picture} 
\caption{Наглядная иллюстрация задачи восстановления линейной}
         зависимости по данным с интервальной неопределённостью. 
\label{UncertBoxesPic}
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Для обозначения $n\times m$-матрицы $\mbf{X} = (\mbf{x}_{ij})$, составленной из данных 
\eqref{EmpInData} для независимых переменных, часто используют термины \textit{матрица 
плана эксперимента} или просто \textit{матрица плана}, которые возникли в теории 
планирования эксперимента (см., к примеру, \cite{ErmakovZhiglyavski}). Интервалы 
$\mbf{x}_{i1}$, $\mbf{x}_{i2}$, \ldots, $\mbf{x}_{im}$, $\mbf{y}_{i}$ мы называем, 
как и раньше, \textit{интервалами неопределённости $i$-го измерения}. Но кроме них 
нам также потребуется обращаться ко всему множеству, ограничиваемому в многомерном 
пространстве $\mbb{R}^{m+1}$ этими интервалами по отдельным координатным осям. 
   
\begin{definition} 
\textsl{Брусом неопределённости} $i$-го измерения функциональной зависимости будем 
называть интервальный вектор-брус $(\mbf{x}_{i1}, \mbf{x}_{i2}, \ldots, \mbf{x}_{im}, 
\mbf{y}_{i}) \subset \mbb{R}^{m+1}$, $i = 1,2,\ldots,n$, образованный $i$-ой строкой 
таблицы данных \eqref{EmpInData}.          \index{брус неопределённости измерения} 
\end{definition} 
  
Таким образом, каждый брус неопределённости измерения зависимости является прямым 
декартовым произведением интервалов неопределённости независимых переменных и зависимой 
переменной. На Рис.~\ref{UncertBoxesPic} на плоскости $0xy$ наглядно показаны брусы 
неопределённости измерений и график линейной функции, которую мы восстанавливаем 
по ним. Далее мы рассматриваем данные \eqref{EmpInData} как уже существующие и никак 
не обсуждаем их получение, выбор или оптимизацию в том или ином смысле. Эти вопросы 
являются предметом теории планирования эксперимента, и для интервальных данных 
они отчасти затронуты в книге \cite{Dyvak}.  \index{планирование эксперимента}
    
При обработке данных с интервальной неопределённостью помимо рассмотренной выше 
постановки задачи встречаются также другие, более уместные в тех или иных условиях. 
Например, для интервальных данных \eqref{EmpInData} можно рассмотреть совокупность 
всех точечных задач восстановления зависимости, исходные данные которых принадлежат 
интервалам из \eqref{EmpInData}. Всевозможные решения этих задач образует какое-то 
множество в пространстве параметров зависимостей, и часто необходимо знать, насколько 
далеко это множество простирается в каком-то направлении или вдоль отдельных координатных 
осей. Фактически, здесь требуется оценить чувствительность решения задачи восстановления 
зависимости по отношению к вариациям данных в интервалах \eqref{EmpInData}. Такая 
постановка задачи рассматривается, к примеру, в работе \cite{SharyMoradi} для традиционных 
оценок параметров линейной функции по методу наименьших квадратов. Ниже в книге мы 
не останавливаемся на подобных задачах, несмотря на их очевидную важность, просто 
по причине дефицита места. 
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section[Накрывающие и ненакрывающие измерения и выборки]% 
        {Накрывающие и ненакрывающие \\* измерения и выборки} 
\label{CoverNCoverSect} 
  
  
Как и ранее в \S\ref{CoverMeasrSect} и Главе~3, будем различать накрывающие и 
ненакрывающие измерения. В отношении функциональных зависимостей мы принимаем следующее 
  
\begin{definition} 
Брус неопределённости измерения функциональной зависимости называется \textsl{накрывающим}, 
если его компоненты гарантированно содержат истинные значения измеряемых величин входных 
и выходных переменных функциональной зависимости.       \index{накрывающий брус} 
\end{definition} 
  
Возможные альтернативные термины --- <<включающий брус неопределённости>>, 
<<охватывающий брус неопределённости>>. \index{включающий брус}\index{охватывающий брус} 
   
Отрицание свойства накрытия истинного значения, казалось бы, требует, чтобы  
<<ненакрывающим>> назывался брус неопределённости измерения, который не является 
накрывающим, т.\,е. не содержит истинного значения измеряемой величины. Но такое 
определение неудобно и не очень полезно. Условимся называть брус неопределённости 
измерения \emph{ненакрывающим}, если нельзя утверждать, что он наверняка содержит 
истинное значение. Иными словами, ненакрывающий брус может включать интересующее 
истинное значение, а может и не включать его, но реальный статус этого включения 
неизвестен.        \index{ненакрывающий брус} 
  
\begin{definition}   
\textsl{Накрывающей выборкой} будем называть совокупность интервальных результатов 
измерений, т.\,е. выборку, в которой\index{накрывающая выборка} доминирующая часть 
измерений (наблюдений) являются накрывающими. \textsl{Ненакрывающей} называем 
выборку, большинство составляющих которую измерений могут не содержать истинных 
значений измеряемой зависимости. 
\end{definition} 
    
По поводу <<ненакрывающей>> выборки можно сказать то же самое, что и по поводу 
ненакрывающего бруса.\index{ненакрывающая выборка} Называть этим термином выборку, 
в которой хотя бы одно из измерений --- ненакрывающее, не имеет большого практического 
смысла, так как реальные выборки чаще всего именно такими и являются: из-за наличия 
выбросов и промахов, из-за недооценки величины погрешности и множества других причин. 
Более уместно использовать термин <<ненакрывающий>> в отношении выборки, в которой  
для большинства измерений мы не гарантируем свойство накрытия истинного значения, 
т.\,е. выборки, в которых количество <<ненакрытий>> уже переходит на новый 
качественный уровень.  
  
Для визуализации интервальных данных, аналогично традиционному точечному случаю, 
используют \emph{диаграммы рассеяния}. В традиционном понимании диаграмма рассеяния 
используется в статистике и анализе данных для визуализации значений двух переменных 
в виде <<облака>> точек на декартовой плоскости и позволяет оценить возможное 
присутствие выбросов, наличие или отсутствие корреляции и других взаимосвязей между 
двумя переменными. На диаграмме рассеяния для интервальных данных каждое интервальное 
наблюдение отображается в виде бруса (бруса неопределённости). При отсутствии 
неопределённости по одной из переменных, брусы наблюдений могут превращаться 
в одномерные вертикальные или горизонтальные отрезки (<<ворота>> для случая 
накрывающих измерений).\index{диаграмма рассеяния} Примерами диаграмм рассеяния 
могут служить, в частности, Рис.~\ref{UncertBoxesPic} и Рис.~\ref{UncSegmentPic} 
из этой главы, а также некоторые другие. 
  
Если интервальные результаты измерений получены с помощью процедуры обинтерваливания 
из точечных базовых значений (см. \S\ref{MeasuResultSect}), то часто имеет смысл также 
отобразить на диаграмме рассеяния эти базовые значения внутри интервалов (брусов) 
неопределённости измерений. Они помогают лучше понять <<основу>> (каркас) интервальных 
данных, по которому они строятся, и, возможно, лучше выбрать способ их обработки. 
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section{Информационное множество задачи} 
\label{InformSetSect} 
  
  
Существует большое количество более или менее стандартных подходов к решению задачи 
восстановления зависимостей для обычных точечных данных. Наиболее популярные из них 
хорошо известны и широко используются в практической обработке данных. Это метод 
наименьших квадратов, метод наименьших модулей, метод максимальной энтропии и другие. 
Часто используется чебышёвское (минимаксное) сглаживание. Все эти методы основаны 
на  нахождении минимума какой-то количественной меры отклонения конструируемой 
функции от приближаемых данных, которую часто называют \emph{функционалом качества}. 
\index{функционал качества} Мы ищем набор параметров, который доставляет минимум этой 
мере отклонения, т.\,е. функционалу качества. Очевидно, что конечные результаты 
существенно различаются в зависимости от конкретного вида этого  функционала. 
  
Но для интервальных данных реализация описанного выше общего принципа становится 
затруднительной, поскольку не вполне ясно, как именно выбирать отклонение функции 
от приближаемых интервальных данных, которые представляют собой целые множества точек. 
Это особенно характерно для накрывающих измерений и накрывающих выборок. Дело в том, 
что накрывающие интервальные данные неявно несут с собой дополнительное требование 
на восстанавливаемую функцию: её график должен тем или иным способом проходить через 
брусы неопределённости измерений, а не просто приближать их.  
  
Для более тонкого и детального анализа ситуации, который, в конце концов, приведёт 
нас к фундаментальному понятию \emph{информационного множества} задачи восстановления 
зависимости по интервальным данным, необходимо откатиться до базисного уровня 
рассмотрения постановки задачи и начать с самых элементарных вещей. 
  
Итак, пусть имеется набор точечных экспериментальных данных 
\begin{equation} 
\label{EmpReData} 
\begin{array}{ccccc} 
x_{11}, & x_{12}, & \ldots & x_{1m}, & y_{1}, \\ 
x_{21}, & x_{22}, & \ldots & x_{2m}, & y_{2}, \\ 
\vdots  & \vdots  & \ddots &  \vdots & \vdots \\ 
x_{n1}, & x_{n2}, & \ldots & x_{nm}, & y_{n}, 
\end{array}
\end{equation} 
и формула для функциональной зависимости \eqref{ParamFunc}, зависящая от параметров, 
\begin{equation*}
y = f(x_{1}, \ldots, x_{m}, \beta_{1}, \ldots, \beta_{l}). 
\end{equation*}
Чтобы определить, насколько хорошо конкретная функция этого вида, задаваемая каким-то 
определённым набором параметров $\hat{\beta} = (\hat{\beta}_{1}, \hat{\beta}_{2}, 
\ldots, \hat{\beta}_{l})$, приближает данные \eqref{EmpReData} (<<согласуется>>, 
<<совместна>> и т.\,п. с ними), мы подставляем их вместе с $\hat{\beta}$ в равенство 
\eqref{ParamFunc} и смотрим, как выполняется оно для каждого измерения $i = 1,2, 
\ldots, n$. В частности, если на всех измерениях равенство \eqref{ParamFunc} 
удовлетворено, т.\,е. график функциональной зависимости \eqref{ParamFunc} 
при рассматриваемых параметрах $\hat{\beta}$ проходит через все точки данных 
\eqref{EmpReData}, то $\hat{\beta}$ --- искомое <<идеальное решение>>. 
  
Если параметры $\beta = (\beta_{1}, \beta_{2}, \ldots, \beta_{l})$ неизвестны, 
то имея данные \eqref{EmpReData} и подставляя их в формулу для зависимости 
\eqref{LinFunc}, получаем для каждого отдельного измерения одно уравнение вида 
\begin{equation*} 
f(x_{i1}, x_{i2}, \ldots, x_{im}, \beta_{1}, \beta_{2}, \ldots, \beta_{l}) = y_{i}, 
\end{equation*} 
или, кратко, 
\begin{equation*} 
f( x_{i:}, \beta) = y_{i}, 
\end{equation*} 
где $x_{i:} = (x_{i1}, x_{i2}, \ldots, x_{im})$ и $\beta = (\beta_1, \ldots, 
\beta_l)^\top$. В целом, в результате этой процедуры возникает система таких уравнений, 
решив которую, мы найдём параметры зависимости. Итог сказанного состоит в том, что 
для решения задачи восстановления зависимостей нужно по функции \eqref{ParamFunc} 
и эмпирическим данным составить систему уравнений, а затем найти её решение 
относительно неизвестного параметра $\beta$. 
  
Но в традиционном случае обработки точечных данных полученная система уравнений является, 
как правило, несовместной, и решений в обычном смысле не имеет. Этому способствует 
неточность данных, отличие восстанавливаемой зависимости от реально имеющейся на практике, 
а также переопределённость системы уравнений, т.\,е. превышение количества уравнений над 
числом переменных. При подстановке любого набора параметров $\beta_1$, $\beta_2$, \ldots, 
$\beta_l$  в уравнения \eqref{ParamFunc}, вытекающие из данных измерений, мы получаем 
ненулевое расхождение левой и правой частей, 
\begin{equation*} 
y_{i} - f(x_{i1}, x_{i2}, \ldots, x_{im}, \beta_{1}, \beta_{2}, \ldots, \beta_{l}), 
\end{equation*} 
которое в традиционном регрессионнном анализе называется \emph{остатком}.\index{остаток} 
В вычислительной математике вектор остатков обычно называют \emph{невязкой} 
приближённого решения.\index{невязка} 
  
В силу сказанного вместо обычных решений системы рассматривают решения в обобщённом 
смысле --- так называемые \emph{псевдорешения}, т.\,е. векторы,\index{псевдорешение} 
на которых достигается минимальное отклонение левой и правой частей системы уравнений. 
Обычно это отклонение рассматривают в какой-то конкретной норме, от выбора которой 
существенно зависит получающееся значение псевдорешения. Фактически, задача нахождения 
псевдорешения --- это и есть задача минимизации <<функционала качества>>, упомянутая 
в начале параграфа, в которой функционал качества является величиной вектора 
остатков измерений, т.\,е. вектора невязки. 
   
Таким образом, имеется два общих подхода к нахождению параметров зависимости 
по эмпирическим данным: 
\begin{itemize} 
\item[(I)]  
на основе вида искомой функциональной зависимости и обрабатываемых данных 
составляется система уравнений и находится её решение или псевдорешение; 
\item[(II)]  
на основе вида искомой функциональной зависимости и обрабатываемых данных 
составляется задача минимизации отклонения функции от эмпирических данных 
и находится её решение. 
\end{itemize} 
В случае точечных данных преимущественное значение имеет второй способ, так как система 
уравнений почти всегда не имеет обычных решений. Но если мы рассматриваем интервальные 
данные, то ситуация меняется на противоположную. Интервальные системы уравнений могут 
быть разрешимыми, т.\,е. иметь непустые множества решений, даже для переопределённого 
случая. В то же время, не вполне ясно, что именно является остатками и невязкой 
интервальной системы уравнений на приближённом решении. 
   
Что следует считать решением задачи восстановления зависимости по интервальным данным 
\eqref{EmpInData}? Это существенно зависит от их характера, и всюду далее, вплоть до 
\S\ref{NCovSampleProcSect}, мы будем рассматривать наиболее важный случай накрывающих 
интервальных данных.\index{накрывающее измерение}\index{накрывающая выборка} 
  
Очевидно, что в этих услловиях функция, вида \eqref{ParamFunc} или \eqref{LinFunc}, 
наиболее полно удовлетворяет условиям задачи восстановления искомой зависимости, если 
её график проходит через все брусы неопределённости данных. Мы будем также говорить, 
что при этом интервальные данные \emph{совместны}\index{совместность данных}
с конструируемой функциональной зависимостью. В случае точечных данных эта идеальная 
ситуация 
\begin{list}{}{\itemsep=1pt\topsep=2pt\parsep=2pt}
\item[$\bullet$] 
почти никогда не реализуется и 
\item[$\bullet$] 
неустойчива к малым возмущениям \\ 
в данных, если она всё-таки реализовалась. 
\end{list} 
Напротив, в случае данных с существенной интервальной неопределённостью прохождение 
графика функции через брусы данных \eqref{EmpInData} не является чем-то исключительным, 
причём в случае общего положения это состояние устойчиво к возмущениям в данных. 
  
\begin{example} 
Рассмотрим задачу восстановления линейной функции одной переменной $y = kx + b$ 
по данным трёх её наблюдений: 
\begin{equation*} 
\begin{array}{c||c|c|c} 
\hline 
x & 1 &  2  & 3 \rule[-1mm]{0mm}{4.5mm}\\ 
\hline 
y & 1 & 1.5 & 2 \rule[-1mm]{0mm}{4.5mm}\\ 
\hline 
\end{array}\,. 
\end{equation*} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb] 
\centering\small  
\includegraphics[width=50mm]{pictures/StabilExamp-11.eps}  
\hspace{7mm} 
\includegraphics[width=50mm]{pictures/StabilExamp-22.eps} 
\caption{\,Прохождение прямой линии через точки данных --- } 
неустойчивая конфигурация, тогда как прохождение прямой \\ 
через интервальные брусы устойчиво к возмущениям. 
\label{StabilExmpPic} 
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Эти данные изображены крестиками на левом чертеже Рис.~\ref{StabilExmpPic}, и 
нетрудно понять, что через них точно проходит прямая линия, являющаяся графиком 
функции $y = \tfrac{1}{2}x + \tfrac{1}{2}$, так что $k = b = \tfrac{1}{2}$. 
Но любое сколь угодно малое шевеление данных, т.\,е. изменение положения 
точек-крестиков, приводит к разрушению этой конфигурации. Три точки данных 
не будут уже лежать на одной прямой и точного прохождения графика линейной 
функции через них мы не обеспечим. 
  
В то же время, для интервальных данных, 
\begin{equation*} 
\begin{array}{c||c|c|c} 
\hline 
x & [0.6, 1.4] & [1.6, 2.4] & [2.7, 3.5] \rule[-1mm]{0mm}{4.5mm}\\ 
\hline 
y & [0.7, 1.5] & [1, 2] & [1.5, 2.5] \rule[-1mm]{0mm}{4.5mm}    \\ 
\hline 
\end{array}\,. 
\end{equation*} 
которые получены <<раздуванием>> точечных (правый чертёж на Рис.~\ref{StabilExmpPic}), 
можно построить бесконечно много прямых, проходящих через брусы неопределённости. 
Большинство этих прямых устойчиво проходят через брусы даже при их малых шевелениях. 
  
Параметры прямых, проходящих через брусы данных на Рис.~\ref{StabilExmpPic}, 
принадлежат множеству решений интервальной системы лиенйных уравнений  
\begin{equation*} 
\arraycolsep=2pt 
\left\{ \ 
\begin{array}{ccccc}
[0.6, 1.4]\,k &+& b &=& [0.7, 1.5], \\[2pt] 
[1.6, 2.4]\,k &+& b &=&   [1, 2],   \\[2pt] 
[2.7, 3.5]\,k &+& b &=& [1.5, 2.5], 
\end{array} 
\right. 
\end{equation*} 
которая получена в результате подстановки интервальных данных в равенство 
$y = kx + b$, выражающее вид искомой функциональной зависимости. Мы более подробно 
рассмотрим такие системы уравнений в следующих параграфах книги. 
  
Ясно также, что в этом примере конкретная величина раздутия (т.\,е. размеры 
интервальных брусов) не принципиальны, и вместо трёх точек и трёх брусов 
неопределённости данных мы могли бы взять любое их количество, большее двух. 
Аналогичные примеры можно построить для задач восстановления функций б\'{о}льшего 
числа переменных.  
\end{example} 
  
Отметим, что дополнительную специфику задаче придаёт то новое обстоятельство, что 
брусы неопределённости данных \eqref{EmpInData}, в отличие от бесконечно малых и 
бесструктурных точек, получают нетривиальную структуру. По этой причине нужно 
различать, как именно проходит график функции через эти брусы. Этот важный момент 
обсуждается далее в \S\ref{GenIDataFitSect}. 
  
Итак, при решении задачи восстановления зависимостей по интервальным данным, 
действуя способом, аналогичным точечному случаю, т.\,е. подставляя данные в равенство 
\eqref{ParamFunc} для искомой функциональной зависимости, мы получим уже интервальную 
систему уравнений 
\begin{equation} 
\label{ParamEqSys} 
\left\{ 
\begin{array}{l} 
f(\mbf{x}_{i1}, \mbf{x}_{i2}, \ldots, \mbf{x}_{im}, 
   \beta_{1}, \beta_{2}, \ldots, \beta_{l}) = \mbf{y}_{i}, \\[3mm] 
\hspace*{38mm} i = 1,2,\ldots,n, 
\end{array} 
\right. 
\end{equation} 
где $\mbf{x}_{i:} = (\mbf{x}_{i1}, \mbf{x}_{i2}, \ldots, \mbf{x}_{im})$. 
Её решением, обычным или в некотором обобщённом смысле, будет вектор оценки параметров 
восстанавливаемой зависимости \eqref{ParamFunc}. Новый принципиальный момент состоит 
в том, что разрешимость получающихся интервальных систем уравнений \eqref{ParamEqSys} 
имеет место для обширных совокупностей интервальных данных задачи и не является 
исключительным фактом, тогда как определение задачи минимизации отклонения 
функциональной зависимости от интервальных данных сталкивается с трудностями. 
  
В соответствии с терминологией, намеченной в Главе~\ref{PrimaryConceptChap} (см. 
\S\ref{InfoSetSect}), информационным множеством задачи восстановления зависимости 
нужно называть множество значений параметров зависимости, <<совместных>> в каком-то 
определённом смысле с данными и характером измеряемой величины. Учитывая сказанное 
выше о задаче восстановления функциональной зависимости, её информационное множество 
может быть множеством решений интервальной системы уравнений, неравенств и т.\,п. 
условий, вытекающих из постановки задачи восстановления зависимостей, т.\,е. вида 
функциональной зависимости, а также обрабатываемых данных и условий на них. 
Формально можно дать следующее 
  
\begin{definition} 
\textsl{Информационное множество} задачи восстановления функциональной зависимости 
по интервальным данным --- это множество решений интервальной системы уравнений 
(неравенств и т.\,п. условий), которая построена по виду восстанавливаемой 
функциональной зависимости и интервальным данным задачи.   
\index{информационное множество} 
\end{definition} 
  
Почему понятие информационного множества столь важно при обработке интервальных 
измерений?  Дело в том, что именно информационное множество учитывает специальный 
характер накрывающих интервальных измерений, когда они являются не просто большими 
<<раздувшимися точками>>, а ещё и включают возможные точные значения измеряемых 
величин. Неявным образом это требует, чтобы график восстанавливаемой функциональной 
зависимости проходил (тем или иным образом) через брусы неопределённости измерений. 
В оптимизационном подходе этот момент --- учёт накрытия/ненакрытия отодвигается 
на второй план, а потому его нужно отслеживать с помощью дополнительных средств. 
  
Из сказанного выше вытекает также, что в случае накрывающих интервальных данных,  
когда предпочтительным подходом является решение интервальной системы уравнений, 
оптимизационный подход, если он применяется нами, должен согласовываться 
с решением интервальной системы уравнений, с фактом существования непустого 
информационного множества задачи. 
  
Отметим также, что ранее в статьях по обработке интервальных данных аналогичное 
понятие называлось также <<множеством допустимых значений>>, <<областью допустимых 
значений>> и т.\,п. терминами. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\section[Прогнозный коридор, коридор совместных зависимостей] 
        {Прогнозный коридор и \\*  коридор совместных зависимостей} 
\label{CmptFunCorSect} 
  
  
Определение параметров функциональной зависимости производится, как правило, для того, 
чтобы затем найденную формулу использовать для предсказания значений этой зависимости 
на интересующих нас аргументах из её области определения. Ясно, что такое предсказание 
будет осуществляться с некоторой погрешностью, вызванной неточностью и неопределённостью
в данных, неоднозначностью самой процедуры восстановления и т.\,п. Эту неопределённость 
предсказания также необходимо знать и учитывать в нашей деятельности. 
  
Напомним, что в теоретико-вероятностной статистике оценки параметров функций, которые 
получаются с помощью методов регрессионного анализа, сами являются случайными величинами. 
Таким образом, вместе с построением функциональной зависимости по обрабатываемым данным 
мы реально получаем помимо неё ещё целое семейство зависимостей, которые <<более или 
менее соответствуют>> этим данным. Задавшись каким-то уровнем доверительной вероятности, 
можно построить целую совокупность (<<пучок>>) из функциональных зависимостей, отвечающих 
на вопрос о возможном поведении функций, восстанавливаемых по рассматриваемым данным. 
В любой точке области определения эти функциональные зависимости, как правило, будут 
принимать целый набор различных значений, задающий коридор неопределённости нашего 
прогноза. Совершенно аналогичным образом имеет смысл действовать в интервальном анализе 
данных, и далее нам необходимо ввести понятия и инструменты, которые формализуют 
неточность и неопределённость прогноза по построенной модели. 
  
Для строгого определения соответствующих математических объектов нам понадобится понятие 
\emph{многозначного отображения} (см., к примеру, \cite{MultivalMaps, AubinEkeland}). 
Для произвольных множеств $X$ и $Y$ \emph{многозначным отображением} $F$ из $X$ в $Y$ 
называется соответствие (правило), сопоставляющее каждому аргументу $x\in X$ непустое 
подмножество $F(x)\subset Y$, называемое  \emph{значением} или \emph{образом} $x$.  
\index{многозначное отображение} От традиционного понятия <<функции>> (<<отображения>>) 
многозначное отображение отличается тем, что каждому элементу области определения $X$ 
сопоставляется не один элемент из $Y$, а целое множество. 
  
Пусть дана задача восстановления функциональной зависимости вида $y = f(x,\beta)$, 
где областью определения независимой переменной $x$ является множество $X$, а 
значения зависимой переменной $y$ принимаются во множестве $Y$. Будем называть 
\emph{прогнозным коридором}\index{прогнозный коридор} для задачи восстановления 
зависимостей по интервальным данным многозначное отображение $\varPi : X\to Y$, 
которое каждой точке области определения $X$ восстанавливаемой зависимости 
сопоставляет множество возможных значений отображений, которые, в рамках 
рассматриваемой модели, могут принимать функциональные зависимости, восстановленные 
по данным задачи.\footnote{В качестве соответствующего англоязычного термина 
мы предлагаем использовать <<prediction corridor>>.} 
  
Данное выше определение --- очень неформальное и общее. Но оно и не может быть сделано 
слишком формальным и однозначным, так как сама процедура восстановления функциональных 
зависимостей неоднозначна, а неопределённость и неточность данных можно обрабатывать 
в разных смыслах. Как можно на практике строить или оценивать прогнозный коридор?  
  
Предположим, что метод восстановления зависимости по интервальным данным, применённый 
к рассматриваемой задаче, удовлетворяет принципу соответствия, который мы обсуждали 
в \S\ref{CorresPrincpSect}. Иными словами, при сужении интервалов данных до точечных 
данных мы получаем с помощью этого метода осмысленные оценки параметров. Тогда 
прогнозный коридор можно построить, например, как множество всевозможных решений 
точечных задач восстановления зависимостей по точечным данным, содержащимся в интервалах. 
Иными словами, мы строим совокупность всех функциональных зависимостей из заданного 
класса, фигурирующего в задаче, которые являются решениями задач для точечных данных 
из рассматриваемых интервалов. Объединение значений полученных функциональных 
зависимостей на каждом отдельном аргументе из области определения даст требуемое 
многозначное отображение $\varPi$.  
  
Другой возможный способ построения прогнозного коридора может заключаться в том, чтобы 
найти вначале решение задачи в виде некоторой <<опорной>> функциональной зависимости, 
построенной по рассматриваемым данным, а затем организовать её <<раздутие>> на величину, 
которую получена каким-то внешним образом для оценки неопределённости прогноза. Этот 
способ не связан напрямую с необходимостью учёта всех возможных точечных данных 
из исходных интервалов задачи. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   
\begin{figure}[htb] 
\centering\small  
    \includegraphics[width=80mm]{pictures/FuncCorridor.eps} 
    \caption{Коридор совместных зависимостей и его сечение}
    для какого-то значения аргумента $x^\ast$. 
    \label{FuncTubePic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
Ещё один способ построения прогнозного коридора --- рассмотреть вместо него так 
называемый коридор совместных зависимостей задачи. Если информационное множество задачи 
восстановления зависимостей непусто, то обычно оно задаёт целое семейство зависимостей, 
совместных с данными задачи, которое имеет смысл рассматривать вместе, как единое целое. 
Это необходимо делать в вопросах, касающихся оценивания неопределённости предсказания, 
учёта всех возможных сценариев развития и т.\,п. Как следствие, возникает необходимость 
рассматривать вместе, единым целым, множество всех функций, совместных с интервальными 
данными задачи восстановления зависимости. 
  
\begin{definition} 
Пусть в задаче восстановления зависимостей информационное множество $\varOmega$ 
параметров зависимостей $y = f(x,\beta)$, совместных с данными, является непустым. 
\textsl{Коридором совместных зависимостей} рассматриваемой задачи называется 
многозначное  отображение $\varUpsilon$, сопоставляющее каждому значению 
аргумента $x$ множество    \index{коридор совместных зависимостей} 
\begin{equation*} 
\varUpsilon(x) \  = \;\bigcup_{\beta\in\varOmega} f(x,\beta). 
\end{equation*} 
\end{definition} 
  
В литературе использовались также другие термины для обозначения этого объекта --- 
<<трубка>> совместных зависимостей \cite{Kumkov2010} (имеет происхождение в теории 
управления), <<полоса>> или даже <<слой неопределённости>> \cite{NovitskiZograf}, 
<<коридор неопределённости>> \cite{Skibitski} и т.\,п. Так или иначе, коридор 
совместных зависимостей во многих случаях может служить прогнозным коридором. 
Вообще говоря, понятие прогнозного коридора шире понятия коридора совместных 
зависимостей. Если информационное множество задачи пусто, то и о коридоре 
совместных зависимостей не имеет смысл говорить, но, как правило, оценку параметров 
мы при этом всё равно должны получить, и какая-то функциональная зависимость будет 
построена. У этого решения задачи некоторая неопределённость всё равно присутствует, 
а потому имеет смысл и прогнозный коридор. Иными словами, коридор совместных 
зависимостей всегда образован зависимостями (того вида, который фигурирует в задаче), 
тогда как прогнозный коридор может получаться другими способами.  
      
Рис.~\ref{FuncTubePic} изображает коридор совместных зависимостей в задаче 
восстановления нелинейной зависимости, но для частных случаев коридор совместных 
зависимостей имеет существенно более специальный вид (см.  Рис.~\ref{GypotezaPic}). 
Нетрудно показать, что для линейной функции \eqref{LinFunc} границы коридора 
совместных зависимостей являются кусочно-линейными (см., в частности, 
\S\ref{IreneExampleSect}). Они, как правило расширяются при удалении аргумента 
от области расположения измерений независимых переменных, и это явления аналогично 
увеличению погрешности при экстраполяции данных. Примеры использования коридора 
совместных зависимостей можно увидеть в работах \cite{Kumkov2010,KumkovIgnatenkova}. 
      
Значение $\varUpsilon(\tilde{x})$ коридора совместных зависимостей при каком-то 
определённом аргументе $\tilde{x}$ (<<сечение коридора>>) --- это множество 
$\,\cup_{\beta\in\varOmega}\, f(\tilde{x},\beta)$, образованное всевозможными 
значениями, которые принимают на этом аргументе функциональные зависимости, 
совместные с интервальными данными измерений (см. Рис.~\ref{FuncTubePic}). 
Это множество имеет большое практическое значение, так как, фактически, оно 
описывает неопределённость прогноза на аргументе $\tilde{x}$. Его нужно уметь 
вычислять или каким-либо образом оценивать. В частности, необходимо знать 
внешние оценки интервала 
\begin{equation} 
\label{PredictIntval} 
\Bigl[\;\min_{\beta\in\varOmega}\, f(\tilde{x},\beta),\,  
   \max_{\beta\in\varOmega}\, f(\tilde{x},\beta)\,\Bigr].   
\end{equation} 
В ряде задач необходимо также знать внутреннюю оценку коридора совместных 
зависимостей. Интервал \eqref{PredictIntval} будем называть \emph{прогнозным 
интервалом} для значения независимой переменной $\tilde{x}$.  
\index{прогнозный интервал} 
  
Понятие прогнозного интервала обобщается также для прогнозного коридора.  
Если восстанавливаемая функциональная зависимость имеет одномерные значения, 
а сечение прогнозного коридора ограничено и связно, то оно является интервалом 
вещественной оси. В этом случае также будем говорить о \emph{прогнозном интервале}. 
    
        
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section[Случай точных измерений входных переменных]% 
        {Случай точных измерений \\* входных переменных} 
\label{ExactInputSect} 
  
  
Важнейшим и часто встречающимся частным случаем рассматриваемой задачи является 
ситуация, когда независимые (экзогенные, предикторные, входные) переменные $x_1$, 
$x_2$, \ldots, $x_m$ измеряются точно, и интервальная неопределённость может быть 
присуща только зависимой (эндогенной, критериальной, выходной) переменной $y$. Тогда 
вместо телесных брусов неопределённости измерений (как на Рис.~\ref{UncertBoxesPic}) 
мы имеем отрезки прямых $(x_{i1}, x_{i2}, \ldots, x_{im}, \mbf{y}_{i})$, $i = 
1,2,\ldots,n$, параллельные оси зависимой переменной (см. Рис.~\ref{UncSegmentPic}). 
Именно такая постановка задачи была рассмотрена для линейной функции в пионерской 
работе Л.В.\,Канторовича \cite{Kantorovich}. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   
\begin{figure}[!htb] 
\centering\small 
\includegraphics[width=80mm]{pictures/UncertSegments.eps} 
\caption{\,Задача \, восстановления \, функциональной \, зависимости\,}  
         по неточным данным, когда входные переменные измерены точно. 
\label{UncSegmentPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Отсутствие неопределённости значений независимых переменных приводит к кардинальному 
упрощению математической модели. Брусы неопределённости измерений, введённые ранее, 
схлопываясь по независимым переменным, превращаются в \emph{отрезки неопределённости}. 
\index{отрезок неопределённости} Но самое главное --- это тот факт, что интервальность 
в системе уравнений \eqref{ParamEqSys} присутствует теперь только в правых частях, и 
поэтому интервальных расширений функции $f$ по переменным $x_{1}$, $x_{2}$, \ldots, 
$x_{m}$ брать не нужно. Напомним, что построение интервальных оценок областей значений 
функций является в общем случае непростым и весьма трудоёмким. Как следствие, для решения 
и полного исследования этого частного случая, начиная с работы \cite{Kantorovich}, 
предложено большое количество различных подходов и эффективных вычислительных методов. 
Рассмотрим эти математические вопросы более детально. 
  
Естественно считать, что функциональная зависимость $y = f(x, \beta)$ \emph{совместна} 
(согласуется) с накрывающими интервальными данными измерений, если её график проходит 
через все отрезки неопределённости, задаваемые интервалами измерений выходной переменной 
$y$, как это изображено на Рис.~\ref{UncSegmentPic}. Подобное понимание совместности 
(согласования) является прямым обобщением того понимания <<совместности>>, которое 
традиционно для неинтервального случая и используется, к примеру, в постановке задачи 
интерполяции. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   
\begin{figure}[htb] 
\centering\small  
\includegraphics[width=80mm]{pictures/LinUncertSegms.eps} 
\caption{Частный случай задачи восстановления линейной} 
         зависимости по неточным данным, когда входные \\
         переменные измеряются точно. 
\label{LinUncSegmsPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Интервальная система уравнений \eqref{ParamEqSys} в этом случае принимает вид 
\begin{equation*} 
\left\{ 
\begin{array}{r} 
f(x_{i1}, x_{i2}, \ldots, x_{im}, 
   \beta_{1}, \beta_{2}, \ldots, \beta_{l}) = \mbf{y}_{i},  \\[3mm] 
i = 1,2,\ldots,n, 
\end{array} 
\right. 
\end{equation*} 
и она равносильна системе двусторонних неравенств 
\begin{equation*} 
\left\{ 
\begin{array}{r} 
\un{\mbf{y}}_{i} \leq f(x_{i1}, x_{i2}, \ldots, x_{im}, 
   \beta_{1}, \beta_{2}, \ldots, \beta_{l}) \leq\ov{\mbf{y}}_{i},  \\[3mm] 
i = 1,2,\ldots,n, 
\end{array} 
\right. 
\end{equation*} 
Такие системы нелинейных неравенств рассматриваются, например, в теории 
оптимизации, той ветви прикладной математики, которую также называют <<математическим 
программированием>> (см., к примеру, \cite{BazaraShetty,ZukhovitskiyAvdeeva,Schrijver}). 
Дело в том, что ряд методов численного решения задачи условной оптимизации с ограничениями 
в виде неравенств выполняет итеративный поиск решения из области допустимых решений задачи, 
которые удовлетворяют системе ограничений. При этом отслеживается принадлежность текущего 
приближения области допустимых решений, то есть, фактически, множеству решений системы 
неравенств. Таким образом, исследование пустоты/непустоты множеств, задаваемых системами 
ограничений в виде неравенств, и нахождение точек из них попадают в предмет вычислительной 
оптимизации. Готовые алгоритмы для решения таких задач реализованы в различных пакетах 
программ оптимизации, а также в составе многих популярных систем компьютерной математики 
--- \textsc{Matlab}, Octave, Scilab и других. Кроме того, существуют также 
специализированные методы исчерпывающего оценивания множеств решений систем 
неравенств (см., например, \cite{EvtuPosypRyTu}). 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[!htb]
\centering\small   
\unitlength=1mm 
\begin{picture}(80,52)
      \put(0,0){\includegraphics[width=80mm]{pictures/UncertStripes.eps}}
      \put(75,2){$\beta_0$}\put(18,47){$\beta_1$} 
\end{picture}
\caption{Образование информационного множества параметров}
    \, линейной зависимости (ограничено красной линией) \\ 
    \, для случая точных входных переменных. 
\label{UncertStripesPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
Восстановление функции по данным, которые точны для независимых переменных, является 
особенно простым для линейного случая. Подставляя в равенство \eqref{LinFunc}, т.\,е. 
\begin{equation*} 
y = \beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2} + \ldots + \beta_{m} x_{m}, 
\end{equation*} 
данные для переменных $x_1$, $x_2$, \ldots, $x_m$ в $i$-ом измерении и требуя 
включения полученного значения в интервалы $\mbf{y}_{i}$, получим 
\begin{equation} 
\label{LinIneqs} 
\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_m x_{im} \in\mbf{y}_{i}, 
\qquad  i = 1,2,\ldots,n.  
\end{equation}  
С одной стороны, это интервальная система линейных алгебраических уравнений вида 
\begin{equation*} 
\arraycolsep=2pt 
\left\{ \ 
\begin{array}{ccccccccccc}
\beta_0 &+& x_{11}\beta_1 &+& 
   x_{12} \beta_2 &+& \ldots &+& x_{1m}\beta_m &=& \mbf{y}_{1}, \\[3pt] 
\beta_0 &+& x_{21}\beta_1 &+& 
   x_{22} \beta_2 &+& \ldots &+& x_{2m}\beta_m &=& \mbf{y}_{2}, \\[3pt] 
 \vdots &&  \vdots && \vdots && \ddots && \vdots && \vdots      \\[3pt] 
\beta_0 &+& x_{n1}\beta_1 &+& 
   x_{n2} \beta_2 &+& \ldots &+& x_{nm}\beta_m &=& \mbf{y}_{n}, 
\end{array} 
\right. 
\end{equation*} 
у которой интервальность присутствует только в правой части. С другой стороны, 
\eqref{LinIneqs} равносильно системе 
\begin{equation} 
\label{LinIneqSys} 
\arraycolsep=2pt 
\left\{ \ 
\begin{array}{ccccc}
\un{\mbf{y}}_{1} & \leq & \beta_0 + x_{11} \beta_1 + 
   x_{12} \beta_2 + \ldots + x_{1m} \beta_m & \leq & \ov{\mbf{y}}_{1}, \\[3pt] 
\un{\mbf{y}}_{2} & \leq & \beta_0 + x_{21} \beta_1 + 
   x_{22} \beta_2 + \ldots + x_{2m} \beta_m & \leq & \ov{\mbf{y}}_{2}, \\[3pt] 
 \vdots & \vdots & \hspace*{12mm} \vdots \qquad 
                         \qquad \ddots \qquad \vdots & \vdots & \vdots \\[3pt] 
\un{\mbf{y}}_{n} & \leq & \beta_0 + x_{n1} \beta_1 + 
   x_{n2} \beta_2 + \ldots + x_{nm} \beta_m & \leq & \ov{\mbf{y}}_{n}. 
\end{array} 
\right. 
\end{equation} 
Это система двусторонних линейных неравенств относительно неизвестных параметров 
$\beta_0$, $\beta_1$, $\beta_2$, \ldots, $\beta_m$, решив которую, мы можем определить 
искомую линейную зависимость. Множество решений системы неравенств \eqref{LinIneqSys} 
является информационным множеством параметров восстанавливаемой зависимости 
для рассматриваемого частного случая. 
  
Для $i$-го двустороннего неравенства из системы \eqref{LinIneqSys} множество решений 
--- это полоса в пространстве $\mbb{R}^{m+1}$ параметров $(\beta_0, \beta_1, \ldots,
\beta_m)$, ограниченная с двух сторон гиперплоскостями с уравнениями  \index{полоса} 
\begin{align*} 
\beta_0 + x_{i1} \beta_1 + x_{i2} \beta_1 + \ldots + x_{im} \beta_1 = \un{\mbf{y}}_{i}, 
\\[3pt] 
\beta_0 + x_{i1} \beta_1 + x_{i2} \beta_1 + \ldots + x_{im} \beta_1 = \ov{\mbf{y}}_{i}. 
\end{align*} 
Множество решений системы неравенств \eqref{LinIneqSys} является пересечением $n$ 
штук таких полос, отвечающих отдельным измерениям. Можно рассматривать эти полосы 
как информационные множества отдельных измерений. На Рис.~\ref{UncertStripesPic} 
изображено формирование множества решений системы неравенств \eqref{LinIneqSys} 
для случая двух параметров (т.\,е. $m = 1$) и трёх измерений (т.\,е. $n = 3$). 
  
В целом множество решений системы линейных алгебраических неравенств \eqref{LinIneqSys} 
является выпуклым многогранным множеством в пространстве $\mbb{R}^{m+1}$ (формальные 
математические определения читатель может увидеть, например, в \cite{Schrijver}). 
Распознавание его пустоты или непустоты, а также нахождение какой-либо точки из него 
могут быть выполнены алгоритмами, сложность которых ограничена полиномом от размера 
системы неравенств (см. также \cite{Schrijver} и другие книги по предмету). Существуют 
эффективные и хорошо разработанные вычислительные методы для решения этих вопросов 
и для нахождения оценок множества решений, например, основанные на сведении 
рассматриваемой задачи к задаче линейного программирования. Напомним, что в настоящее 
время задача линейного программирования является одной из наиболее разработанных 
вычислительных задач, для решения которой создано большое количество программного 
обеспечения, в том числе свободных программ и пакетов программ. 
  
Если число переменных $m$ равно всего $2$ или $3$ (а количество измерений $n$ 
произвольно), то может оказаться полезным выполнить визуализацию множеств решений 
систем линейных неравенств. Из различных подходов, предназначенных для решения этой 
задачи, своими возможностями выделяется метод граничных интервалов \cite{IreneJCT2015, 
IreneRC2014}. На его основе для систем компьютерной математики \textsc{Matlab} и Scilab 
созданы свободные пакеты \texttt{lineq} и \texttt{lineqs} соответственно \cite{lineqs}. 
  
В общем случае, когда независимые (экзогенные, предикторные) переменные известны 
неточно, ситуация существенно усложняется и множество параметров, совместных 
с интервальными данными не может быть описано так же просто, с помощью одной системы 
линейных неравенств \eqref{LinIneqSys}. Трудоёмкость распознавания его пустоты 
или непустоты становится уже экспоненциальной в зависимости от количества 
переменных (см. \S\ref{GenIDataFitSect} и подробности в \cite{SSharyBook}). 
   
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\section[Конкретный пример и сравнение с оценками МНК]%
        {Конкретный пример \\* и сравнение с оценками МНК}  
\label{IreneExampleSect} 
  
Рассмотрим  пример, который иллюстрирует понятия и конструкции, введённые 
в предшествующих параграфах, а также даёт сравнение оценки, полученной с помощью 
интервального подхода, с традиционной оценкой по методу наименьших квадратов (МНК). 
Этот пример был предложен И.А.\,Шарой и приведён в работе \cite{SharysJCT2013}. 
  
Пусть $x$ и $y$ --- вещественные переменные, и $y$ линейно зависит от $x$, так что 
\begin{equation} 
\label{SampleLinFun} 
y = \beta_{0} + \beta_{1} x.
\end{equation}
Неизвестные параметры этой зависимости --- $\beta_0$ и $\beta_1$ --- надо определить 
по данным измерений, которые приведены в следующей таблице: 
\begin{equation*}
\begin{array}{|c||@{\hspace{4mm}}c@{\hspace{4mm}}|@{\hspace{4mm}}c@{\hspace{4mm}}|c|}
\hline
\text{Номер опыта}& 1 & 2 & 3\rule[-6pt]{0mm}{20pt}\\  \hline\hline
x & 0 & 1 & 2\\  \hline
y & 1 & 2 & -0.5 \\  \hline 
\end{array}
\end{equation*}
Предположим также, что в каждом опыте переменная $x$ измеряется точно, тогда как 
для переменной $y$ измерения дают только интервал, середина (центр) которого приведена  
в таблице. Радиус интервала неопределённости во всех измерениях равен единице, 
а истинное значение $y$ может быть любым числом из этого интервала. Требуется 
оценить информационное множество, т.\,е. множество пар значений $\beta_0$ и $\beta_1$, 
согласующихся с данными измерений в сформулированном выше смысле. 
  
Обозначим \begin{tabular}[t]{l}
          $x_i$ --- значение переменной $x$ в $i$-м опыте,\\
          $y_i$ --- центр интервала для переменной $y$ в $i$-м опыте.
          \end{tabular}
  
\smallskip\noindent 
Информационное множество в рассматриваемом примере описывается интервальной системой 
линейных алгебраических уравнений 
\begin{equation}
\label{IreneExmpSys}
\begin{pmatrix} 1 & 0 \\[3pt] 1 & 1\\[3pt] 1 & 2 \end{pmatrix} 
\begin{pmatrix} \beta_{0}\\[3pt] \beta_{1} \end{pmatrix}
= 
\begin{pmatrix} 1+[-1,1]\\[3pt] 2+[-1,1]\\[3pt] -0.5+[-1,1] \end{pmatrix}, 
\end{equation}
которая, фактически, является системой включений 
\begin{equation*}
\begin{pmatrix} 1 & 0 \\[3pt] 1 & 1\\[3pt] 1 & 2 \end{pmatrix} 
\begin{pmatrix} \beta_{0}\\[3pt] \beta_{1} \end{pmatrix}
\in
\begin{pmatrix} [0, 2]\\[3pt] [1,3]\\[3pt] [-1.5,0.5] \end{pmatrix}. 
\end{equation*} 
Её множество решений представляет собой пересечение трех полос: 
\begin{equation*} 
\begin{array}{c@{\hspace{5mm}}l} 
(\rm{I})& \beta_{0}\in [0,2],\\[7pt] 
(\rm{II})& \beta_{1}\in -\beta_{0} + [1,3],\\[7pt] 
(\rm{III})& \beta_{1}\in -0.5\,\beta_{0} + [-0.75,0.25]. 
\end{array}
\end{equation*}
На Рис.~\ref{InfoSet} эти полосы занумерованы, а информационное множество --- их 
непустое пересечение --- выделено зелёной заливкой. Это треугольник с вершинами 
$(2, -1)$, $(1.5, -0.5)$ и $(2, -0.75)$. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb]
\unitlength=1mm
\centering\small  
\begin{picture}(80,77)
\put(-2,-2){\includegraphics[width=80mm]{pictures/LSqEstFails.eps}} 
\put(74,33){$\beta_{0}$}
\put(25.5,72){$\beta_{1}$}
\put(37,6){(I)}
\put(33,52){(II)}
\put(72,8){(III)} 
\put(6,38){$-1$}
\put(21.5,38){$0$}
\put(37.5,38){$1$}
\put(52.5,38){$2$}
\put(67,38){$3$}
\put(19,7.2){$-2$}
\put(19,22){$-1$}
\put(21.5,51.3){$1$}
\put(21.5,66){$2$}
\end{picture}
\caption{В пространстве переменных $\beta_0$ и $\beta_1$ оценка по МНК} 
         (синяя звёздочка) не лежит в информационном множестве \\ 
         (закрашенный зелёный треугольник).
\label{InfoSet} 
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Для оценивания параметров зависимости \eqref{SampleLinFun} воспользуемся каким-либо 
из существующих методов восстановления зависимостей по интервальным данным, например, 
методом максимума совместности (см.~\S\ref{MCMSect}). В нём требуется нахождение 
максимума  специального распознающего функционала Tol, который является мерой 
совместности (согласования) параметров и данных. Имеем 
\begin{equation*}
\max\Tol = 0.125,
\qquad\arg\max\Tol =
\left(\begin{array}{@{}r@{.}l@{}} 1&875\\ -0&75\end{array}\right), 
\end{equation*}
т.\,е. восстанавливаемая зависимость получается в виде 
\begin{equation*} 
y = 1.875 - 0.75x. 
\end{equation*} 
На Рис.~\ref{InfoSet} видно, что полученная оценка параметров (жирная точка) находится 
в информационном множестве задачи. На Рис.~\ref{GypotezaPic} ей соответствует выделенная 
штрих-пунктиром регрессионная прямая, которая лежит примерно посредине среди всех 
возможных прямых, согласованных с данными (коридора совместных зависимостей). 
Совершенно те же результаты получаются при использовании метода центра неопределённости  
\cite{OskorbinMaksiZhilin, Zhilin2005}. 
  
Воспользуемся теперь для оценивания параметров $\beta_0$ и $\beta_1$ традиционным 
методом наименьших квадратов (МНК),\index{метод наименьших квадратов} предполагая, 
что неопределённость в измерениях выходной переменной $y$ имеет вероятностный 
характер \cite{DraperSmith, Linnik}. Значения $y$, представленные в таблице выше, 
можно тогда считать средними значениями (математическими ожиданиями), а интервалы 
неопределённости пусть содержат все или <<почти все>> возможные значения этой 
величины. Например, если погрешности измерения $y$ подчинены нормальному закону 
распределения с дисперсией $\sigma$, то интервалы $[-1, 1]$ для погрешностей значений 
$y$ могут быть хорошо известными инженерам интервалами <<плюс-минус 3$\sigma$>> или 
даже <<плюс-минус  6$\sigma$>>, которые обеспечивают достоверность на уровне 
$99.73\%$ и $99.99966\%$ соответственно. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb]
\unitlength 1mm
\centering\small  
\begin{picture}(100,74)
\put(0,0){\includegraphics[width=100mm]{pictures/InteRegrTube.eps}}
\put(96,23){$x$}
\put(33,68){$y$}
\put(7,19){$-2$}
\put(22,19){$-1$}
\put(50,19){$1$}
\put(63,19){$2$}
\put(76,19){$3$}
\put(38,36){$1$}
\put(38,49){$2$}
\put(38,63){$3$}
\put(37,7){$-1$}
\end{picture}
\caption{В пространстве переменных $x$ и $y$ прямая $y = \beta_{0}^{\star} + 
\beta_{1}^{\star} x$,} определяемая с помощью МНК (пунктир), не лежит в коридоре\\ 
совместных прямых, проходящих через интервалы измерений. 
\label{GypotezaPic} 
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Обозначим через $\beta_{0}^{\star}$ и $\beta_{1}^{\star}$ оценки по МНК 
для $\beta_0$ и $\beta_1$. Известно (см. \cite{DraperSmith, Linnik}), что 
величины $\beta_{0}^{\star}$ и $\beta_{1}^{\star}$ определяются как решения 
нормальной системы уравнений 
\begin{equation}
\label{NormSystem}
\begin{pmatrix}  1 & 1 & 1 \\[2pt] 0 & 1 & 2 \end{pmatrix}
\begin{pmatrix} 1 & 0 \\[2pt] 1 & 1\\[2pt] 1 & 2 \end{pmatrix}
\begin{pmatrix}\beta_{0} \\[2pt] \beta_{1}\end{pmatrix}
=
\begin{pmatrix}  1 & 1 & 1 \\[2pt] 0 & 1 & 2 \end{pmatrix} 
\begin{pmatrix} 1\\[2pt] 2\\[2pt] -0.5\end{pmatrix}, 
\end{equation} 
которая получается из средней системы уравнений для \eqref{IreneExmpSys} домножением 
обеих частей слева на транспонированную матрицу коэффициентов. После выполнения 
умножений в \eqref{NormSystem} получаем 
\begin{equation*}
\begin{pmatrix} 3 & 3\\[2pt] 3 & 5\end{pmatrix}
\begin{pmatrix} \beta_{0} \\[2pt] \beta_{1}\end{pmatrix} 
=
\begin{pmatrix} 2.5\\[2pt] 1\end{pmatrix},
\end{equation*}
\begin{equation*}
\det\begin{pmatrix} 3 & 3\\[2pt] 3 & 5\end{pmatrix}=6, \ 
\text{обратная матрица ---} \
\begin{pmatrix} 3 & 3\\[2pt] 3 & 5 \end{pmatrix}^{-1}=
\frac{1}{6}\begin{pmatrix} 5 & -3\\[2pt] -3 & 3\end{pmatrix},
\end{equation*}
и потому искомая оценка 
\begin{equation*}
\begin{pmatrix}\beta_{0}^{\star} \\[2pt] \beta_{1}^{\star}\end{pmatrix} 
= 
\frac{1}{6}\begin{pmatrix} 5 &-3\\[2pt] -3 & 3 \end{pmatrix}
\begin{pmatrix} 2.5\\[2pt] 1 \end{pmatrix}=
\frac{1}{6}\begin{pmatrix} 9.5\\[2pt] -4.5 \end{pmatrix}=
\begin{pmatrix} 19/12 \\[2pt] -3/4 \end{pmatrix}=%\approx
\left(\begin{array}{@{}r@{.}l@{}}1&58(3)\\ -0&75\end{array}\right). 
\end{equation*} 
Итак, метод наименьших квадратов даёт в качестве восстановленной зависимости 
\begin{equation*}
y = 1.58333 - 0.75x. 
\end{equation*}
  
Угловой коэффициент построенной прямой совпадает в тем, что получен выше 
с помощью метода максимума согласования. Но точка с координатами $(\beta_{1}^{\star}, 
\beta_{0}^{\star})$, показанная на Рис.~\ref{InfoSet} звёздочкой, лежит на заметном 
удалении от информационного множества задачи. Сравнение зависимости, построенной 
с помощью МНК, с коридором линейных зависимостей, которые совместны с интервальными 
данными задачи, представлено на Рис.~\ref{GypotezaPic} в координатах $(x,y)$. Из него 
мы можем видеть, что МНК-линия заметно отстоит от оси этого коридора и вообще 
не проходит через коридор в окрестности аргумента $x = 1$. 
\index{метод наименьших квадратов} 
  
% Если в качестве правой части интервальной системы \eqref{IreneExmpSys} взять 
% интервальный вектор $([0, 2], [1.25,2.75], [-1.5,0.5])^\top$, то её множество 
% решений стянется в одну точку $(2, -0.75)^\top$, которая задаёт прямую, 
% параллельную МНК-линии и не совпадающую с ней.   
  
Пример аналогичного свойства, в котором оценка параметров линейной зависимости 
по методу наименьших квадратов  не лежит в информационном множестве задачи 
с интервальными данными, приведён также в работе \cite{VoschiBochkovSotirov}. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\section[Интервальные уравнения и системы уравнений]% 
        {Интервальные уравнения\\* и системы уравнений} 
\label{InteSystemSect} 
  
В этом параграфе рассмотрим подробнее основной математический объект, который 
возникает и используется при решении задачи восстановления функциональных зависимостей 
по интервальным данным --- интервальные системы уравнений. 
  
Как известно, \emph{уравнением} называется равенство с неизвестной переменой или 
несколькими переменными. Обычно их значения предполагается найти из условий, определяемых 
этими уравнениями. \emph{Решением} уравнения называется значение неизвестной переменной 
(или же набор значений неизвестных переменных, если их несколько), которые превращают 
это уравнение в верное равенство.\index{уравнение}\index{решение уравнения} 
  
\emph{Система уравнений} --- это набор уравнений, рассматриваемый\index{система уравнений} 
совместно друг с другом, т.\,е. вместе, как одно целое. \emph{Решением} системы уравнений 
называется набор значений неизвестных переменных, присутствующих в уравнениях системы,
который обращает в истинные равенства все уравнения этой системы. 
  
Составление различных уравнений и их решение --- один из древнейших и наиболее 
разработанных математических инструментов для решения практических задач. Вспомним 
из школьной математики <<метод уравнений>> и его преимущества перед арифметическими 
методами решения задач! В последние столетия понятие уравнения чрезвычайно расширилось 
и охватило собой многие новые области. Неизвестные переменные в уравнениях могут 
означать числа, векторы, матрицы, функции. В последнем случае говорят о <<функциональных 
уравнениях>>, и их популярнейшим примером являются дифференциальные и интегральные 
уравнения. В этой книге мы рассмотрим так называемые интервальные уравнения, которые 
возникли и вошли в математику в последние десятилетия XX века. 
  
Предположим, что интересующее нас уравнение или система уравнений помимо неизвестных 
переменных содержит также какие-то параметры, т.\,е. переменные, значения которых 
известны, но могут меняться в заданных для них множествах. При этом, строго говоря, 
изменяются также уравнение или система уравнений, которые их содержат, хотя общая 
структура уравнений и их природа, как правило, не меняется. Числовые уравнения остаются 
числовыми, векторные и матричные --- векторными и матричными уравнениями и т.\,д. 
Будем говорить, что задан \emph{интервальный параметр},\index{интервальный параметр} 
если областью изменения параметра назначен интервал (интервальный вектор или матрица). 
Если, скажем, рассматривалось уравнение $F(x, a) = b$ с некоторым выражением $F$ 
(которое может быть векторным), неизвестной переменной $x$ и параметрами $a$ и $b$, 
которые могут изменяться в пределах интервалов $\mbf{a}$ и $\mbf{b}$ (и которые также 
могут быть многомерными), то можно кратко и выразительно описать эту ситуацию 
с помощью интервального уравнения 
\begin{equation} 
\label{IntervalEqn} 
F(x, \mbf{a}) = \mbf{b}. 
\end{equation}
   
\begin{definition} 
\textsl{Интервальным уравнением} (интервальной системой уравнений) называется 
уравнение (система уравнений) с одним или несколькими интервальными параметрами, 
т.\,е. параметрами, которые могут изменяться в пределах заданных интервалов.% 
\index{интервальное уравнение}\index{интервальная система уравнений}% 
\end{definition} 
   
Неформально говоря, интервальное уравнение или система уравнений --- это совокупность 
обычных (точечных) уравнений или систем уравнений, параметры которых могут принимать 
значения из заданных интервалов. Интервальные уравнения, в действительности, часто 
возникают в математическом моделировании различных явлений и процессов. 
   
\begin{example} 
Рассмотрим задачу прокладки трассы лазерного пучка для позиционирования различных 
устройств, находящихся на расстоянии  нескольких десятков метров друг от друга. 
Такая задача возникает, например, при лазерном зондировании плазмы на токамаках 
\cite{OpticsSchema}.

Одной из подзадач этой задачи является вычисление площади пересечения сечений 
контрольного пучка и области видимости телескопа на различных расстояниях от лазера. 
Пространственная задача может решаться с приемлемой точностью как пересечение цилиндра 
и конуса. 
%Перед каждым зеркалом в момент измерения располагают экран. Пространственная область 
%телескопа \cite{Telescope} с хорошей точностью описывается круговым цилиндром 
%с центром ($x_t, y_t$) и радиусом $R$. Пространственная область лазера является 
%эллиптическим конусом небольшой расходимости \cite{Laser}. В круглое поле визирной 
%трубы попадает часть сечения конуса излучения лазера с координатами центра эллипса 
%($x_l, y_l$). 
В контрольных сечениях задача сводится к решению системы нелинейных алгебраических 
уравнений второй степени для нахождения точек $(x_1, y_1)$ и $(x_2, y_2)$ пересечения 
круга и эллипса. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{figure}[h!tb]
	\unitlength=1mm
	\centering\small  
	\begin{picture}(110, 40)
		\put(0,5){\includegraphics[width=50mm]{pictures/Laser3d.png}}
		\put(60,0){\includegraphics[width=50mm]{pictures/LaserModel.png}}		
		\put(53,18){\large$\Rightarrow$}
		\put(60.5,19){\line(1,0){48}}
		\put(85,1){\line(0,1){35}}
		\put(73,20){\normalsize$\mbf{a}$}
		\put(86,25){\normalsize$\mbf{b}$}
	\end{picture}
	\caption{Профиль мощности лазера и модель эллипса}
	для системы уравнений \eqref{ConeCylIntersect}. 
	\label{LaserProfile}
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
В системе координат, в которой оси совпадают с главными полуосями эллипса, имеем
\begin{equation}
\label{ConeCylIntersect} 
\left\{ \ 
\begin{aligned} 
(x-x_t)^2 + (y-y_t)^2 &= R^2,   \\[3pt] 
\frac{(x-x_l)^2}{a^2} + \frac{(y-y_l)^2}{b^2} &= 1. 
\end{aligned} 
\right. 
\end{equation}
Индексы $t$, $l$ соответствуют телескопу и лазеру. Первое уравнение в системе 
\eqref{ConeCylIntersect} описывает область видимости телескопа, а второе уравнение 
отвечает сечению конуса светимости лазера. 
   
Для реального пучка лазера параметры $a$, $b$ эллипса находят экспериментально 
Измеренный профиль светимости лазера KLM-532-100 (см. \cite{Laser}) показан 
на Рис.~\ref{LaserProfile}. Слева --- результат измерения CCD-матрицы фотодетектора, 
справа --- математическая модель <<толстого>> эллипса. Синим цветом на правом рисунке 
дан один из уровней постоянной мощности лазерного излучения. Параметры внешнего 
и внутреннего эллипсов дают интервалы $\mbf{a}$ и $\mbf{b}$ возможных значений 
параметров $a$, $b$. Как следствие, вместо \eqref{ConeCylIntersect} получаем 
интервальую систему уравнений 
\begin{equation*}
\left\{ \ 
\begin{aligned} 
(x-x_t)^2 + (y-y_t)^2 & = R^2,   \\[3pt] 
\frac{(x-x_l)^2}{\mbf{a}^2} + \frac{(y-y_l)^2}{\mbf{b}^2} & =1. 
\end{aligned} 
\right. 
\end{equation*} 
Это совокупность всевозможных систем уравнений вида \eqref{ConeCylIntersect}, 
у которых параметры $a$ и $b$ принимают значения в интервалах $\mbf{a}$ и $\mbf{b}$ 
соответственно. 
\end{example} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\section[Решения интервальных уравнений и систем]% 
        {Решения интервальных \\*  уравнений и систем} 
  
Что считать <<решением>> интервального уравнения или интервальной системы уравнений? 
Отвечая на этот вопрос, естественно отталкиваться от решений отдельных точечных 
уравнений (систем уравнений), образующих интервальное уравнение (систему уравнений). 
Для каждого точечного уравнения (системы уравнений) понятие решения определено, и 
потому для интервального уравнения (системы уравнений) <<решение>> должно составляться 
из этих отдельных решений точечных уравнений. Но как именно? 
  
Ясно, что на этом пути вместо традиционного понятия <<решения>> уравнения или системы 
уравнений мы получим некоторые множества, совокупности отдельных решений точечных 
уравнений (систем уравнений) образующих интервальное уравнение (систему уравнений). 
Поэтому имеет смысл начать говорить о <<множествах решений>> интервальных уравнений 
(систем уравнений).  \index{множество решений} 
  
Самый простой и, по-видимому, наиболее естественный способ образования множества 
решений интервального уравнения --- объединить все индивидуальные решения точечных 
уравнений, которые его составляют. Для уравнения $F(x, \mbf{a}) = \mbf{b}$ нужно 
взять множество решений в виде 
\begin{equation} 
\label{USSdefi11} 
\USS (F, \mbf{a}, \mbf{b}) \  = \  
   \bigcup_{a\in\mbf{b}} \  \bigcup_{b\in\mbf{b}} \  
   \{\; \tilde{x} \mid F(\tilde{x}, a) = b \;\}. 
\end{equation} 
Это множество решений называется \emph{объединённым множеством решений} интервального 
уравнения $F(x, \mbf{a}) = \mbf{b}$.\index{объединённое множество решений} Оно было 
исторически первым и по настоящее время является одним из наиболее популярных 
множеств решений интервальных уравнений и систем уравнений. 
  
Определение \eqref{USSdefi11} можно переписать в следующем равносильном виде 
\begin{equation} 
\label{USSdefi22} 
\USS (F, \mbf{a}, \mbf{b}) \  = \   \bigl\{\; \tilde{x} 
   \mid (\exists a\in\mbf{a})(\exists b\in\mbf{b})\; F(\tilde{x}, a) = b \;\bigr\}, 
\end{equation} 
который основан на так называемой <<аксиоме выделения>> формальной теории множеств 
(см. \cite{Kleene,Uspenskii}). Достоинство представления \eqref{USSdefi22} состоит 
в том, что оно ясно показывает тип интервальной неопределённости, соответствующий 
интервальным параметрам $\mbf{a}$ и $\mbf{b}$ и, как следствие, позволяет более 
строго анализировать содержательный смысл этого множества и строить обобщения. 
  
При восстановлении линейной зависимости по интервальным данным мы будем иметь дело 
с интервальными системами линейных алгебраических уравнений вида 
\begin{equation*} 
\arraycolsep=2pt 
\left\{ \ 
\begin{array}{ccccccccc}
\mbf{a}_{11} x_1 &+& 
   \mbf{a}_{12} x_2 &+& \ldots &+& \mbf{a}_{1m} x_m &=& \mbf{b}_{1},  \\[3pt] 
\mbf{a}_{21} x_1 &+& 
   \mbf{a}_{22} x_2 &+& \ldots &+& \mbf{a}_{2m}\beta_m &=& \mbf{b}_{2}, \\[3pt] 
 \vdots && \vdots && \ddots && \vdots && \vdots                         \\[3pt]  
\mbf{a}_{n1} x_1 &+& 
   \mbf{a}_{n2} x_2 &+& \ldots &+& \mbf{a}_{nm} x_m &=& \mbf{b}_{n}, 
\end{array} 
\right. 
\end{equation*} 
или, кратко, 
\begin{equation*}
\mbf{A}x = \mbf{b},
\end{equation*} 
где $\mbf{A} = (\mbf{a}_{ij})$ --- интервальная $n\times m$-матрица, $\mbf{b} = 
(\mbf{b}_{i})$ --- интервальный $n$-вектор, которые образованы данными измерений. 
Её объединённым множеством решений является множество всевозможных решений систем 
линейных уравнений $Ax = b$ для матриц $A\in\mbf{A}$ и векторов правой части 
$b\in\mbf{b}$, т.\,е. множество 
\begin{equation*} 
\USS(\mbf{A}, \mbf{b}) \  = \  \bigl\{\, x\in\mbb{R}^{m} 
   \mid \text{$Ax = b$ для каких-то $A\in\mbf{A}$ и $b\in\mbf{b}$}\,\bigr\}. 
\end{equation*} 
На языке формальной логики и теории множеств это определение можно переписать в виде 
\begin{equation} 
\label{LinUSSdefi}
\USS(\mbf{A}, \mbf{b}) \  = \  \bigl\{\, x\in\mbb{R}^{m} 
   \mid (\exists A\in\mbf{A})(\exists b\in\mbf{b})(Ax = b)\,\bigr\},  
\end{equation} 
из которого видно, что все интервальные параметры в матрице и правой части имеют 
E-неопределённость (см. \S\ref{DualUncertSect}).\index{объединённое множество решений} 
Справедливо теоретико-множественное представление 
\begin{equation} 
\label{UnionRepres} 
\arraycolsep=2pt 
\begin{array}{rcl}
\USS\Ab \   
&=& \displaystyle\bigcup_{A\in\mbf{A}} \  \bigcup_{b\in\mbf{b}} \ 
                              \{\, x\in\mbb{R}^m \mid  Ax = b\,\}             \\[7mm] 
&=& \displaystyle\bigcup_{A\in\mbf{A}} \{\, x\in\mbb{R}^m \mid  Ax\in\mbf{b}\,\}. 
\end{array} 
\end{equation} 
  
Дальнейшее развитие интервального анализа привело к необходимости введения и других 
множеств решений для интервальных уравнений и систем уравнений. В начале 70-х годов 
прошлого века появилось \textit{допусковое множество решений}.\footnote{В ранних 
публикациях на эту тему точки допускового   множества решений назывались терминами 
restricted solutions, inner solutions и др.} Для интервального уравнения 
\eqref{IntervalEqn} его определение записывается как 
\begin{align} 
\TSS (F, \mbf{a}, \mbf{b}) \         \label{TSSdefi} 
&= \  \bigl\{\; \tilde{x} \mid 
    (\forall a\in\mbf{a})\; F(\tilde{x}, a)\in\mbf{b}\;\bigr\} \\[2mm] 
&= \  \bigl\{\; \tilde{x} \mid   \notag  
    (\forall a\in\mbf{a})(\exists b\in\mbf{b})\; F(\tilde{x}, a)\in\mbf{b}\;\bigr\}. 
\end{align} 
Это определение отличается от определения \eqref{USSdefi22} объединённого множества 
решений тем, что интервальный параметр $a\in\mbf{a}$ имеет в нём другой тип 
непределённости --- A-неопределённость вместо E-неопределённости. 
  
Для интервальной линейной системы уравнений допусковое множество решений --- 
это множество  \index{допусковое множество решений} 
\begin{equation*} 
\TSS(\mbf{A}, \mbf{b}) \  = \  \bigl\{\, x\in\mbb{R}^{m} 
   \mid \text{для любой $A\in\mbf{A}$ имеет место $Ax\in\mbf{b}$}\,\bigr\}, 
\end{equation*} 
которое образовано всеми такими векторами $x$, что произведение $Ax$ попадает 
в интервал правой части $\mbf{b}$ при любых точечных матрицах $A$ из заданной 
интервальной матрицы $\mbf{A}$. На языке формальной логики и теории множеств 
это определение можно переписать в виде 
\begin{align} 
\TSS(\mbf{X}, \mbf{y}) \  
&= \  \bigl\{\,\beta\in\mbb{R}^{m} \mid  \label{LinTSSdefi}
               (\forall A\in\mbf{A})(Ax\in\mbf{b})\,\bigr\} \notag\\[2mm] 
&= \  \bigl\{\,\beta\in\mbb{R}^{m} \mid 
               (\forall A\in\mbf{A})(\exists b\in\mbf{b})(Ax = b)\,\bigr\},  
\end{align} 
Множество $\TSS(\mbf{A}, \mbf{b})$ также образовано решениями систем линейных уравнений 
$Ax = b$ для некоторых матриц $A\in\mbf{A}$ и векторов правой части $b\in\mbf{b}$, 
а потому тоже имеет право называться <<множеством решений>> для интервальной системы 
линейных алгебраических уравнений $\mbf{A}x = \mbf{b}$. Имеет место представление 
\begin{equation} 
\label{IsectRepres} 
\arraycolsep=2pt 
\begin{array}{rcl}
\TSS\Ab \   
&=& \displaystyle\bigcap_{A\in\mbf{A}} \  \bigcup_{b\in\mbf{b}} \ 
                              \{\, x\in\mbb{R}^n \mid  Ax = b\,\}             \\[7mm] 
&=& \displaystyle\bigcap_{A\in\mbf{A}} \{\, x\in\mbb{R}^n \mid  Ax\in\mbf{b}\,\}. 
\end{array} 
\end{equation} 
которое двойственно для \eqref{UnionRepres}: оно получается заменой операции 
объединения по $A\in\mbf{A}$ на пересечение. 
  
Из самого определения объединённого и допускового множеств решений интервальных 
уравнений следует, что справедливо включение 
\begin{equation*} 
\USS(F, \mbf{a}, \mbf{b})\;\supseteq\;\TSS(F,\mbf{a}, \mbf{b}). 
\end{equation*} 
В частности, для интервальных линейных систем уравнений 
\begin{equation*} 
\USS(\mbf{X}, \mbf{y})\;  \supseteq\; \TSS(\mbf{X}, \mbf{y}).    
\end{equation*} 
Равенства вместо этих включений имеют место, в частности, если $\mbf{a}$ --- 
вырожденный интервал, а матрица $\mbf{X}$ --- точечная (не интервальная). 
  
Определения \eqref{LinUSSdefi} и \eqref{LinTSSdefi} являются ясными и хорошо 
привязываются к практическим ситуациям, но работать с ними затруднительно, а ещё 
сложнее --- организовывать на их основе вычисления. Нужны другие эквивалентные 
способы описания этих множеств. Основываясь на свойствах интервальных арифметических 
операций, нетрудно показать, что точка $\tilde{x}$ из $\mbb{R}^n$ принадлежит 
объединённому множеству решений интервальной линейной системы уравнений 
$\mbf{A}x = \mbf{b}$ тогда и только тогда, когда 
\begin{equation*} 
\mbf{A}\cdot \tilde{x}\,\cap\,\mbf{b}\neq\varnothing, 
\end{equation*} 
или, что равносильно, 
\begin{equation*} 
0\in\mbf{A}\cdot \tilde{x} - \mbf{b}.  
\end{equation*} 
Этот результат известен как \emph{критерий Бекка} принадлежности точки объединённому 
множеству решений ИСЛАУ (см. \cite{SSharyBook}).  \index{критерий Бекка} 
  
С помощью полной интервальной арифметики Каухера (см. \S\ref{KaucherArithmSect}) этим 
условиям можно придать следующий элегантный вид \cite{SSharyBook}: точка $\tilde{x}$ 
из $\mbb{R}^n$ принадлежит объединённому множеству решений интервальной линейной 
системы уравнений $\mbf{A}x = \mbf{b}$ тогда и только тогда, когда 
\begin{equation} 
\label{USScharact} 
\dual\mbf{A}\cdot \tilde{x}  \,\subseteq\,\mbf{b}.  
\end{equation} 
Аналогично, точка $\tilde{x}$ из $\mbb{R}^n$ принадлежит допусковому множеству 
решений интервальной линейной системы уравнений $\mbf{A}x = \mbf{b}$ тогда и только 
тогда, когда 
\begin{equation} 
\label{TSScharact}
\mbf{A}\cdot \tilde{x}\,\subseteq\,\mbf{b}. 
\end{equation} 
Включения \eqref{USScharact} и \eqref{TSScharact} называют \emph{характеристическими 
включениями}\index{характеристическое включение} для точек объединённого и допускового 
множеств решений ИСЛАУ соответственно. 
  
Для интервальных систем нелинейных уравнений общего вида \eqref{IntervalEqn} 
теоретико-множественные условия принадлежности точки множествам решений формулируются 
аналогичным образом. Более точно, точка $\tilde{x}$ из $\mbb{R}^n$ принадлежит 
объединённому множеству решений $\USS(F,\mbf{a}, \mbf{b})$ интервальной системы 
уравнений \eqref{IntervalEqn} тогда и только тогда, когда 
\begin{equation} 
\label{UniSScriter} 
\bigl\{\,F(\tilde{x},a) \mid a\in\mbf{a}\,\bigr\}\cap\mbf{b}\neq\varnothing. 
\end{equation}
Точка $\tilde{x}$ из $\mbb{R}^n$ принадлежит допусковому множеству решений 
$\TSS(F,\mbf{a}, \mbf{b})$ интервальной системы уравнений \eqref{IntervalEqn} 
тогда и только тогда, когда 
\begin{equation}
\label{TolSScriter} 
\bigl\{\,F(\tilde{x},a) \mid a\in\mbf{a}\,\bigr\}\subseteq\mbf{b}. 
\end{equation} 
  
Но вычисление области значений $\{\,F(\tilde{x},a) \mid a\in\mbf{a}\,\}$ 
вектор-функции $F = (F_{1}, F_{2}, \ldots, F_{n})$ из \eqref{IntervalEqn} 
может быть очень непростым и существенно зависит от того, как именно входят 
интервальные параметры в отдельные компоненты $F_i$. Если компоненты $F(\tilde{x}, 
\mbf{a})$ оказываются связанными интервальными величинами, то область значений 
вектор-функции $\{\,F(\tilde{x},a) \mid a\in\mbf{a}\,\}$ \emph{не} является брусом, 
т.\,е. прямым декартовым произведением интервалов. Она может иметь более сложную 
форму, так что исследовать и оценивать такие множества и работать с ними существенно 
труднее. 
  
Тем не менее, в задачах восстановления по интервальным данным нелинейных 
зависимостей вида \eqref{ParamFunc}, 
\begin{equation*}
y = f(x,\beta), 
\end{equation*}
где зависимая переменная $y$ одномерна, мы приходим к необходимости решать 
интервальную нелинейную систему уравнений некоторого специального вида, 
для которого сформулированные выше затруднения успешно преодолеваются. Дело в том, 
что интервальные величины, отвечающие различным измерениям, независимы друг от друга 
(см. \S\ref{IndepIntvalSect}), а потому области значений вектор-функций являются 
прямыми декартовыми произведениями отдельных компонент.  
Как следствие, критерии принадлежности объединённому множеству решений 
\eqref{UniSScriter} и допусковому множеству решений \eqref{TolSScriter} получают 
более простой вид, так как распадаются на одномерные покоординатные отношения. 
  
Наконец, необходимо упомянуть \emph{формальные решения} интервальных уравнений 
и систем уравнений,\index{формальное решение} которые своей природой отличаются 
от рассмотренных выше множеств решений.  Интервал (интервальные вектор, матрица 
и т.\,п.) называется \textit{формальным решением} интервального уравнения (системы 
уравнений или неравенств), если подстановка этого интервала в рассматриваемое 
уравнение (систему уравнений или неравенств) и выполнение всех интервальных 
арифметических, аналитических и т.\,п. операций и отношений приводят к истинному 
соотношению. Например, интервалы $\mbf{x}_{1} = [-1,0]$, $\mbf{x}_{2} = [2, 3]$ 
является формальным решением для интервального уравнения   \label{FormalSols} 
\begin{equation*} 
[10, 11]\,\mbf{x}_{1} - [7, 10]\,\mbf{x}_{2} = [3, 30]. 
\end{equation*} 
  
Формальные решения интервальных уравнений и систем уравнения соответствуют, как видим, 
традиционному общематематическому пониманию решений уравнений. Выделение для них 
отдельного специального термина вызвано, скорее, историческими причинами. Формальные 
решения находят важные и практичные применения в современном интервальном анализе, 
в частности, с помощью формальных решений можно эффективно находить внутренние и 
внешние оценки различных множеств решений интервальных уравнений (см., например, 
\cite{SSharyIzvAN97, SSharyBook}). Но изложение этой техники выходит за рамки 
настоящей книги. 
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
   
\section[Общий случай задачи восстановления зависимостей]%
        {Общий случай задачи \\* восстановления зависимостей} 
\label{GenIDataFitSect} 
  
  
Рассмотрим теперь случай, когда неопределённость присутствует как в измерениях 
значений зависимой переменной, так и в измерениях значений аргументов 
(Рис.~ \ref{UncertBoxesPic}). Это может быть вызвано различными причинами. 
Например, существенно неточное измерение входных переменных происходит 
в ситуациях, когда они должны устанавливаться в течение значительного времени 
(см. Пример~\ref{MeasrConstChap}.\arabic{BazhenovExmp}). Тогда их уместно выразить 
какими-то интервалами, а не точечными значениями. 
  
\begin{example} 
В работе А.П.\,Вощинина, А.Ф.\,Бочкова и Г.Р.\,Сотирова \cite{VoschiBochkovSotirov} 
в качестве практического примера восстановления линейной зависимости по интервальным 
данным рассматривается построение модели производительности котлоагрегата для выработки 
пара высокого давления. Этот пример вошёл также в книгу \cite{VoschininSotirov}. 
  
Основной эксплуатационной характеристикой котлоагрегата в этой задаче является 
функциональная зависимость $y = g(x)$, описывающая расход $y$ произведённого пара 
на выходе агрегата в зависимости от расхода топлива $x$ на входе. В результате 
испытаний котлоагрегата получаются несколько (не более десятка) измерений расхода 
пара на выходе, причём эти измерения выполняются, как правило, с заметной погрешностью: 
её относительная величина достигает $5\%$. 
  
Рассмотренные в работе \cite{VoschiBochkovSotirov} котлоагрегаты конкретного типа 
неплохо изучены энергетиками, и известно, что зависимость $y = g(x)$ является для них 
гладкой неубывающей функцией. Она может быть хорошо приближена линейной функцией 
\begin{equation*} 
y = \beta_{0} + \beta_{1}x,
\end{equation*} 
для которой в \cite{VoschiBochkovSotirov} по конкретным данным успешно строятся оценки 
параметров $\beta_0$ и $\beta_1$.\footnote{Числовые данные этого примера и результаты 
расчётов с ними можно увидеть в Интернете в электронном приложении к книге по адресу 
\url{https://github.com/szhilin/octave-interval-examples/blob/master/SteamGenerator.ipynb}.}
Далее они подвергаются анализу на предмет их значимости и вариабельности, строится 
прогнозный коридор. 
  
Но в дискуссии, развернувшейся на страницах журнала <<Заводская лаборатория>> 
вокруг работы \cite{VoschiBochkovSotirov} (см., в частности, заметку 
\cite{DemidenkoNote}), справедливо отмечалось, что в постановке задачи исходные 
предпосылки являются не самыми общими и это отражается и на практической ценности 
результатов. Трудно представить, что расход пара на выходе котлоагрегата измеряется 
с погрешностью, но на входе расход топлива или его теплотворная способность измеряются 
совершенно точно. Как следствие, приходим к необходимости учёта интервальной 
погрешности для входной переменной $x$. 
\end{example} 
  
Если выборка измерений независимых переменных и зависимой переменной --- накрывающая, 
то 
\begin{equation*} 
\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_m x_{im} \in\mbf{y}_{i}, 
\qquad  i = 1,2,\ldots,n, 
\end{equation*} 
где все $x_{i1}$ могут принимать значения из соответствующих интервалов $\mbf{x}_{i1}$, 
$i = 1,2,\ldots,n$, $j = 1,2,\ldots,m$. Как следствие, получаем интервальную систему 
линейных алгебраических уравнений 
\begin{equation} 
\label{InLinEqSys} 
\arraycolsep=2pt 
\left\{ \ 
\begin{array}{ccccccccccc}
\beta_0 &+& \mbf{x}_{11}\beta_1 &+& 
   \mbf{x}_{12} \beta_2 &+& \ldots &+& \mbf{x}_{1m}\beta_m &=& \mbf{y}_{1}, \\[3pt] 
\beta_0 &+& \mbf{x}_{21}\beta_1 &+& 
   \mbf{x}_{22} \beta_2 &+& \ldots &+& \mbf{x}_{2m}\beta_m &=& \mbf{y}_{2}, \\[3pt] 
 \vdots &&  \vdots && \vdots && \ddots && \vdots && \vdots                  \\[3pt]  
\beta_0 &+& \mbf{x}_{n1}\beta_1 &+& 
   \mbf{x}_{n2} \beta_2 &+& \ldots &+& \mbf{x}_{nm}\beta_m &=& \mbf{y}_{n}. 
\end{array} 
\right. 
\end{equation} 
Это формальная запись, означающая совокупность обычных (точечных) систем линейных 
алгебраических уравнений того же размера и с теми же неизвестными переменными, 
у которых коэффициенты и правые части лежат в предписанных им интервалах 
(см. \cite{SSharyBook}). Восстановление параметров линейной зависимости можно 
рассматривать как <<решение>>, в том или ином смысле, выписанной интервальной системы 
уравнений. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb]
\centering\small 
\unitlength=1mm 
\begin{picture}(80,44)
    \put(0,1){\includegraphics[width=80mm]{pictures/OneEqSolSet.eps}}
    \put(41,1){$\beta_1$} \put(-2,22){$\beta_2$}  
\end{picture} 
\caption{Объединённое множество решений интервального} 
линейного уравнения $[1,2]\beta_{1} + [2, 3]\beta_{2} = [10, 12]$.
\label{OneEqSoluPic}  
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
В случае присутствия погрешностей как в измерениях аргумента, так и в измерениях 
зависимости множество параметров зависимостей, совместных (согласующихся) с данными, 
характеризуются новыми свойствами, которыми не обладают задачи с точным заданием 
входных переменных. Прежде всего, множества решений отдельных интервальных уравнений 
уже не являются полосами в пространстве $\mbb{R}^n$, вроде тех, что изображены 
на Рис.~\ref{UncertStripesPic}. Они выглядят существенно иначе, и их конкретный вид 
зависит от того, какой смысл вкладывается в понятие совместности (согласования) 
параметров и данных, т.\,е. от того, какое множество решений ИСЛАУ взято в качестве 
информационного множества (см. Рис.~\ref{OneEqSoluPic}). 
  
Дело в том, что теперь само понятие совместности (согласования) параметров и данных 
должно быть расширено и переосмыслено. В обычном неинтервальном случае результаты 
измерений --- это бесконечно малые и бесструктурные точки. Прохождение через них 
графика функциональной зависимости адекватно описывается двумя значениями --- <<да>> 
или <<нет>>, т.\,е. имеет булевский (логический) тип данных. Если мы переходим от точек 
к брусам неопределённости, имеющим заметные размеры и свою структуру, то прохождение 
графика зависимости через них можно понимать по-разному. Брус неопределённости 
измерений $(\mbf{x}_{i1}, \mbf{x}_{i2}, \ldots, \mbf{x}_{im}, \mbf{y}_{i})$ является 
прямым декартовым произведением интервалов по различным осям координат, и эти оси 
имеют разный смысл: интервалы $\mbf{x}_{i1}$, $\mbf{x}_{i2}$, \ldots, $\mbf{x}_{im}$ 
соответствуют входным (экзогенным, предикторным) переменным, а интервал $\mbf{y}_{i}$ 
соответствует выходной (эндогенной, критериальной) переменной. По этой причине 
становится важным, как именно проходит график восстанавливаемой зависимости через 
брусы неопределённости измерений (см. Рис.~\ref{BoxLineIxPic}). 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[htb]
\centering\small 
\unitlength=1mm 
\begin{picture}(82,30)
    \put(0,0){\includegraphics[width=82mm]{pictures/BoxLineIntersect.eps}} 
\end{picture} 
\caption{Различные способы пересечения линии с брусом}
неопределённости измерения зависимости. 
\label{BoxLineIxPic}  
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
Функциональную зависимость назовём \textit{слабо совместной} с интервальными данными, 
если её график проходит через каждый брус неопределённости измерений хотя бы для одного 
значения аргумента. Наглядно это означает, что график зависимости пересекает брусы 
неопределённости, но как именно --- неважно (средний чертёж на Рис.~\ref{BoxLineIxPic}), 
достаточно не менее одной точки пересечения. Для случая линейной зависимости это условие 
наиболее удобно выразить с помощью формального языка логического исчисления предикатов: 
\begin{equation*}
\index{слабо совместная зависимость} 
\begin{array}{l}
(\exists x_{i1}\in\mbf{x}_{i1}) \cdots (\exists x_{im}\in\mbf{x}_{im}))
                                                    (\exists y_{i}\in\mbf{y}_{i}) \\[1pt]  
\qquad \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_m x_{im} = y_{i}, 
\qquad i = 1,2,\ldots,n. 
\end{array} 
\end{equation*} 
Равносильная упрощённая формулировка этого свойства выглядит следующим образом: 
\begin{equation*}
\begin{array}{l}
(\exists x_{i1}\in\mbf{x}_{i1}) \cdots (\exists x_{im}\in\mbf{x}_{im}) \\[1pt]  
\qquad \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_m x_{im} \in\mbf{y}_{i}, 
\qquad i = 1,2,\ldots,n. 
\end{array} 
\end{equation*} 
  
Функциональную зависимость назовём \textit{сильно совместной} с интервальными данными, 
если её график проходит через каждый брус неопределённости измерений для любого значения 
аргумента из интервалов неопределённости входных переменных. Наглядно это означает, что 
график зависимости целиком содержится в коридорах, задаваемых интервалами выходной 
переменной при всех значениях входных переменных из соответствующих им интервалов 
(левый чертёж на Рис.~\ref{BoxLineIxPic}). Для случая линейной зависимости это условие 
может быть формально записано в следующем виде:  
\begin{equation*} 
\index{сильно совместная зависимость} 
\begin{array}{l}
(\forall x_{i1}\in\mbf{x}_{i1}) \cdots (\forall x_{im}\in\mbf{x}_{im}) 
                                                    (\exists y_{i}\in\mbf{y}_{i}) \\[1pt]   
\qquad \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_m x_{im} = y_{i}, 
\qquad i = 1,2,\ldots,n. 
\end{array} 
\end{equation*} 
Равносильная упрощённая формулировка этого свойства выглядит следующим образом: 
\begin{equation*} 
\begin{array}{l}
(\forall x_{i1}\in\mbf{x}_{i1}) \cdots (\forall x_{im}\in\mbf{x}_{im}) \\[1pt]  
\qquad \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_m x_{im} \in\mbf{y}_{i}, 
\qquad i = 1,2,\ldots,n. 
\end{array} 
\end{equation*} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[htb]
\centering\small 
\unitlength=1mm 
\begin{picture}(67,50)
    \put(0,0){\includegraphics[width=67mm]{pictures/WeakStrongCmp.eps}}
    \put(17,33){\mbox{\begin{tabular}{c}слабо совместная\\[-1pt] зависимость\end{tabular}}} 
    \put(41,24){\mbox{\begin{tabular}{c}сильно совместная\\[-1pt] зависимость\end{tabular}}} 
\end{picture} 
\caption{Линейные зависимости с разными типами} 
            согласования с данными.
\label{WeakStrongPic}  
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
На Рис.~\ref{BoxLineIxPic} имеется ещё правый чертёж, соответствующий ситуации, когда 
график зависимости лежит в коридоре, задаваемом интервалом входной переменной $\mbf{x}$, 
при любых значениях выходной переменной $y$ из соответствующего ей интервала $\mbf{y}$. 
Она описываемой следующей логической формулой: 
\begin{equation*}
\begin{array}{l}
(\forall y_{i}\in\mbf{y}_{i}) 
(\exists x_{i1}\in\mbf{x}_{i1}) \cdots (\exists x_{im}\in\mbf{x}_{im}) \\[1pt]  
\qquad \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_m x_{im} = y_{i}, 
\qquad i = 1,2,\ldots,n. 
\end{array} 
\end{equation*} 
Подобный способ прохождения графика зависимости через брус неопределённости измерения 
также имеет смысл, но его практическое значение существенно меньше двух перечисленных 
выше, и потому мы его подробно не рассматриваем. 
  
Понятие слабой совместности интуитивно понятно и, как правило, больших вопросов 
не вызывает. В чём содержательный смысл сильной совместности? 
  
На практике измерения на входах и выходах системы осуществляются, как правило, 
разными способами и даже в разное время. Мы измеряем выход (зависимую переменную) 
обычно уже тогда, когда входные значения (независимых переменных) зафиксированы, и мы 
их измерили. Получив при этом какие-то интервалы. Сильная совместность функциональной 
зависимости с интервальными данными означает тогда, что выходная величина остаётся 
в пределах измеренного для неё интервала вне зависимости от того, какими конкретно 
в своих интервалах являются значения входных переменных. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[htb]
\centering\small 
\unitlength=1mm 
\begin{picture}(70,45)
    \put(0,0){\includegraphics[width=67mm]{pictures/OverLappedBoxes.eps}}
\end{picture} 
\caption{Сложный случай восстановления зависимости} 
        \;по широким перекрывающимся интервальным данным. 
\label{OverlapDataPic}
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
Затронутые выше вопросы впервые были подробно рассмотрены в работах \cite{SSharyJCT2017, 
SSharyADSAA}, где показано, что требование сильной совместности параметров и данных 
позволяет более адекватно обрабатывать сложные случаи восстановление зависимостей 
по широким и существенно <<перекрывающимся>> интервальным данным (похожим на те, 
что изображены на  Рис.~\ref{OverlapDataPic}). В частности, множество параметров 
зависимости, сильно совместных с интервальными данными, почти всегда ограниченно, 
т.\,е. имеет конечную вариабельность. Для слабой совместности это не так, и 
соответствующее информационное множество может быть неограниченным, что противоречит 
здравому смыслу и создаёт большие технические неудобства.  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
К настоящему моменту предложено несколько методов восстановления линейных 
зависимостей по интервальным данным, каждый из которых отталкивается от одной общей 
идеи, но далее обработку данных проводит по-своему. Это метод центра неопределённости 
\cite{ZhilinDiss, OskorbinMaksiZhilin, Zhilin2005}, метод максимума совместности 
(максимума согласования) \cite{SShary2012, SharysJCT2013, SSharyIzvAN2017, SSharyPLab2020, 
SSharyADSAA}, <<метод простого интервального оценивания>> \cite{Rodionova}, метод 
польского физика М.\,Гутовски \cite{Gutowski} (<<нарезка>> информационного множества), 
метод парциальных информационных множеств \cite{Kumkov2010, Kumkov2013} и др. 
Перечисленные методы ориентированы на разные задачи, используют разные предположения 
об обрабатываемых данных. Иногда они дают одинаковые или близкие результаты, иногда 
--- разные. Краткому обзору этих методов посвящён \S\ref{SurveyMethodSect}. 
  
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   где-то здесь нужно поместить критический обзор популярных свойств оценок 
%   из вероятностной статистики и их уместность в статистике интервальных данных 
%   - несмещённости, эффективности, и т.п. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\section[Приближение и оценивание информационного множества]% 
        {Приближение и оценивание \\* информационного множества} 
\label{ApprInfoSetSect} 
  
В общем случае для функциональной зависимости \eqref{LinFunc} информационное множество 
является подмножеством пространства $\mbb{R}^{m+1}$ размерности два и более. При этом 
информационное множество может иметь довольно сложную форму, которая не обязательно 
совпадает с многомерными интервалами-брусами. 
  
\begin{example} 
На Рис.~\ref{SampleIDataPic} показаны объединённое и допусковое множества решений 
интервальной линейной системы 
\begin{equation} 
\label{Sample2DSys} 
\left( 
\begin{array}{@{\,}cc@{\,}} 
1 & [0, 1] \\[3pt]
1 & [1, 2] \\[3pt]
1 & [2, 3] 
\end{array} 
\right)
\begin{pmatrix}
\beta_{0} \\[3pt] \beta_{1} 
\end{pmatrix}
= 
\left(
\begin{array}{@{\,}c@{\,}}
[4, 10] \\[3pt]  
[8, 11] \\[3pt]  
[6, 14]
\end{array}
\right). 
\end{equation} 
Она соответствует задаче восстановления линейной зависимости вида \eqref{LinFunc} 
по данным измерений, которые изображены на Рис.~\ref{SampleIDataPic} слева 
(вместе с некоторыми прямыми, совместными с этими данными). 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   
\begin{figure}[ht]
\centering\small 
\unitlength=1mm
\begin{picture}(40,36) 
\put(0,4){\includegraphics[width=40mm]{pictures/SampleInteData.eps}} 
\small\put(37,3.5){$x$}\put(4,39){$y$} 
      \put(1,6.7){$0$}   \put(10.4,6.7){$1$} 
      \put(18.5,6.7){$2$}\put(26.7,6.7){$3$} 
      \put(34.7,6.7){$4$} 
      \put(1,14){$4$} \put(1,26.2){$8$} \put(-0.6,34.5){$12$} 
\end{picture} 
\hspace{8mm} 
\begin{picture}(56,47) 
\put(0,-1){\includegraphics[width=56mm]{pictures/SampleSolSets.eps}} 
\put(52,10){$\beta_{0}$} \put(17.5,41){$\beta_{1}$} 
\small\put(4,14.5){$-4$} \put(17.4,14.5){$0$} \put(25,14.5){$4$} 
      \put(34,14.5){$8$} \put(44,14.5){$12$}   
      \put(12,3.5){$-4$} \put(17.4,22.5){$4$} \put(17.4,37){$8$}   
\end{picture} 
\caption{Наглядное представление интервальных данных,} 
соответствующих интервальной линейной системе \eqref{Sample2DSys},\\
и её множества решений. 
\label{SampleIDataPic} 
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
Это множество решений является невыпуклым многоугольником с восемью вершинами,  
\begin{equation*} 
\begin{pmatrix}  0 \\  4 \end{pmatrix}, \quad 
\begin{pmatrix}  8 \\  0 \end{pmatrix}, \quad 
\begin{pmatrix} 10 \\ -2 \end{pmatrix}, \quad 
\begin{pmatrix} 14 \\ -4 \end{pmatrix}, \quad 
\begin{pmatrix} 10 \\  0 \end{pmatrix}, \quad  
\begin{pmatrix} 10 \\  1 \end{pmatrix}, \quad
\begin{pmatrix}  8 \\  3 \end{pmatrix}, \quad
\begin{pmatrix} -6 \\ 10 \end{pmatrix},  
\end{equation*} 
которые перечислены по направлению обхода против часовой 
стрелки.\footnote{На Рис.~\ref{SampleIDataPic} многоугольники множеств решений и их 
вершины построены с помощью свободно распространяемого пакета \texttt{IntLinIncR2}
\cite{IreneSoft}.} Оно простирается в три квадранта (из четырёх) пространства 
$\mbb{R}^2$. 
  
Допусковое множество решений интервальной линейной системы \eqref{Sample2DSys} 
--- это маленький пятиугольник внутри большого многоугольника объединённого 
множества решений. Его вершинами являются точки 
\begin{equation*} 
\begin{pmatrix}  8 \\  0 \end{pmatrix}, \quad 
\begin{pmatrix} 10 \\ -1 \end{pmatrix}, \quad 
\begin{pmatrix} 10 \\  0 \end{pmatrix}, \quad 
\begin{pmatrix}  9 \\  1 \end{pmatrix}, \quad 
\begin{pmatrix}  5 \\  3 \end{pmatrix}, 
\end{equation*} 
которые перечислены также против часовой стрелки. 
\end{example} 

Система \eqref{Sample2DSys} сходна по виду с системой \eqref{InLinEqSys}, а интервалы 
коэффициентов при неизвестных переменных $\beta_i$ и интервальные правые части намеренно 
взяты неотрицательными, чтобы придать этому примеру реалистичность. Но можно показать 
(см. подробности в \cite{SSharyBook}), что и в самом общем случае множества решений 
интервальных линейных систем выглядят похожим образом. Они являются объединениями 
нескольких выпуклых многогранных (полиэдральных) множеств, число которых может 
быстро расти с увеличением количества неизвестных параметров. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb]
\centering\small 
\unitlength=1mm 
\begin{picture}(100,57)
    \put(0,0){\includegraphics[width=100mm]{pictures/TriDSoluSet.eps}} 
    \put(63,10){$\beta_0$} \put(8,7){$\beta_1$} \put(-3,36){$\beta_2$} 
\end{picture} 
\caption{Множество решений интервальной} 
линейной системы уравнений \eqref{3DSampleSys}. 
\label{TriDSolSetPic}
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{example} 
Для интервальной системы линейных алгебраических уравнений 
\begin{equation} 
\label{3DSampleSys} 
\left( 
\begin{array}{ccc@{\,}}
1 & [-3, -2] &  [10, 12] \\[3pt] 
1 &  [4, 5]  &  [14, 16] \\[3pt] 
1 &  [7, 9]  & [-18, -15]\\[3pt]
1 & [-4, -2] &  [14, 15] 
\end{array} 
\right)
\begin{pmatrix} 
\beta_{0} \\[3pt] \beta_{1} \\[3pt] \beta_{2} 
\end{pmatrix} 
= 
\left( 
\begin{array}{@{\,}c@{\,}} 
[10, 12]\\[3pt] [18, 22]\\[3pt] [24, 26]\\[3pt] [6, 7] 
\end{array} 
\right) 
\end{equation} 
объединённое множество решений показано с разных точек зрения на Рис.~\ref{TriDSolSetPic} 
и Рис.~\ref{3DSolSetPic}. Они нарисованы с помощью системы компьютерной математики 
\textsc{Matlab} и пакета программ \texttt{IntLinIncR3} \cite{IreneSoft}.\footnote{В основе 
программ этого пакета лежит очень общий и мощный метод визуализации полиэдральных множеств 
решений --- метод граничных интервалов \cite{IreneJCT2015,IreneRC2014}.}   Как видим, 
множество решений является многогранником с 11 вершинами. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[htb]
\centering\small 
\unitlength=1mm 
\begin{picture}(80,70)
    \put(0,0){\includegraphics[width=80mm]{pictures/ThreeDSoluSet.eps}} 
    \put(15,20){$\beta_0$} \put(60,0){$\beta_1$} \put(1,50){$\beta_2$} 
\end{picture} 
\caption{Множество решений интервальной линейной системы} 
уравнений \eqref{3DSampleSys} (включён режим прозрачности рисунка). 
\label{3DSolSetPic}
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
  
Третья координата точек множества решений меняет знак, т.\,е. множество решений 
принадлежит двум ортантам и <<разрезается>> координатной гиперплоскостью $\beta_{2} = 0$. 
На ней граница множества решений <<ломается>>, так что в целом множество решений 
невыпукло. 
\end{example} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Другие выразительные изображения различных множеств решений интервальных систем 
уравнений можно увидеть, к примеру, в \cite{SSharyJCT2017, SSharyIzvAN2017, SSharyBook} 
и в инструкции к пакету программ \cite{IreneSoft}.  
  
Если матрица системы \eqref{InLinEqSys} уравнений --- точечная, т.\,е. коэффициенты 
при неизвестных $\beta_i$ являются обычными вещественными числами, то объединённое 
множество решений в целом является выпуклым. Но в общем случае, когда матрица 
интервальной системы линейных алгебраических уравнений существенно интервальна, то 
объединённое множество решений может быть невыпуклым. Допусковое множество решений 
всегда выпукло. В целом, количество гиперплоскостей, ограничивающих множества 
решений, может быть очень большим. Подробное обсуждение этих результатов читатель 
может найти в книге \cite{SSharyBook}. 
   
Возвращаясь к решению задачи восстановления зависимостей, следует отметить, что 
непростое строение множеств решений интервальных систем уравнений делает очень 
трудоёмким и малополезным их точное и полное описание. Скажем, для интервальных линейных 
систем выписывание всех гиперплоскостей, ограничивающих множество решений в отдельных 
ортантах пространства, может потребовать экспоненциально больших трудозатрат. Имеет смысл 
найти какое-нибудь приближённое  описание информационного множества, которое удовлетворит 
заказчика. Здесь могут встретиться различные ситуации. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb]
\centering\small 
\unitlength=1mm  
\begin{picture}(80,60)
    \put(0,0){\includegraphics[width=84mm]{pictures/SolSetEstim.eps}} 
    \put(46,52){\mbox{внешняя оценка}}  
    \put(21,44){\mbox{оптимальная внешняя оценка}}      
    \put(32,32){\mbox{внутренняя оценка}}      
\end{picture}
\caption{Различные способы оценивания} 
информационного множества.
\label{EstimModesPic}  
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Часто бывает необходимо оценить разброс точек из информационного множества, то есть 
определить, насколько сильно оно <<растекается>> в пространстве параметров. Нередко 
это делается для его отдельных компонент, так что в целом нам требуется интервальный 
брус, содержащий множество решений. Это \emph{внешняя оценка} информационного множества, 
или, точнее, \emph{внешняя интервальная оценка} (см. Рис.~\ref{EstimModesPic}). Среди 
всех внешних оценок наилучшей служит минимальная по размерам внешняя оценка, которую 
также называют \emph{оптимальной внешней оценкой} (см. Рис.~\ref{EstimModesPic}). 
Она единственна и является интервальной оболочкой информационного множества задачи. 
\index{внешняя  оценка}\index{оптимальная внешняя оценка} 
  
Внешняя оценка информационного множества необходима, к примеру, при построении внешней 
оценки трубки совместных зависимостей (см. \S\ref{CmptFunCorSect}), когда мы хотим 
просчитать гарантированный эффект от реализации всех сценариев, могущих встретиться 
по восстановленным зависимостям. 
  
Во многих задачах требуется оценивание информационного множества с помощью какого-то 
несложно описываемого подмножества --- \emph{внутреннее оценивание}. Такая оценка будет 
содержать только точки из информационного множества\index{внутренняя оценка} и ничего 
лишнего. Внешняя оценка информационного множества в этом смысле плоха тем, что включает 
в себя точки, не принадлежащие информационному множеству.\footnote{Английские термины 
для обозначения внешней и внутренней оценки --- outer estimate и inner estimate 
соответственно. Для внешней оценки часто используют также термин <<enclosure>>.} 
  
Если в качестве подмножества информационного множества берётся вписанный брус, 
то он называется \emph{внутренней интервальной оценкой} множества решений (см. 
Рис.~\ref{EstimModesPic}). Среди двух внутренних оценок лучшей является та, которая 
целиком содержит другую, но максимальных по включению внутренних оценок, которые 
несравнимы друг с другом, может быть много. 
  
Кроме внешнего и внутреннего оценивания информационных множеств могут встретиться 
и другие, которые требуются по смыслу задачи. Весьма популярно, например \emph{слабое 
внешнее оценивание},\index{слабая внешняя оценка} когда для множества ищется брус, 
включённый в его интервальную оболочку (см. \S\ref{VariabIdeasSect}). Встречается 
также оценивание вдоль какого-то специального выделенного направления, исчерпывающее 
оценивание с помощью набора брусов (см. \S\ref{ExhaInfoSetSect}) и т.\,п. 
  
Помимо оценивания информационного множества <<целиком>>, во многих ситуациях 
достаточно найти какую-либо точку из него (здесь мы имеем аналогию с оцениванием 
<<точечным>> и <<интервальным>> в традиционной статистике).  Естественно выбирать 
такую одну точку удовлетворяющей некоторым условиям оптимальности. 
\index{точечная оценка} 
 
Частый выбор --- взять центр интервального бруса, который является минимальной 
по включению внешней оценкой информационного множества, т.\,е. его интервальной 
оболочкой. 
   
Другие возможные варианты --- это 
    так называемый центр Оскорбина \cite{OskorbinMaksiZhilin}, 
    чебышёвский центр информационного множества 
    \cite{VoschininSotirov,ZukhovitskiyAvdeeva},
    центр тяжести информационного множества \cite{VoschininSotirov},  
    точка максимума совместности (аргумент максимума распознающего функционала, 
    который является точкой максимума совместности соответствующей интервальной 
    системы уравнений \eqref{InLinEqSys}, см. \cite{SShary2012, SharysJCT2013, 
    SSharyIzvAN2017, SSharyPLab2020, SSharyADSAA}). 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section[Численные методы для интервальных систем уравнений]%
        {Численные методы \\* для интервальных систем уравнений} 
  
  
Решение различных постановок задач для интервальных систем уравнений является развитым 
разделом современного интервального анализа. Обзоры результатов на эту тему читатель 
может увидеть в книгах \cite{AlefeldHerzberg, SSharyBook, MayerBook, MooreBakerCloud, 
NeumaierBook, RohnHandbook}. Некоторые важные частные вопросы, касающиеся интервальных 
линейных и нелинейных систем уравнений, подробно освещены в книгах \cite{AschDavBook, 
CzechBook, HansenWalster}. Тем не менее, ряд задач, возникших в анализе интервальных 
данных, на настоящий момент проработаны относительно слабо. Это относится, прежде 
всего, к решению общих интервальных систем уравнений, у которых число уравнений 
может не совпадать с числом неизвестных. Кроме того, подавляющее большинство численных 
методов для интервальных систем уравнений, линейных и общих нелинейных, разработаны 
для задачи внешнего интервального оценивания объединённого множества решений, тогда 
как другие способы оценивания и другие множества решений получили гораздо меньшее 
внимание. 
  
Тем не менее, работы по оцениванию множеств решений общих интервальных линейных систем 
имеются. В книге \cite{SSharyBook}, Глава~11, излагаются методы внутреннего оценивания 
объединённого множества решений интервальных линейных систем уравнений, не обязательно 
квадратных. Там же в Главе~6 рассматривается внутреннее оценивание допускового 
множества решений. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb]
\centering\small 
\unitlength=1mm  
\begin{picture}(60,57)
   \put(8,0){\includegraphics[width=60mm]{pictures/SubSquareSys.eps}} 
   \put(-7,44){$\mbf{A}'x = \mbf{b}'$}     
   \put(-5,26.5){$\mbf{A}''x = \mbf{b}''$} 
   \put(-4,11.5){$\mbf{A}'''x = \mbf{b}'''$}     
\end{picture}
\caption{Иллюстрация разбиения интервальной линейной}
системы уравнений на квадратные подсистемы, которые \\
выделены штриховками с разными наклонами. 
\label{SubSquarePic}  
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Рассмотрим простой способ внешнего оценивания множеств решений переопределённых 
интервальных систем уравнений с помощью решения квадратных подсистем. Пусть дана 
интервальная $m\times n$-система линейных алгебраических уравнений $\mbf{A}x = \mbf{b}$, 
в которой $m > n$. Разобъём интервальную матрицу $\mbf{A}$ на квадратные подматрицы 
размера $n\times n$, возможно, пересекающиеся друг с другом, которые в объединении дают 
$\mbf{A}$ (см. Рис.~\ref{SubSquarePic}). Тогда исходная система уравнений разобъётся 
на подсистемы 
\begin{equation*} 
\mbf{A}'x = \mbf{b}', \hspace{12mm} 
   \mbf{A}''x = \mbf{b}'', \hspace{12mm} \mbf{A}'''x = \mbf{b}''', 
\end{equation*} 
которые можно рассматривать и решать отдельно друг от друга. 
  
Пусть $\varXi'$, $\varXi''$, $\varXi'''$ --- множества решений соответствующих интервальных 
систем уравнений, тогда множество решений $\varXi$ всей исходной системы, очевидно, 
является их пересечением:
\begin{equation*} 
\varXi\; = \;\varXi' \cap\varXi'' \cap\varXi'''. 
\end{equation*} 
Поэтому если $\mbf{U}'$, $\mbf{U}''$, $\mbf{U}'''$ --- внутренние интервальные оценки 
для множеств решений $\varXi'$, $\varXi''$, $\varXi'''$, т.\,е. $\mbf{U}'\subseteq\varXi'$, 
$\mbf{U}''\subseteq\varXi''$, $\mbf{U}'''\subseteq\varXi'''$, то 
\begin{equation*} 
\mbf{U}'\cap\mbf{U}''\cap\mbf{U}''' \  \subseteq \  
   \varXi' \cap\varXi'' \cap\varXi''' \; = \;\varXi. 
\end{equation*} 
Если $\mbf{V}'$, $\mbf{V}''$, $\mbf{V}'''$ --- внешние интервальные оценки для множеств 
решений $\varXi'$, $\varXi''$, $\varXi'''$, т.\,е. $\mbf{V}'\supseteq\varXi'$, 
$\mbf{V}''\supseteq\varXi''$, $\mbf{V}'''\supseteq\varXi'''$, то 
\begin{equation*} 
\mbf{V}'\cap\mbf{V}''\cap\mbf{V}''' \  \supseteq \  
   \varXi' \cap\varXi'' \cap\varXi''' \; = \;\varXi. 
\end{equation*} 
  
Как следствие, приходим к следующим практическим рецептам решения задач внутреннего и 
внешнего оценивания множеств решений переопределённых интервальных систем уравнений. 
  
Решим задачи внутреннего оценивания для полученных подсистем с помощью численных методов, 
предназначенных для квадратных интервальных линейных систем уравнений. Затем пересечём 
полученные интервальные оценки, и полученный брус будет внутренней оценкой множества 
решений исходной системы. 
  
Решим задачи внешнего оценивания для полученных подсистем с помощью численных методов, 
предназначенных для квадратных интервальных линейных систем уравнений. Затем пересечём 
полученные интервальные оценки, и полученный брус будет внешней оценкой множества 
решений исходной системы. 
      
Рассмотренный выше метод будем называть \textit{методом квадратных подсистем}. 
\index{метод квадратных подсистем} 
  
Напомним, что задача распознавания и оценивания объединённого множества решений 
для интервальный систем линейных алгебраических уравнений является труднорешаемой 
(NP-трудной). По этой причине все алгоритмы для оценивания объединённого множества 
решений можно разделить на два класса: 
\begin{itemize} 
\item \ 
алгоритмы быстрые,  
\item \ 
алгоритмы точные.  
\end{itemize} 
Первые ориентированы на получение оценки за приемлемое время, но могут давать весьма 
грубые ответы. Вторые предназначены для того, чтобы давать точные или гарантированно 
близкие к точным оценки множеств решений, но могут потребовать для своей работы 
очень больших ресурсов (времени, памяти и т.\,п.). 
  
Допусковое множество решений ИСЛАУ обладает более благоприятными свойствами. Оно 
является выпуклым полиэдральным множеством, так что распознавание его пустоты или 
непустоты может быть выполнено полиномиально сложными алгоритмами. Получение 
покоординатных оценок допускового множества решений сводится к решению задачи 
линейного программирования и также может быть выполнено за полиномиальное время. 
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section[Парадоксы интервального анализа данных]% 
        {Парадоксы интервального \\* анализа данных} 
\label{IStatParadoxesSect}
  
Парадоксами в науке называют суждения, умозаключения или факты, которое являются, 
на первый взгляд, противоречивыми или же противоречащими здравому смыслу, но которые 
могут быть объяснены при более глубоком рассмотрении предмета. Анализ парадоксов и 
их преодоление чрезвычайно полезны для лучшего понимания сути рассматриваемой науки 
и её методов. Напомним, что свои парадоксы имеет вероятностная статистика, в частности, 
регрессионный анализ. Читатель может увидеть их описание в книгах Г.\,Секея \cite{Sekei}, 
Ф.\,Мостеллера и Дж.\,Тьюки \cite{MostellerTukey}, в \S 21.6 книги М.Б.\,Лагутина 
\cite{Lagutin}, в статье В.\,Амрхайна с соавторами \cite{Amrhein2019} и других 
ей аналогичных. 
  
Восстановление зависимостей по данным, имеющим интервальную неопределённость, также 
обладает рядом парадоксальных свойств, которые были отмечены Е.З.\,Демиденко в заметке 
\cite{DemidenkoNote} и А.И.\,Хлебниковым в статьях  \cite{Khlebnikov1996,  Khlebnikov1999}. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Парадокс Е.З.\,Демиденко} 
\label{DemidParadoxSect} 
  
Суть парадокса Е.З.\,Демиденко может быть кратко выражена фразами <<чем грубее данные 
--- тем лучше>> или <<чем точнее измерения --- тем хуже>>.  \index{парадокс Демиденко} 
  
В самом деле, присутствие любой неопределённости в данных является нежелательным 
феноменом, который искажает истинную картину реальности. Это относится, в частности,
к интервальной неопределённости. Уменьшение этой неопределённости, т.\,е. сужение 
интервалов в данных, является благом, которое на практике должно приветствоваться. 
С другой стороны, при более широких интервалах исходных данных информационное 
множество множество задачи --- множество решений интервальной системы уравнений, 
неравенств и т.\,п., построенное по данным измерений, как правило, также является 
более широким, и потому появляется больше возможностей для выбора из него параметров 
модели, чем в случае узких интервальных данных, когда информационное множество вообще 
может стать пустым. Итак, чем выше точность исходных данных, чем меньше их интервальная 
неопределённость, тем хуже для оценивания параметров. И наоборот, чем шире интервальные 
неопределённости, чем меньше мы знаем о точных значениях измеряемых величин, тем лучше 
для процесса оценивания параметров и тем более богатый набор результатов можно получить. 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb] 
\centering\small  
\includegraphics[width=55mm]{pictures/GreenLine.eps}
\hspace{2mm}
\includegraphics[width=55mm]{pictures/RedLine.eps}
\caption{\,Широкие брусы неопределённости измерений позволяют} 
строить много моделей,  совместных с интервальными данными. \\ 
Для узких брусов неопределённости измерений модель, которая \\
совместна с этими данными, может не существовать.  
\label{DemiParadoxPic}
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
Эта ситуация наглядно изображена на Рис.~\ref{DemiParadoxPic}, где интервалы 
неопределённостей в данных правого чертежа получаются сужением этих интервалов правого 
чертежа. При этом теряется возможность проведения прямой, проходящей через все брусы 
неопределённости, т.\,е. совместной со всеми данными. 
  
Отметим, что для сильной совместности параметров и данных парадокс Е.З.\,Демиденко 
верен лишь частично --- только для неопределённости в выходных (зависимых или 
критериальных) переменных. Дело в том, что допусковое множество решений, которое 
является информационным множеством задачи для этого случая, уменьшается при расширении 
интервалов в матрице ИСЛАУ, так что увеличение  неопределённости по входным переменным 
не приводит к облегчению выбора параметров модели. Скорее, он затрудняется при 
расширении интервалов независимых переменных. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\subsection{Парадокс А.И.\,Хлебникова}  
  
А.И.\,Хлебниковым в работах \cite{Khlebnikov1996,Khlebnikov1999} отмечено несколько 
противоречивых свойств <<наивной>> обработки интервальных данных\footnote{Критика 
А.И.\,Хлебникова направлена против конкретной версии метода центра неопределённости, 
которая изложена в книге Белов В.М., Суханов В.А., Унгер Ф.Г. <<Теоретические и 
прикладные аспекты метода центра неопределённости>>, Новосибирск, Наука, 1995. 
Но, по существу, она носит общий характер.} и, фактически, сформулирован ещё один 
интересный парадокс: <<чем больше интервальных измерений, тех хуже для выбора 
параметров модели>>.                           \index{парадокс Хлебникова}
  
Парадокс А.И.\,Хлебникова относится к тем подходам к обработке интервальных данных, 
в которых оценка параметров берётся из информационного множества задачи. Оно, 
как мы знаем, является множеством решений системы уравнений, неравенств и пр., 
каждое из которых соответствует отдельному измерению. Тогда добавление в систему 
дополнительных уравнений или неравенств приводит к тому, что общее множество решений 
системы может только уменьшаться или даже стать пустым, так как оно образуется в 
результате пересечения всех множеств решений отдельных уравнений и неравенств. 
С другой стороны, здравый смысл подсказывает, что увеличение количества измерений 
не должно влиять на трудность получения оценки, её доступность и т.\,п. Каждое 
измерение приносит дополнительную информацию об объекте, и разумные процедуры 
обработки данных должны быть, по крайней мере, не слишком чувствительны 
к их количеству. 
  
В более мягкой форме парадокс А.И.\,Хлебникова, фактически, был сформулирован 
в книге \cite{MudrovKushko} (с.~130), где разбирается предложенная Л.В.\,Канторовичем 
в~\cite{Kantorovich} модель интервальной неопределённости данных и сопутствующие ей 
вычислительные методы. 
  
Для преодоления отмеченных парадоксов требуется более тонкий анализ задачи 
восстановления зависимостей и сути интервальной неопределённости в данных. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\subsection{Анализ парадоксов и их объяснение} 
  
Отметим, прежде всего, что при обработке интервальных данных возможны две принципиально 
различные ситуации. 
  
Первая из них реализуется в случае, когда интервалы в данных адекватно представляют 
границы погрешностей измерений (т.\,е. выборка накрывающая). Тогда уменьшение ширины 
интервалов данных, уменьшение неопределённости является позитивным фактом. При этом 
невозможность выбора каких-либо параметров модели, согласующихся с этими интервальными 
данными (когда множество решений интервальной системы пусто), является признаком 
неадекватности самой модели, которая выбрана для описания объекта. Как следствие, 
модель должна быть сменена, а процесс оценивания параметров повторён заново с другой 
моделью. Возможно, в этой ситуации необходимо даже перейти к другому способу описания 
изучаемого явления. 
    
Во второй ситуации предполагается, что интервалы неопределённости данных могут 
не вполне достоверно отражать множества возможных значений соответствующих величин
(т.\,е. выборка ненакрывающая), так что, в принципе, для выбранной модели объекта 
можно и не получать полного согласования с ней экспериментальных данных. Допустима 
некоторая несогласованность (как и в традиционном случае точечных данных с зашумлением), 
и решается задача --- тем или иным способом минимизировать это несогласование. 
  
Другая причина, в которой вынуждены идти по этому пути, связана с необходимостью 
сохранения избранной модели, вида функциональной зависимости между рассматриваемыми 
величинами, относительно которой дано или a priori известно, что <<так должно быть>>. 
  
На этом втором пути необходимо выбрать количественную <<меру несогласования>> 
данных с параметрами объекта, и тогда искомой оценкой можно будет, к примеру,
взять ту точку пространства параметров, в которой эта несогласованность 
минимальна. 
  
Важный методический вывод, который вытекает из анализа парадоксов Е.З.\,Деми\-денко и 
А.И.\,Хлебникова, заключается в том, что они являются прямым следствием определённых 
специфических свойств информационного множества задачи восстановления зависимостей, 
т.\,е. множеств решений интервальных системы уравнений или неравенств. Поэтому наши 
оценки параметров восстанавливаемой зависимости нельзя основывать только лишь 
на существовании непустого информационного множества и брать их только из этого 
множества, если хотим избежать их парадоксальности. Разумные и непарадоксальные 
процедуры оценивания в условиях интервальной неопределённости должны включать 
в себя, в качестве органичной составной части, обработку случая пустого 
информационного множества. 
 
Последнее заключение, как нетрудно понять, находится в согласии с <<принципом 
соответствия>> отмеченным в \S\ref{CorresPrincpSect}: методы обработки интервальных 
данных должны превращаться в разумные методы для точечных данных, если интервальная 
неопределённость исчезает и, как следствие, информационное множество становится пустым. 
Для задач с точечными данными информационное множество, как правило, всегда пусто. 
Его непустота является исключительной ситуацией, которая обычно разрушается при любом 
сколь угодно малом возмущении в данных. 
  
В целом, мы приходим к ситуации, когда оценки параметров, полученные по данным 
с интервальной неопределённостью, получают дополнительную качественную характеристику: 
\begin{itemize} 
\item 
либо оценка соответствует непустому информационному множеству задачи 
и, как следствие, берётся из него,  
\item 
либо информационное множество задачи пусто и оценка параметров берётся на основе 
каких-то других соображений. 
\end{itemize} 
Аналогов этому в традиционной статистике нет. Но новая характеристика оценки, 
связанная с пустотой/непустотой информационного множества относится не только 
(и не столько) к оценке параметров, как таковой, а описывает свойства решённой 
задачи --- её интервальные данные, вид модели, их взаимоотношение и т.\,п. 
Разумное использование этой информации даёт ценнейшие сведения о моделируемом 
объекте и обрабатываемых данных. 
  
Из предложенных на сегодняшний день подходов к оценивания параметров зависимостей 
по интервальным данным модифицированный метод центра неопределённости \cite{ZhilinDiss} 
и метод максимума совместности \cite{SShary2012, SSharyJCT2017, SSharyIzvAN2017, 
SSharyPLab2020, SSharyADSAA} не опираются на непустоту информационного множества 
задачи (хотя и делают это по-разному). Напротив, в методе <<простого интервального 
оценивания>> \cite{Rodionova}, в методе М.\,Гутовски \cite{Gutowski} и в методе 
парциальных информационных множеств \cite{Kumkov2010, Kumkov2013} оценка параметров 
зависимости всегда берётся из непустого информационного множества. Если же информационное 
множество пусто, то все эти методы рекомендуют коррекцию данных или модели, либо того 
и другого вместе, а далее --- повторное определение параметров зависимости. 
  
Те же самые парадоксы, как нетрудно сообразить, относятся также к оцениванию 
постоянной величины и налагают те же самые требования на методы их измерения и 
обработки измерений. 
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\section[Обзор методов восстановления зависимостей\\* по интервальным данным]% 
        {Обзор методов восстановления \\* зависимостей по интервальным\\* данным} 
\label{SurveyMethodSect} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\subsection{Простейшие методы} 
     
Методика восстановления линейной зависимости по интервальным данным, кратко намеченная 
Л.В.\,Канторовичем в его работе \cite{Kantorovich} и подробно изложенная выше 
в \S\ref{ExactInputSect}, далее развивалась и модифицировалась многими авторами. 
Здесь стоит упомянуть работы С.М.\,Спивака и его учеников (см., в частности, 
\cite{KantorSpivak,SpivakEtAl}), работы О.Е.\,Родионовой \cite{PomeRodionova, Rodionova}, 
работы А.П.\,Вощинина и его коллег \cite{VoschininSotirov, VoschiBochkovSotirov}. 
  
О.Е.\,Родионова, которая использовала аналогичный подход, использовала для его 
обозначения аббревиатуру <<ПИО>> --- простое интервальное оценивание. 
     
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\subsection{Метод центра неопределённости} 
  
Метод центра неопределённости был предложен Н.М.\,Оскорбиным \cite{Oskorbin1983} 
и затем развит Н.М.\,Оскорбиным и С.И.\,Жилиным в работах \cite{ZhilinDiss, 
MaksimovOskorbin, OskorbinMaksiZhilin}.    \index{метод центра неопределённости} 
  Методу центра неопределённости посвящена также книга \cite{SukhanovSI} 
  и часть статей сборника \cite{SukhanovSlava}. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\subsection[Метод парциальных информационных множеств]%   
           {Метод парциальных \\*  информационных множеств} 
  
Основная идея метода парциальных информационных множеств (кратко --- метода ПИМ), 
развиваемого С.И.\,Кумковым (см. \cite{Kumkov2010,Kumkov2013} и другие работы), 
заключается в вычленении из общей выборки некоторых подвыборок небольшой длины и 
их отдельном исследовании.\footnote{Термин <<парциальный>> происходит от латинского 
слова partialis и означает <<частичный, составляющий часть чего-нибудь>>.} Малый объём 
подвыборок делает возможным быстрое исследование их совместности и, как следствие, 
на основе этой информации мы можем ценой приемлемых трудозатрат делать заключения 
о совместности всей выборки и наличии в ней совместных подвыборок.  
   
Эта основная идея может реализовываться разными способами в зависимости от решаемой 
задачи и условий обработки данных. Можно по-разному выбирать подвыборки, изменяя их 
длину и расположение в общей выборке. Можно по-разному обрабатывать их пересечение 
и т.\,д. Всё это приводит к различным возможным реализациях основной идеи и различным 
вариантам метода парциальных информационных множеств. Например, при обработке данных, 
поступающих в режиме реального времени, вся выборка формируется динамически, так что 
мы не имеем её целиком и должны принимать решение по имеющимся данным. В этом случае 
исследуемая подвыборка может иметь вид скользящего окна, которое на каждом очередном 
шаге сдвигается по ходу времени. Если обрабатываемые данные представлены одной целостной 
выборкой, то мы можем выбирать из неё подвыборки с помощью каких-либо комбинаторных 
соображений. Например, из общей выборки длины $n$ можно извлечь все подвыборки длины $k$, 
их будет $\mrm{C}_n^k$ штук.    \index{метод парциальных информационных множеств} 
  
Метод парциальных информационных множеств можно применять для восстановления любых 
линейных по параметрам зависимостей. Следуя \cite{Kumkov2010}, рассмотрим работу 
метода ПИМ на простом примере восстановления линейной зависимости вида  
\begin{equation}
\label{SimpleLinear} 
\begin{array}{l}
y(x) = \beta_{0} + \beta_{1}x,
\end{array}
\end{equation}
где  $x$ --- аргумент функции, $\beta_0$, $\beta_1$ --- параметры, подлежащие определению. 
Предположим, что для линейной зависимости \eqref{SimpleLinear} получена выборка измерений 
$\{(x_k, \mbf{y}_{k})\}$, $k = 1, \ldots, n$, c интервалами неопределённости $\mbf{y}_k$ 
для зависимой переменной $y$, тогда как значения аргументов измерений $x_k$ известны 
точно (Рис.~\ref{PISmethodPic}а). 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb]
\centering\small  
\unitlength=1mm 
\begin{picture}(70,120) 
    \put(-2,117){a)}
    \put(0,75){\includegraphics[width=70mm]{pictures/PartInfoSet-11.eps}} 
    \put(61,119){$y(x) = \un{\beta}_{0} + \ov{\beta}_{1}x$} 
    \put(60,95){$y(x) = \ov{\beta}_{0} + \un{\beta}_{1}x$} 
    \put(21,77.7){$x_k$} \put(51,77.7){$x_l$} 
    \put(18,99){$\mbf{y}_k$} \put(48.5,107){$\mbf{y}_l$} 
    \put(4.5,77.5){$\un{\beta}_0$} \put(4.5,111){$\ov{\beta}_0$}     
    \put(-2,63){б)}
    \put(0,0){\includegraphics[width=70mm]{pictures/PartInfoSet-22.eps}} 
    \put(64,26){$\beta_{0}$} \put(9,64){$\beta_{1}$} 
    \put(58.5,30.5){$\ov{\beta}_0$} \put(14,54){$\ov{\beta}_1$} 
    \put(7,26){$\un{\beta}_0$} \put(9,7){$\un{\beta}_1$} 
    \put(43,45){$\varOmega_{\text{апр}}$} 
    \put(34,32){$\varOmega^*_{kl}$}  
    \put(18,44){$\varOmega_{kl}$}      
\end{picture} 
\caption{Построение парциального информационного множества для разноширинных 
интервальных измерений $(x_{k}, \mbf{y}_{k})$, $(x_{l}, \mbf{y}_{l})$.} 
\label{PISmethodPic} 
\end{figure}       
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Возьмём пару интервальных результатов измерений, соответствующих $k$-му и $l$-му  
измерениям, т.\,е. 
\begin{equation*} 
(x_{k}, \mbf{y}_{k}) \qquad \text{ и }\qquad (x_{l}, \mbf{y}_{l}). 
\end{equation*} 
\emph{Парциальным информационным множеством} $\varOmega_{kl}(\beta_{0}, \beta_{1})$ 
будем называть множество параметров функциональной зависимости, которые совместны 
с частью всей выборки --- данными $k$-го и $l$-го наблюдений, т.\,е. с отрезками 
неопределённости $(x_{k},\mbf{y}_{k})$ и $(x_{l}, \mbf{y}_{l})$. Оно легко строится 
по их граничным точкам и зависимостям с экстремальными и промежуточными значениями 
параметров. В общем случае, для пары разноширинных интервальных измерений это множество 
является неправильным выпуклым четырехугольником. На плоскости $0\beta_{0}\beta_{1}$ 
на  Рис.~\ref{PISmethodPic} оно отмечено слабой серой заливкой. Аналогично определяются 
\index{парциальное информационное множество} парциальные информационные множества, 
соответствующие другим подвыборкам исходной выборки. 
  
Далее, если, например, из предыдущих экспериментов, известен некоторый априорный 
выпуклый многоугольник $\varOmega_{\text{апр}}(\beta_{0},\beta_{1})$ для значений 
параметров (на Рис.~\ref{PISmethodPic}б  отмечен прямоугольником), то можно улучшить 
$\varOmega_{kl}(\beta_{0}, \beta_{1})$ пересечением: 
\begin{equation*} 
\varOmega^{\ast}_{kl}(\beta_{0}, \beta_{1})=\varOmega_{kl}(\beta_{0}, \beta_{1}) 
   \cap \varOmega_{\text{апр}}(\beta_{0}, \beta_{1}). 
\end{equation*} 
Результат выделен темной заливкой на Рис.~\ref{PISmethodPic}б. Искомое информационное 
множество $\varOmega(\beta_{0}, \beta_{1})$ находится пересечением набора улучшенных 
парциальных информационных множеств $\varOmega^{\ast}_{kl}(\beta_{0}, \beta_{1})$: 
\begin{equation*} 
\varOmega(\beta_{0}, \beta_{1}) \  = 
   \bigcap_{\begin{subarray}{c} k = 1,\ldots, n-1\\* l= k+1,\ldots,n\end{subarray}} 
   \varOmega^{\ast}_{kl}(\beta_{0}, \beta_{1}). 
\end{equation*} 
Операции пересечения выпуклых многоугольников выполняются с помощью стандартных 
алгоритмов и реализующих их программ. 
  
Точечная оценка параметров берётся в методе ПИМ каким-либо образом из полученного 
информационного множества. Например, её можно взять как центр интервальной оболочки 
информационного множества, т.\,е. как точку, на которой минимизируется максимальное 
отклонение по координатам от всех точек информационного множества. 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   
\subsection{Метод максимума совместности} 
\label{MCMSect} 
  
  
\textit{Метод максимума совместности} (или метод максимума согласования) был предложен 
и развит С.П.\,Шарым в цикле работ \cite{SShary2012, SharysJCT2013, SSharyIzvAN2017, 
SSharyPLab2020, SSharyJGO2016, SSharyADSAA} и других. В этом методе оценка параметров 
восстанавливаемой зависимости берётся как точка, в которой достигается <<максимальная 
совместность>> интервальной системы уравнений, построенной по данным задачи и виду 
восстанавливаемой зависимости. При этом <<совместность>> интервальной системы может 
пониматься в разных смыслах, так что метод максимума совместности имеет слабую и сильную 
версии, которые технологически сходны друг с другом, но обладают существенно разными 
математическими свойствами. \index{метод максимума  совместности} В слабой версии 
метода максимума совместности\index{метод максимума согласования} информационным 
множеством задачи служит объединённое множество решений интервальной системы уравнений, 
а в сильной --- допусковое множество решений. 
  
Достоинства метода максимума совместности --- возможность решения любых типов интервальных 
задач восстановления зависимостей с помощью единообразной вычислительной схемы, а также 
возможность получения дальнейшей информации о решаемой задаче, которая позволяет выполнить 
её коррекцию и оценить влияние различных факторов на ответ. Если информационное множество 
задачи непусто, то метод максимума совместности выдаёт точку из него, вокруг которой далее   
можно строить его интервальные оценки. 
  
Сильная версия метода максимума совместности обладает дополнительными достоинствами. 
Это полиномиальная сложность нахождения точечной оценки параметров при восстановлении 
линейной зависимости. Это конечная вариабельность оценки (см. \S\ref{VariabilitySect}), 
которая позволяет успешно отсекать <<нефизичные>> ответы и получать однозначную и разумную 
оценку параметров даже в сложных случаях широких и сильно перекрывающихся интервалов данных, 
и т.\,п. Кроме того, сильная версия метода максимума совместности позволяет частично 
преодолеть называемый <<парадокс Демиденко>> (см. \S\ref{DemidParadoxSect}).  Последнее 
обстоятельство позволяет считать оценки параметров, полученные с помощью этого подхода, 
по-видимому, <<наиболее соответствующими здравому смыслу>>.  
  
В работе \cite{KreinovichShary} показано, что оценки параметров, получаемые с помощью 
слабой версии метода максимума совместности, совпадают с оценками максимума правдоподобия 
для равномерных вероятностных распределений на интервалах входных данных. Недавно 
В.\,Крейновичем и С.П.\,Шарым теоретико-вероятностная интерпретация была также дана 
для допускового множества решений и, как следствие, для сильной версии метода максимума 
совместности \cite{VladikShary-2021}. 
  
Как слабая, так и сильная версии метода максимума совместности при стремлении ширины 
интервалов данных к нулю переходят в известный метод чебышёвского (равномерного, 
минимаксного) сглаживания данных, когда в качестве оценки параметров берётся точка, 
на которой достигается минимум максимального отклонения от данных. Таким образом, 
метод максимума совместности удовлетворяет принципу соответствия в обработке интервальных 
данных (см. \S\ref{CorresPrincpSect}), а реализующее его программное обеспечение хорошо 
работает также для традиционных неинтервальных данных. Изложим кратко, следуя работам 
\cite{SharysReserve,SShary2022}, сам метод и мотивирующие его идеи. 
  
Для обычных неинтервальных уравнений и систем уравнений об их совместности на том 
или ином приближённом решении можно судить по величине невязки --- разности между 
левой и правой частями этого уравнения или системы уравнений. К примеру, для системы 
линейных уравнений $Ax = b$ невязка приближённого решения $\tilde{x}$, равна, 
по определению, разности $A\tilde{x} - b$, и она даёт представление о совместности 
системы на векторе $\tilde{x}$. Беря какую-нибудь норму невязки, т.\,е. рассматривая 
$\|A\tilde{x} - b\|$, получаем количественную меру совместности приближённого 
решения $\tilde{x}$. Её зануление свидетельствует о том, что найдено точное решение 
системы уравнений. 
  
Таким образом, способ выбора оценки параметров восстанавливаемой функциональной 
зависимости как точки, в которой достигается максимальная совместность системы уравнений, 
построенной по виду функции и обрабатываемым данным, используется давно и плодотворно. 
Например, в популярном методе наименьших квадратов или в методе чебышёвского сглаживания 
оценки параметров выбираются из условия минимизации нормы невязки этой системы уравнений, 
среднеквадратичной или чебышёвской (равномерной) соответственно. Это соответствует 
нахождению <<наибольшей совместности>> системы в выбранном нами смысле. 
    
Для интервальных уравнений и систем уравнений этот способ не подходит, так как их 
совместность, понимаемая как непустота соответствующего множества решений (объединённого, 
допускового или любого другого), не требует полного совпадения левой и правой частей 
уравнения. Интервалы левой и правой частей уравнения могут находиться при этом в более 
сложном соотношении. Например, для совместности интервальной линейной системы уравнений 
$\mbf{A}x = \mbf{b}$ в слабом смысле, т.\,е. для того, чтобы объединённое множество 
решений $\USS\Ab$ было непустым содержало точку $\tilde{x}$, необходимо и достаточно, 
чтобы левая и правая части системы уравнений непусто пересекались при подстановке 
в неё $\tilde{x}$, т.\,е. $\mbf{A}\tilde{x}\cap\mbf{b}\neq\varnothing$ (критерий Бекка, 
рассмотренный в \S\ref{InteSystemSect}; см. \cite{SSharyBook}). Для совместности 
интервальной линейной системы уравнений $\mbf{A}x = \mbf{b}$ в сильном смысле, т.\,е. 
для того, чтобы допусковое множество решений $\TSS\Ab$ было непустым и содержало точку 
$\tilde{x}$, необходимо и достаточно, чтобы левая часть системы уравнений включалась 
в правую при подстановке в неё $\tilde{x}$, т.\,е. $\mbf{A}\tilde{x}\subseteq\mbf{b}$ 
(см. \eqref{TSScharact} и также книгу \cite{SSharyBook}). 
  
Но почти все практически важные частные случаи совместности интервальных линейных 
систем уравнений могут быть единообразно рассмотрены с помощью конструкции <<резерва 
характеристического включения>>, предложенной в работе \cite{SharysReserve}. Она позволяет 
также ввести естественный способ количественного измерения совместности для интервальных 
уравнений и систем уравнений. 
  
Напомним, что характеристическими включениями называются включения вида \eqref{USScharact} 
и \eqref{TSScharact},\index{характеристическое включение} дающие условия принадлежности 
точек соответствующим множествам решений интервальных уравнений в виде включений 
выражения от левой части в выражение от правой части. Для каждой конкретной точки можно 
дать количественную меру того, <<насколько сильно>> выполняются эти включения, тем самым 
получив количественную характеристику совместности ИСЛАУ для данной точке. Дальнейшее 
последовательное развитие этого подхода приводит к так называемым \emph{распознающим 
функционалам} --- специальным функциям, с помощью которых можно исследовать положение 
точки относительно множества решений, совместность интервальной системы и тому подобные 
вопросы. 
    
Пусть даны два интервала $\mbf{p}, \mbf{q}\subset\mbb{R}$, причём $\mbf{p}\subseteq\mbf{q}$. 
Как можно количественно охарактеризовать <<запас включения>> интервала в интервал т.\,е. то, 
<<насколько сильно>>  $\mbf{p}$ включён в $\mbf{q}$? Один из возможных естественных способов 
сделать это предложен в работе \cite{SharysReserve}. Начнём равномерно <<раздувать>> меньший  
интервал относительно его середины на величину $t$, т.\,е. организуем семейство интервалов 
$\mbf{p} + [-t, t]$ для различных $t\in\mbf{R}$, и отследим момент, когда включение 
получающегося интервала в $\mbf{q}$ нарушится. Чем больше нужно будет взять значение $t$ 
для нарушения включения $\mbf{p} + [-t, t]\subseteq\mbf{q}$, тем б\'{о}льшим является 
резерв (запас) включения интервала $\mbf{p}$ в интервал $\mbf{q}$. В многомерном случае, 
если $\mbf{p}$ и $\mbf{q}$ --- интервальные векторы, эту конструкцию можно применить 
покомпонентно. Основываясь на сформулированных идеях, введём 
  
\begin{definition}   
Для интервальных $n$-векторов $\mbf{p}$ и $\mbf{q}$, \textsl{резервом интервального включения} 
$\mbf{p}\subseteq\mbf{q}$ (или же просто \textsl{резервом}) называется наибольшее вещественное 
число $\Rsv$, такое что            \index{резерв включения} 
\begin{equation*} 
\mbf{p} + [-\Rsv, \Rsv]\cdot(1,1,\ldots,1)^{\top}\subseteq\mbf{q}. 
\end{equation*} 
\end{definition} 
  
Это определение имеет смысл также при отрицательном Rsv, если все арифметические операции 
и отношения рассматриваются  в  полной  интервальной арифметике Каухера $\mbb{KR}$  (см. 
\S\ref{KaucherArithmSect}). Если $\Rsv < 0$, то $[-\Rsv, \Rsv]$ --- это неправильный 
интервал из $\mbb{KR}$, и абсолютное значение резерва показывает, насколько сильно 
в отношении  $\mbf{p}\subseteq\mbf{q}$ левая часть далека от включения в правую. В целом 
смысл понятия резерва вполне очевиден из его определения: если рассматриваемое включение 
истинно, то его <<резерв>> --- это наибольший радиус интервала, на который можно <<раздуть>> 
левую часть включения (или сузить правую), чтобы оно ещё оставалось истинным. Если же 
рассматриваемое включение неверно, то мы всегда можем добиться его выполнения, сужая 
левую часть, хотя для этого может понадобиться перейти к неправильным интервалам. В этом 
случае резерв превращается в <<дефицит включения>> и показывает, насколько нужно сузить 
левую часть, чтобы она стала включаться в правую. 
  
Выведем аналитическое представление для резерва $\Rsv$. Если $\mbf{p}$ и $\mbf{q}$ --- 
одномерные интервалы, для которых  $\mbf{p}\subseteq\mbf{q}$, то $\un{\mbf{p}} \geq 
\un{\mbf{q}}$ и $\ov{\mbf{q}} \geq \ov{\mbf{p}}$, а потому  
\begin{equation} 
\label{OneDimRsv} 
\Rsv = \  \min\,\bigl\{\,\un{\mbf{p}} - \un{\mbf{q}}, \,\ov{\mbf{q}} - \ov{\mbf{p}}\,\bigr\}. 
\end{equation} 
Если $\mbf{p}$ и $\mbf{q}$ --- интервальные $n$-векторы, для которых $\mbf{p}\subseteq
\mbf{q}$, то 
\begin{equation} 
\label{MultiDimRsv}
\Rsv = \   \min_{1\leq i\leq n} 
           \min\,\bigl\{\,\un{\mbf{p}}_{i} - \un{\mbf{q}}_{i}, \, 
                          \ov{\mbf{q}}_{i} - \ov{\mbf{p}}_{i}\,\bigr\}. 
\end{equation} 
  
Выражения \eqref{OneDimRsv} и \eqref{MultiDimRsv} не очень удобны для исследования, 
поскольку дают представление для резерва через концы интервалов, тогда как в определении 
резерва раздутие интервала $\mbf{p}$ выполняется симметрично относительно его середины. 
Как следствие, желательно иметь альтернативное представление резерва через середины 
и радиусы интервалов. Имеем 
\begin{align*} 
\min\,\bigl\{\un{\mbf{p}} - \un{\mbf{q}}, \ov{\mbf{q}} - \ov{\mbf{p}}\bigr\} \ 
&=\;\min\,\bigl\{\un{\mbf{p}} - \m\mbf{q} + \r\mbf{q}, 
                             \m\mbf{q} + \r\mbf{q} - \ov{\mbf{p}}\bigr\}               \\[2mm] 
&=\;\r\mbf{q} + \min\,\bigl\{\un{\mbf{p}} - \m\mbf{q}, \m\mbf{q} - \ov{\mbf{p}}\bigr\} \\[2mm] 
&=\;\r\mbf{q} - \max\,\bigl\{\m\mbf{q} - \un{\mbf{p}}, \ov{\mbf{p}} - \m\mbf{q}\bigr\} \\[2mm] 
&=\;\r\mbf{q} - | \mbf{p} - \m\mbf{q}| = \r\mbf{q} - | \m\mbf{q} - \mbf{p}|, 
\end{align*} 
где использовано представление модуля интервала в виде 
\begin{equation*} 
|\mbf{x}| = \max\{ -\un{\mbf{x}}, \ov{\mbf{x}}\} 
\end{equation*} 
(см. \cite{SSharyBook}). Соответственно,  для многомерного случая вместо 
\eqref{MultiDimRsv} получаем следующее выражение для резерва включения: 
\begin{equation*} 
\Rsv \  = \  
   \min_{1\leq i\leq n}\,\bigl\{\; \r\mbf{q}_{i} - |\m\mbf{q}_{i} - \mbf{p}_{i}|\,\bigr\}. 
\end{equation*} 
  
Отталкиваясь от полученного соотношения, можем переписать критерий \eqref{TSScharact} 
принадлежности точки допусковому множеству решений ИСЛАУ в аналитическом виде. Точка 
принадлежит $\tilde{x} = (\tilde{x}_{1}, \tilde{x}_{2}, \ldots, \tilde{x}_{n})$ допусковому 
множеству решений интервальной системы линейных алгебраических уравнений $\mbf{A}x = \mbf{b}$ 
тогда и только тогда, когда 
\begin{equation*} 
\Rsv \,  = \,\min_{1\leq i\leq m}
  \left\{ \,\r\mbf{b}_i - \left|\; \m\mbf{b}_i - \sum_{j=1}^n 
  \,\mbf{a}_{ij} \tilde{x}_{j} \,\right| \,\right\}.  
\end{equation*} 
   
С другой стороны, даже отрицательные значения выражения в левой части имеют смысл, так как 
они показывают «дефицит совместности» системы в точке $\tilde{x}$. В силу сказанного будет 
удобно ввести специальную функцию 
\begin{equation}
\label{TolFuncExpr} 
\index{распознающий функционал} 
\Tol(x, \mbf{A}, \mbf{b})\,  = \,\min_{1\leq i\leq m}
   \left\{ \,\r\mbf{b}_i - \left|\; \m\mbf{b}_i - \sum_{j=1}^n 
   \,\mbf{a}_{ij} x_{j} \,\right| \,\right\}, 
\end{equation} 
которая зн\'{а}ком и величиной своих значений показывает совместность или несовместность 
точки с данными задачи восстановления зависимости и даёт количественную меру этой 
совместности. Будем называть отображение, задаваемое посредством \eqref{TolFuncExpr}, 
распознающим функционалом допускового множества решений  интервальной системы уравнений (5). 
Он обладает следующим основным свойством:  
\begin{equation*}  
\Tol (\beta, \mbf{X}, \mbf{y}) \geq 0  
   \qquad\Leftrightarrow\qquad \beta\in\TSS. 
\end{equation*} 
Описанные выше рассуждения и конструкции можно провести также для случая общих нелинейных функциональных зависимостей, и это сделано в работе \cite{SShary2022}. 
    
Пусть дана интервальная $m\times n$-система линейных алгебраических уравнений $\mbf{A}x 
= \mbf{b}$, в которой $\mbf{A}$ --- интервальная $m\!\times\! n$-матрица, $\mbf{b}$ --- 
интервальный $m$-вектор, $x\in\mbb{R}^n$. Распознающие функционалы объединённого 
множества решений 
\begin{align} 
\Uss (x, & \,\mbf{A}, \mbf{b}) = \notag \\[-5pt] 
             \label{UssExpr}            \\[-2pt] 
\min_{1\leq i\leq m} & 
  \left\{\,\r\mbf{b}_i + \sum_{j=1}^n \,(\r\mbf{a}_{ij})\,|x_{j}| - 
  \left|\,\m\mbf{b}_i - \sum_{j=1}^n \,(\m\mbf{a}_{ij})\, x_j 
  \,\right|\,\right\}, \notag           \\[5mm]  
\Uni(x, & \,\mbf{A}, \mbf{b})\,  = \,\min_{1\leq i\leq m} 
  \left\{ \,\r\mbf{b}_i - \left\langle\; \m\mbf{b}_i - \sum_{j=1}^n 
  \,\mbf{a}_{ij} x_{j} \,\right\rangle \,\right\}.  
\end{align}
Распознающие функционалы посредством знака и величиной своих значений дают 
информацию о том, принадлежит ли точка $x$ соответствующему множеству решений 
и насколько велик запас этой принадлежности или, напротив, насколько далеко 
точка $x$ отстоит от множества решений. Иными словами, распознающие функционалы 
играют роль <<меры совместности>> интервальной системы уравнений на заданной 
точке $x$. 
  
Сказанное выше мотивирует следующий подход к выбору параметров восстанавливаемой 
зависимости: 
\begin{center}
\fboxsep=5pt 
{\color{magenta} 
\fbox{\color{black}%
\begin{tabular}{c}
\rule{0mm}{4mm}%
Оценкой параметров берём точку, в которой достигается \\[1pt]  
наибольшее \ значение \ распознающего \ функционала% 
\rule[-2mm]{0mm}{3mm}  
\end{tabular}}}\;. 
\end{center} 
Этот способ определения параметров зависимости \eqref{LinFunc} будем называть 
\emph{методом максимума совместности}, поскольку распознающий функционал характеризует, 
как отмечено выше, меру совместности параметров восстанавливаемой зависимости и 
интервальных данных. Смотря по тому, какой конкретно распознающий функционал 
применяется, Uni или Tol, мы получаем слабую или сильную версию метода максимума 
согласования. 
  
Рассмотрим далее для определённости сильную версию метода максимума совместности 
и распознающий функционал Tol: 
\begin{itemize}
\item[\color{cyan}$\bullet$] 
если $\max\,\Tol \geq 0$, то точка аргумента максимума функционала Tol 
лежит в непустом информационном множестве, т.\,е. множестве параметров, 
совместных с данными; 
\item[\color{cyan}$\bullet$] 
если $\max\,\Tol < 0$, то информационное множество пусто, но в точке аргумента 
максимума функционала Tol эта несовместность достигает наименьшего значения. 
\end{itemize} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb]
\centering\small  
\unitlength=1mm 
\begin{picture}(90,57)
    \put(0,0){\includegraphics[width=90mm]{pictures/TolGraph.eps}} 
    \put(25,3){\rotatebox{-5}{\scriptsize Ось $0x_1$}}
    \put(78,7){\rotatebox{44}{\scriptsize Ось $0x_2$}} 
    \put(-3,10){\rotatebox{90}{\scriptsize Значения функционала}}    
\end{picture} 
\caption{График распознающего функционала Tol} 
для интервальной системы уравнений \eqref{Sample2DSys}.
\end{figure}       
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Ключевым моментом реализации метода максимума совместности является нахождение точки 
максимума распознающего функционала, т.\,е. решение задачи оптимизации. Целевые функции 
этой задачи --- распознающие функционалы Uss, Tol и другие --- являются негладкими 
кусочно-линейными функциями. В настоящее время задачи нахождения их максимумов решаются 
численно. 
  
Особенно благоприятны свойства распознающего функционала Tol, который является вогнутой 
функцией (см. \cite{SSharyJCT2017,SSharyBook}). Таким образом, задача вычисления 
max Tol — это задача безусловной максимизации вогнутой негладкой целевой функции. 
Её решение может опираться на методы негладкой выпуклой оптимизации, интенсивно 
развиваемые уже в течение нескольких десятков лет различными научными школами у нас 
в стране и за рубежом. В частности, авторы успешно используют результаты работ 
Н.З.~Шора и его сотрудников из Института кибернетики НАН Украины (см., в частности, 
\cite{Stetsyuk, ShorZhurbenko}). 
  
С 2010 года свободно распространяется программа \texttt{tolsolvty}, которая реализует 
так называемый $r$-алгоритм, доступна для систем компьютерной математики Scilab, 
Octave и Matlab, а также в библиотеках на языке Питон. Её можно загрузить с веб-сайта 
<<Интервальный анализ и его приложения>> \cite{InteWebSite} (раздел <<Программное 
обеспечение>> и далее <<Некоторые интервальные программы для Scilab>> или <<Некоторые 
интервальные программы для MATLAB>>). Программа предназначена для численного нахождения 
безусловного максимума распознающего функционала Tol и использует в качестве основы 
алгоритм ralgb5, созданный П.И.\,Стецюком (ему посвящена статья \cite{Stetsyuk}). 
Фактически, \texttt{tolsolvty} --- очень хорошая и проверенная временем реализация 
метода максимума совместности в сильном смысле, которую можно рекомендовать 
для решения практических задач. 
  
Недавно появилась возможность использовать для нахождения максимума распознающего 
функционала Tol методы отделяющих плоскостей, предложенные Е.А.\,Нурминским 
\cite{Nurminski} и развитые далее Е.А.\,Воронцовой \cite{Vorontsova}. На веб-сайте 
<<Интервальный анализ и его приложения>> выложена свободная программа 
\texttt{tolspaclip}, реализующая метод отделяющих плоскостей с дополнительным 
отсечением и предназначенная для тех же целей, что и \texttt{tolsolvty}. Методы 
отсекающих плоскостей хорошо работают при размерностях пространства параметров 
до нескольких тысяч.  
  
Наконец, двумя авторами этой книги в работе \cite{SSharyZhilin} были предложены 
простые и технологичные способы нахождения максимума распознающего функционала, 
опирающиеся на сведение исходной задачи к одной или нескольким задачам линейного 
программирования. Свободная программа \texttt{tolinprog}, реализующая эти алгоритмы 
для системы компьютерной математики Octave, также находится на веб-сайте  
<<Интервальный анализ и его приложения>> \cite{InteWebSite} (раздел <<Программное 
обеспечение>> и далее <<Некоторые интервальные программы для Octave>>).     
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\subsection[Исчерпывание информационного множества]% 
           {Методы исчерпывания \\* информационного множества} 
\label{ExhaInfoSetSect} 
  
   
В этом параграфе книги мы кратко расскажем о тех методах восстановления зависимостей 
по интервальным данным, которые основаны\index{исчерпывание информационного множества} 
на исчерпывающем оценивании информационного множества задачи с помощью семейства брусов. 
В этом случае для информационного множества $\varOmega\subset\mbb{R}^l$ ищутся такие 
семейства брусов $\mcl{U}$ и $\mcl{V}$, 
\begin{equation*} 
\mcl{U} = \{\,\mbf{U}^{(1)}, \mbf{U}^{(2)}, \ldots, \mbf{U}^{(r)}\},  
\qquad 
\mcl{V} = \{\,\mbf{V}^{(1)}, \mbf{V}^{(2)}, \ldots, \mbf{V}^{(s)}\}, 
\end{equation*} 
$\mbf{U}^{(i)}$, $\mbf{V}^{(i)}\in\mbb{IR}^l$, что 
\begin{align*}
&\bigcup_{\mbf{U}^{(i)}\in\mcl{U}} \mbf{U}^{(i)} \  \subseteq \  \varOmega 
   && \text{ --- для внутреннего оценивания,} \\[3mm]   
&\bigcup_{\mbf{V}^{(i)}\in\mcl{V}} \mbf{V}^{(i)} \  \supseteq \  \varOmega 
   && \text{ --- для внешнего оценивания.} 
\end{align*} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{figure}[ht]
\centering\small  
\setlength{\unitlength}{1mm} 
\begin{picture}(75,58) 
\put(0,1){\includegraphics[width=75mm]{pictures/SofiSolSet.eps}} 
\put(71,0.5){$\beta_1$} 
\put(19.5,53){$\beta_2$} 
\end{picture} 
\caption{Исчерпывающее оценивание} 
информационного множества.
\label{ExhEstimPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
    
Стремление получать внутреннюю или внешнюю оценки информационного множества с помощью 
семейств брусов вызвано тем обстоятельством, что оценивание с помощью единственного 
бруса может быть недостаточно качественным из-за значительного отличия формы информационного 
множества от формы оценивающих брусов (см. Рис.~\ref{EstimModesPic}). Внешняя оценка одним 
брусом может содержать много <<лишних точек>>, не принадлежащих информационному множеству, 
а внутренняя оценка одним брусом даёт слишком малую часть информационного множества. 
Тогда естественно попытаться для оценивания использовать конечные совокупности брусов. 
Работать с ними несколько сложнее, чем с отдельными брусами, но не принципиально сложнее. 
В то же время, точность оценивания информационных множеств радикально увеличивается. 
   
Часто возможно одновременное построение семейств брусов $\mcl{U}$ и $\mcl{V}$ в одном 
алгоритме, и потому удобно считать, что $\mcl{U}\subseteq\mcl{V}$, т.\,е. $\mbf{U}^{(i)} 
= \mbf{V}^{(i)}$, $i = 1,2,\ldots,r$, и $r\leq s$. Иными словами, семейство брусов 
$\mcl{U}$ является подсемейством в $\mcl{V}$. Тогда 
\begin{equation} 
\label{TwoSideEstim} 
\bigcup_{i=1}^r \,\mbf{V}^{(i)} \  
   \subseteq \  \varOmega \  \subseteq \ 
   \bigcup_{i=1}^s \,\mbf{V}^{(i)}.  
\end{equation} 
При этом теоретико-множественная разность $\mcl{V}\setminus\mcl{U}$ даёт семейство брусов, 
которое является внешней оценкой границы информационного множества $\varOmega$. Ясно также, 
что подходы к построению и уточнению внутренней оценки информационного множества и оценки 
его границы должны быть различными. Оценивание границы, особенно криволинейной, является 
тем более точным, чем меньше размеры используемых для этого брусов, тогда как брусы, целиком 
лежащие в информационном множестве, выполняют свою оценивающую функцию при любом размере.  
По этой причине при организации алгоритмов исчерпывающего оценивания разумно встроить в них 
измельчение брусов из $\mcl{V}\setminus\mcl{U}$ до определённого предельного размера. 
   
Рассмотрим теперь общую задачу восстановления нелинейной функциональной зависимости 
\eqref{ParamFunc}, 
\begin{equation*}
y = f(x, \beta).  
\end{equation*}
Для неё нужно определить вектор параметров $\beta = (\beta_{1}, \beta_{2}, \ldots, 
\beta_{l})$, при котором функция наилучшим образом приближает (аппроксимирует и т.п.) 
набор интервальных данных \eqref{EmpInData}, измеренных для независимых переменных 
$x_1$, $x_2$, \ldots, $x_m$ и зависимой переменной $y$. Множество всех векторов 
параметров $\beta$, совместных с экспериментальными данными \eqref{EmpInData} --- 
информационное множество $\varOmega$ задачи восстановления зависимости \eqref{ParamFunc} 
--- является некоторым множеством решений интервальной системы уравнений 
\begin{equation*} 
\left\{ 
\begin{array}{l} 
f(\mbf{x}_{i1}, \mbf{x}_{i2}, \ldots, \mbf{x}_{im}, 
   \beta_{1}, \beta_{2}, \ldots, \beta_{l}) = \mbf{y}_{i}, \\[3mm] 
\hspace*{38mm} i = 1,2,\ldots,n, 
\end{array} 
\right. 
\tag{\ref{ParamEqSys}}  
\end{equation*} 
которая получается подстановкой данных \eqref{EmpInData} в соотношение 
\eqref{ParamFunc}. Оценивание $\varOmega$ можно выполнять, как мы знаем, разными 
способами, и ниже рассмотрим исчерпывающее оценивание информационного множества 
$\varOmega$, в котором строятся его внутренняя и внешняя оценки в виде объединений 
брусов с заданным предельным размером. 
  
Предположим, что задан некоторый \emph{начальный брус} $\mbf{V} = (\mbf{V}_{1}$, 
$\mbf{V}_{2}, \ldots, \mbf{V}_{l})\subset\mbb{R}^l$, той же размерности, что и 
вектор $\beta$, в котором заведомо содержится информационное множество $\varOmega$. 
Начальный брус $\mbf{V}$ может быть назначен, исходя из каких-то содержательных 
соображений, которые ограничивают значения $\beta$. Например, это могут быть 
условия неотрицательности на физические величины модели, выражающие массу, 
промежуток времени и т.\,п. Это могут быть условия на естественные верхние границы 
каких-то величин, к примеру, явное ограничение сверху на температуру процесса, 
на его энергетические показатели или координаты его составных частей и т.\,п. 
Исчерпывающее оценивание информационного множества $\varOmega$ будет получено 
с помощью процесса дробления-измельчения начального бруса $\mbf{V}$ и отбрасывания 
его лишних частей, которые гарантированно не содержат точки из $\varOmega$. 
  
Идея алгоритма, который рассматривается ниже, была впервые представлена Р.Е.\,Муром 
в работе  \cite{Moore1992}, а затем усовершенствована в работе Л.\,Жолена и 
Э.\,Вальтера  \cite{JaulinWalter}  (см. также \cite{ApplInteAnal}). Французские 
авторы закрепили за этим алгоритмом название SIVIA, как аббревиатуру фразы <<Set 
Inversion via Interval Analysis>>. Далее мы излагаем собственную модифицированную 
версию алгоритма SIVIA, которая адаптирована для решения задач восстановления 
нелинейных зависимостей с интервальной неопределённостью как во входных, так 
и выходных переменных. Кроме того, наша версия может учитывать различные типы 
совместности параметров с данными. \index{SIVIA, алгоритм} 
   
Пусть $\tilde{\beta} = (\tilde{\beta}_{1}, \ldots, \tilde{\beta}_{l})$ --- какой-то 
вектор  параметров  восстанавливаемой  зависимости.  Подставим его  в левые  части 
уравнений системы \eqref{ParamEqSys} и вычислим интервальные оценки областей значений 
для $f(\mbf{x}_{i1}, \mbf{x}_{i2}, \ldots, \mbf{x}_{im}, \tilde{\beta}_{1}, 
\tilde{\beta}_{2}, \ldots, \tilde{\beta}_{l})$, $i = 1,2,\ldots,n$. Предположим 
также, что эти оценки точны, т.\,е. дают точные области значений соответствующих 
выражений. Тогда по взаимному расположению полученного бруса и бруса $\mbf{y} = 
(\mbf{y}_{1}, \mbf{y}_{2}, \ldots, \mbf{y}_{n})^\top$ правой части системы 
\eqref{ParamEqSys} можно судить о том, как расположен вектор $\tilde{\beta}$ 
относительно множества решений интервальной системы уравнений \eqref{ParamEqSys}. 
  
В самом деле, обозначим 
\begin{equation*} 
\mbf{f} := 
\begin{pmatrix} 
f(\mbf{x}_{11}, \mbf{x}_{12}, \ldots, \mbf{x}_{1m}, 
            \tilde{\beta}_{1}, \tilde{\beta}_{2}, \ldots, \tilde{\beta}_{l}) \\[5pt] 
f(\mbf{x}_{21}, \mbf{x}_{22}, \ldots, \mbf{x}_{2m}, 
            \tilde{\beta}_{1}, \tilde{\beta}_{2}, \ldots, \tilde{\beta}_{l}) \\[5pt] 
\vdots\hspace{12mm}\ddots \hspace{10mm}  \vdots 
                                   \hspace{10mm}  \ddots \hspace{3mm} \vdots \\[5pt]
f(\mbf{x}_{n1}, \mbf{x}_{n2}, \ldots, \mbf{x}_{nm}, 
            \tilde{\beta}_{1}, \tilde{\beta}_{2}, \ldots, \tilde{\beta}_{l}) 
\end{pmatrix}. 
\end{equation*} 
Брус $\mbf{f}$ при сделанных предположениях о точности интервального оценивания 
даёт область значений вектор-функции 
\begin{equation} 
\label{VectorFunc} 
f := 
\begin{pmatrix} 
f(x_{11}, x_{12}, \ldots, x_{1m}, 
      \tilde{\beta}_{1}, \tilde{\beta}_{2}, \ldots, \tilde{\beta}_{l}) \\[5pt] 
f(x_{21}, x_{22}, \ldots, x_{2m}, 
      \tilde{\beta}_{1}, \tilde{\beta}_{2}, \ldots, \tilde{\beta}_{l}) \\[5pt] 
\vdots\hspace{12mm}\ddots \hspace{10mm}  \vdots 
                                   \hspace{10mm}  \ddots \hspace{3mm} \vdots \\[5pt]
f(x_{n1}, x_{n2}, \ldots, x_{nm}, 
            \tilde{\beta}_{1}, \tilde{\beta}_{2}, \ldots, \tilde{\beta}_{l}) 
\end{pmatrix} 
\end{equation} 
при $x_{11}\in\mbf{x}_{1n}$, $x_{12}\in\mbf{x}_{12}$, \ldots, $x_{nm}\in\mbf{x}_{nm}$, 
так как интервальные параметры, входящие в отдельные компоненты вектор-функции $f$, 
независимы друг от друга (см. \S\ref{IndepIntvalSect}). Как следствие, 
\begin{itemize} 
\item[(i)] 
если $\mbf{f}\cap\mbf{y}\neq\varnothing$, то вектор $(\tilde{\beta}_{1}, \ldots, 
\tilde{\beta}_{l})$ лежит в объединённом множестве решений системы уравнений 
\eqref{ParamEqSys}, т.\,е. обеспечивает слабую совместность с интервальными 
данными задачи; 
\item[(ii)]  
если $\mbf{f}\subseteq\mbf{y}$, то вектор $(\tilde{\beta}_{1}, \ldots, 
\tilde{\beta}_{l})$ лежит в допусковом множестве решений интервальной системы 
уравнений \eqref{ParamEqSys}, т.\,е. обеспечивает сильную совместность с данными 
задачи; 
\item[(iii)]  
если $\mbf{f}\cap\mbf{y} = \varnothing$, то вектор $(\tilde{\beta}_{1}, \ldots, 
\tilde{\beta}_{l})$ не лежит в объединённом множестве решений системы уравнений 
\eqref{ParamEqSys}, т.\,е. не является решением задачи, ни сильным, ни слабым. 
\end{itemize} 
   
Предположим теперь, что $\mbf{\beta} = (\mbf{\beta}_{1}, \ldots, \mbf{\beta}_{l})$ 
--- какой-то брус значений вектора параметров $\beta$. Подставим его в левые части 
уравнений системы \eqref{ParamEqSys} и вычислим интервальные оценки областей значений 
для $f(\mbf{x}_{i1}, \mbf{x}_{i2}, \ldots, \mbf{x}_{im}, \mbf{\beta}_{1}, \mbf{\beta}_{2}, 
\ldots, \mbf{\beta}_{l})$, $i = 1,2,\ldots,n$. Как и ранее, обозначим 
\begin{equation} 
\label{InVectFunc} 
\mbf{f} := 
\begin{pmatrix} 
f(\mbf{x}_{11}, \mbf{x}_{12}, \ldots, \mbf{x}_{1m}, 
                \mbf{\beta}_{1}, \mbf{\beta}_{2}, \ldots, \mbf{\beta}_{l}) \\[5pt] 
f(\mbf{x}_{21}, \mbf{x}_{22}, \ldots, \mbf{x}_{2m}, 
                \mbf{\beta}_{1}, \mbf{\beta}_{2}, \ldots, \mbf{\beta}_{l}) \\[5pt] 
\vdots\hspace{14mm}\ddots \hspace{8mm}  \vdots 
                                 \hspace{10mm}  \ddots \hspace{3mm} \vdots \\[5pt]
f(\mbf{x}_{n1}, \mbf{x}_{n2}, \ldots, \mbf{x}_{nm}, 
                \mbf{\beta}_{1}, \mbf{\beta}_{2}, \ldots, \mbf{\beta}_{l}) 
\end{pmatrix}. 
\end{equation} 
В отличие от предыдущего случая, когда рассматривались точечные значения $\beta_1$, 
$\beta_2$, \ldots, $\beta_l$, этот брус $\mbf{f}$, вообще говоря, не совпадает 
с областью значений вектор-функции \eqref{VectorFunc} для $x_{ij}\in\mbf{x}_{ij}$ 
и $\beta_{k}\in\mbf{\beta}_k$, $k = 1,2,\ldots,l$, даже если соответствующие 
интервальные оценки точны. Причина состоит в том, что  интервальные параметры 
$\beta_{1}\in\mbf{\beta}_1$, $\beta_{2} \in\mbf{\beta}_2$, \ldots, $\beta_{l}\in 
\mbf{\beta}_l$ входят одновременно в различные компоненты вектор-функции 
\eqref{VectorFunc}. Тем не менее, брус $\mbf{f}$ является внешней интервальной 
оценкой области значений \eqref{VectorFunc} при $x_{ij}\in\mbf{x}_{ij}$ и $\beta_{k} 
\in\mbf{\beta}_k$, даже интервальной оболочкой этой области значений при сделанных 
нами предположениях. Как следствие, по взаимному расположению бруса $\mbf{f}$, 
полученного как \eqref{InVectFunc}, и бруса $\mbf{y}$ правой части системы 
\eqref{ParamEqSys} всё-таки можно делать некоторые выводы о расположении 
$\mbf{\beta}$ относительно множества решений $\varOmega$. 
  
Более точно, 
\begin{itemize} 
\item[(I)]  
если $\mbf{f}\subseteq\mbf{y}$, то брус $\mbf{\beta} = (\mbf{\beta}_{1}, \ldots, 
\mbf{\beta}_{l})$ лежит в допусковом множестве решений интервальной системы уравнений 
\eqref{ParamEqSys}, т.\,е. все его точки обеспечивают сильную совместность с данными 
задачи; \\[2mm]  
условие $\mbf{f}\subseteq\mbf{y}$ также означает, что брус $\mbf{\beta} = 
(\mbf{\beta}_{1}, \ldots, \mbf{\beta}_{l})$ лежит в объединённом множестве решений 
системы уравнений \eqref{ParamEqSys}, т.\,е. все его точки удовлетворяют слабой 
совместности с данными задачи; 
\item[(II)]  
если $\mbf{f}\cap\mbf{y} = \varnothing$, то брус $\mbf{\beta} = (\mbf{\beta}_{1}, 
\ldots, \mbf{\beta}_{l})$ не лежит в объединённом множестве решений системы уравнений  
\eqref{ParamEqSys} и, как следствие, не лежит и в допусковом множестве решений, т.\,е. 
не является сильным или слабым решением задачи. 
\end{itemize} 
Если же $\mbf{f}\cap\mbf{y}\neq\varnothing$, то мы не можем сделать никакого 
определённого заключения о том, лежит ли брус $\mbf{\beta} = (\mbf{\beta}_{1}, \ldots, 
\mbf{\beta}_{l})$ во множестве решений или нет. Тем не менее, желая обеспечить 
доказательность процесса решения и не потерять значений параметров, совместных 
с данными, мы должны сохранить этот брус, хотя и с более низким статусом, до будущей 
обработки, которая, возможно, определит этот статус точнее. 
  
Итак, выписанные выше инструкции (I)--(II) указывают способ проверки того, принадлежит 
ли брус $\mbf{\beta} = (\mbf{\beta}_{1}, \ldots, \mbf{\beta}_{l})$ информационному 
множеству $\varOmega$ задачи восстановления зависимости. И этот способ даёт тем 
более точные результаты, чем меньше размеры бруса $\mbf{\beta}$, так как тогда меньше 
величина огрубления оценки области значений вектор-функции \eqref{VectorFunc} с помощью 
бруса \eqref{InVectFunc}. Поэтому брусы больших размеров, на которых условия (I)--(II) 
не дают определённого заключения, естественно раздробить на более мелкие части, 
для которых возможностей их успешного применения больше. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{table}[!p]
\centering
\caption{Алгоритм SIVIA для исчерпывающего оценивания \\* 
         параметров нелинейной функциональной зависимости}  
\label{SIVIAlgorithm}  \index{SIVIA, алгоритм}  
\vspace{2pt} 
\color{blue} 
\fboxsep=3mm
\fboxrule=0.5pt
\fbox{\color{black} 
\begin{minipage}{88mm}
\centering 
\begin{tabbing}
A\= AAA\= A\= AA\= \hspace{7em}\=\kill
\>\>\>\>\> \hspace{10pt}\textsf{Вход}\\[2mm] 
\> Функциональная зависимость $y = f(x, \beta)$.                   \\[2pt] 
\> Интервальные эмпирические данные \eqref{EmpInData}.             \\[2pt]   
\> Начальный интервальный брус параметров $\mbf{V}$.               \\[2pt]  
\> Порог мелкости интервальных брусов $\epsilon > 0$.              \\[6mm]
\>\>\>\>\> \hspace{7pt}\textsf{Выход} \\[2mm]
\> Списки брусов $\mcl{U}$ и $\mcl{L}$, 
                      удовлетворяющие условию \eqref{TwoSideEstim} \\[2pt]  
\> покрытия информационного множества задачи и такие,              \\[2pt]  
\> что брусы из $\mcl{U}\setminus\mcl{V}$ имеют размер не более $\epsilon$.                       \\[6mm] 
\>\>\>\>\> \textsf{Алгоритм}\\[2mm] 
\> Инициализируем рабочий список $\mcl{U}$ пустым\,;                  \\[3pt]  
\> Инициализируем рабочий список $\mcl{L}$ брусом $\mbf{V}\,$;        \\[3pt]  
\> \texttt{DO WHILE} ( первый брус списка имеет размер $>\epsilon$ )  \\[3pt] 
\>\> выбираем из списка $\mcl{L}$ брус $\mbf{B}$;                     \\[3pt]  
\>\> дробим $\mbf{B}$ по самой широкой компоненте пополам             \\[2pt]  
\>\>\> на потомки $\mbf{B}'$ и $\mbf{B}''$;                           \\[3pt] 
\>\> применяем к $\mbf{B}'$ тест совместности, по его результатам \;  \\[2pt]   
\>\>\> либо удаляем $\mbf{B}'$ из дальнейшей обработки,               \\[2pt]   
\>\>\> либо заносим в список $\mcl{U}$,                               \\[2pt]  
\>\>\> либо заносим в конец списка $\mcl{L}\,$;                       \\[3pt]  
\>\> применяем к $\mbf{B}''$ тест совместности, по его результатам \; \\[2pt]  
\>\>\> либо удаляем $\mbf{B}''$ из дальнейшей обработки,              \\[2pt]   
\>\>\> либо заносим в список $\mcl{U}$,                               \\[2pt]  
\>\>\> либо заносим в конец списка $\mcl{L}\,$;                       \\[2pt]  
\> \texttt{END DO} 
\end{tabbing} 
\end{minipage}} 
\end{table} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
Далее \emph{дроблением} бруса $\mbf{B}$ мы будем называть порождение, для какого-то 
индекса $\kappa\in \{1, 2, \ldots,l\}$, двух новых брусов --- 
\begin{align*} 
\mbf{B}' = (\mbf{B}_{1}, \ldots, \mbf{B}_{\kappa-1}, & 
\,[\un{\mbf{B}}_{\kappa},\m\mbf{B}_{\kappa}], \mbf{B}_{\kappa+1}, \ldots, \mbf{B}_l) \\[1mm] 
& \text{ и }                                                                         \\[1mm] 
\mbf{B}'' = (\mbf{B}_{1}, \ldots, \mbf{B}_{\kappa-1}, & 
\,[\m{\mbf{B}}_{\kappa},\ov{\mbf{B}}_{\kappa}], \mbf{B}_{\kappa+1}, \ldots, \mbf{B}_l), 
\end{align*} 
которые назовём \emph{потомками} бруса $\mbf{B}$. Фактически, это бисекция, т.\,е. 
дробление на две части, на две равные половинки, и в некоторых ситуациях более 
уместными могут оказаться другие способы разбиения бруса $\mbf{B}$. Например, 
на многопроцессорных вычислительных устройствах можно дробить $\mbf{B}$ на столько 
частей, сколько реально имеется процессоров, поручив обработку каждого потомка 
отдельному процессору. В алгоритме Табл.~\ref{SIVIAlgorithm} дробление бруса 
выполняется по самой широкой компоненте, т.\,е. индекс $\kappa$ ---  это номер, 
на котором достигается $\max_{1\leq i\leq l}\w\mbf{B}_{i}$. Подобный способ дробления 
выбран для того, чтобы в пределе обеспечивалось неограниченное уменьшение размеров 
брусов, т.\,е. сходимость их размеров к нулю \cite{SSharyBook}. Для других способов 
выбора дробимой компоненты $\kappa$ такой сходимости может не быть. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{figure}[ht]
\centering\small  
\setlength{\unitlength}{1mm} 
\begin{picture}(85,65) 
\put(0,1){\includegraphics[width=85mm]{pictures/SophiSolSet.eps}} 
\put(7,8){\mbf{V}} 
\put(81,1){$\beta_1$} 
\put(22,61){$\beta_2$} 
\end{picture} 
\caption{Иллюстрация работы алгоритма SIVIA.} 
\label{SIVIApic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
Те брусы, для которых наверняка показано, согласно (II), что они не содержат точек 
из информационного множества $\varOmega$, можно удалять из рассмотрения. Остальные 
брусы либо лежат в информационном множестве, либо являются подозрительными на включение 
части этого множества. Первые можно выдавать в качестве ответа, а вторые имеет смысл 
хранить далее, до тех пор, пока их статус не определится в процессе обработки алгоритмом. 
Таким образом, мы свяжем с алгоритмом два \emph{рабочих списка}, которые мы будем 
обозначать $\mcl{U}$ и $\mcl{L}$ и в которых, соответственно, будут храниться все брусы, 
\begin{itemize} 
\item 
либо содержащиеся в информационном множестве, 
\item 
либо не лежащие в информационном множестве, \\ 
но могущие включать в себя его точки. 
\end{itemize} 
Один шаг алгоритма состоит в выборе бруса из рабочего списка $\mcl{L}$, его раздроблении 
на более мелкие части и проверке их статуса согласно условиям (I) и (II). Мы будем называть 
применение (I) и (II) \emph{тестом совместности} для бруса (см. Табл.~\ref{SIVIAlgorithm}). 
    
Алгоритм останавливается, когда размеры обрабатываемых брусов делаются меньше 
некоторого заранее назначенного порога $\epsilon$. Под <<размером>> бруса мы понимаем 
какую-нибудь норму вектора ширин его компонент, например, 
\begin{equation*} 
\max_{1\leq i\leq l} |\w\mbf{B}_{i}| 
\qquad\text{ или }\qquad 
\sum_{i=1}^l |\w\mbf{B}_{i}|, 
\end{equation*} 
или какую-нибудь другую аналогичную конструкцию. Результатом работы алгоритма являются 
два таких списка брусов $\mcl{U}$ и $\mcl{L}$, что размеры брусов из $\mcl{L}$ 
не превосходят $\epsilon$, и в целом все брусы из $\mcl{U}$ и $\mcl{L}$ покрывают 
информационное множество задачи, а вне их объединения точек информационного множества 
нет. 
  
Отметим, что алгоритм Табл.~\ref{SIVIAlgorithm} хорошо распараллеливается, обеспечивая 
практически линейное ускорение в зависимости от числа используемых параллельных 
процессоров.  Для  классического  варианта  алгоритма  SIVIA  существуют  готовые 
к использованию реализации на языках \textsc{Matlab} \cite{SIVIAMatlab} и Python  
\cite{SIVIAPython}. 
  
Достоинство методов исчерпывающего оценивания информационного множества --- б\'{о}льшая 
точность и детальность в описании и приближении, но у них есть и существенные недостатки. 
Точечная оценка параметров может быть взята лишь из непустого информационного множества, 
и если оно пусто, то решение задачи восстановления зависимости не может быть получено. 
Таким образом, эти методы в полной мере страдают от парадоксов интервального анализа 
данных, которые разбираются в \S\ref{IStatParadoxesSect}. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\subsection{Метод М.\,Гутовски} 
\label{GutowskiMethSect} 
  
Изложим теперь \emph{метод нарезки информационного множества}, предложенный польским 
физиком М.\,Гутовски \cite{Gutowski}.\index{метод нарезки информационного множества} 
Он аналогичен алгоритму SIVIA, но проще и грубее. Рассмотрим случай восстановления 
линейной функции одной переменной вида $y = \beta_{1}x + \beta_{0}$ по интервальным 
данным $(\mbf{x}_{i}, \mbf{y}_{i})$, $i = 1,2,\ldots,n$. 
  
Как мы уже видели, нахождение параметров восстанавливаемой зависимости по интервальным 
данным \eqref{EmpInData} сводится к решению некоторой задачи удовлетворения ограничениям 
для параметров зависимости. Предполагается, что в интервалах из \eqref{EmpInData} 
существует такой набор точечных значений $x$ и $y$, для которого искомая зависимость 
выполнена. Станем аппроксимировать данные интервальной <<прямой>>, задаваемой выражением 
\begin{equation*} 
y = \mbf{\beta}_{1}x + \mbf{\beta}_{0}. 
\end{equation*} 
Это -- совокупность прямых с уравнениями $y = \beta_{1}x + \beta_{0}$, для которых 
$\beta_{0}\in\mbf{\beta}_0$ и $\beta_{1}\in\mbf{\beta}_1$. Она определяется парой 
интервалов $(\mbf{\beta}_{0}, \mbf{\beta}_{1})$ и в декартовой системе координат 
имеет вид расширяющейся с двух сторон полосы, изображённой на Рис.~\ref{OneEqSoluPic}. 
  
Предположим, что даны интервальные векторы $(\mbf{x}_{i}, \mbf{y}_{i})$, 
$i = 1,2,\ldots,n$, в которых лежат пары значений аргумента и восстанавливаемой функции. 
Слабая совместность параметров и данных диктует следующий набор условий: 
\begin{equation} 
\label{ConsistConds} 
(\mbf{\beta}_{1}\mbf{x}_i + \mbf{\beta}_{0}) \cap \mbf{y}_i \neq\varnothing, 
   \qquad i = 1,2,\ldots,n. 
\end{equation} 
С геометрической точки зрения это означает, что <<интервальная прямая>> с угловым 
коэффициентом $\mbf{\beta}_1$ и свободным членом $\mbf{\beta}_0$ пересекает брусы 
неопределённости $(\mbf{x}_i, \mbf{y}_i)$, $i = 1,2,\ldots,n$, полученные в результате 
измерения зависимости. В терминах концов интервалов условие \eqref{ConsistConds} 
равносильно следующему: 
\begin{equation}
\label{BoxConsisIneqs} 
\un{\mbf{\beta}_{1}\mbf{x}_i + \mbf{\beta}_0} \leq \ov{\mbf{y}}_{i} 
   \  \text{ или } \  \ov{\mbf{\beta}_{1}\mbf{x}_i + \mbf{\beta}_0}  
   \geq \un{\mbf{y}}_{i},   \qquad  i = 1,2,\ldots,n. 
\end{equation} 
Выписанное условие можно использовать для уточнения множества параметров, на котором 
может достигаться их совместность с данными. 
  
Предположим, что нам известен некоторый брус $\mbf{V} = (\mbf{\beta}_{0}, \mbf{\beta}_{1})$, 
такой что все неравенства \eqref{BoxConsisIneqs} \emph{могут быть} удовлетворены внутри 
него, но \emph{точно} не на его границах. Чтобы уточнить интервалы для $a$ и $b$ можно 
выполнить следующие шаги: 
\begin{enumerate} 
\item 
Возьмём $\mbf{V}$ и с помощью \emph{алгоритма дробления бруса} (он приведён ниже), 
используя неравенства $(\un{\mbf{\beta}_{1}\mbf{x}_j + \mbf{\beta}_0}) \leq 
\ov{\mbf{y}}_{j}$, получим новый брус $\mbf{V}'$, на котором всё ещё выполняются 
неравенства; 
\item 
Аналогично с помощью алгоритма дробления бруса получим из $\mbf{V}$ брус $\mbf{V}''$, 
на котором выполнены неравенства $(\ov{\mbf{\beta}_{1}x_j + \mbf{\beta}_0}) \geq 
\un{\mbf{y}}_{j}$. 
\item 
Если $\mbf{V}' \cap \mbf{V}'' \neq \mbf{V}$, то присваиваем $\mbf{V} \leftarrow 
\mbf{V}' \cap \mbf{V}''$ и переходим к шагу~1, иначе останавливаемся. 
\end{enumerate} 
  
Последний шаг иллюстрирует очень важное и часто применяемое правило интервальных 
вычислений: если внешняя оценка искомого значения может быть получена более чем одним 
способом, следует использовать их все и пересечь полученные результаты. Иногда на этом 
шаге пересечение $\mbf{V}' \cap\mbf{V}''$ может оказаться пустым. В этом случае можно  
быть уверенным, что исходный брус $\mbf{V}$ не содержит решений, и в случае накрывающих 
измерений это означает одно из двух: 
\begin{itemize}
\item  данные содержат один или более выбросов (промахов); 
\item  используемая зависимость между входными и выходными \\ 
       переменными неадекватно описывает поведение системы.
\end{itemize}

Алгоритм дробления бруса приведён ниже. Непосредственный псевдокод описывает фазу 
\emph{отсечения слева}. Отсечение справа может быть получено заменой строчек 2, 5 и 7 
на комментарии (окружены последовательностями символов «/*» и «*/»). Полный алгоритм, 
приведённый в таблице \ref{code2} представляет собой отсечение слева и отсечение 
справа, применённые к брусу в произвольном порядке. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\bigskip 
\begin{table}[!htb]
\centering
\caption{Алгоритм нарезки бруса параметров}  
\label{code2} 
\vspace{2pt} 
\color{blue} 
\fboxsep=3mm
\fboxrule=0.5pt
\fbox{\color{black} 
\begin{minipage}{88mm}
\centering   
\begin{tabbing} 
A\= \qquad \= \qquad \= \qquad \= \kill 
\> \textbf{\tt{for}} $i = 1$ \textbf{\tt{to}} $p$ \textbf{\tt{do}}    \\[2pt]  
\>\> $\xi \leftarrow 1$ \qquad /*$\xi \leftarrow 0$*/                 \\[2pt] 
\>\> $k \leftarrow 1$                                                 \\[2pt]  
\>\> \textbf{\tt{repeat}}                                             \\[2pt] 
\>\>\> $\xi \leftarrow \xi/2$ \qquad /*$\xi \leftarrow (1 + \xi)/2$*/ \\[2pt] 
\>\>\> $k \gets k + 1$                                                \\[2pt] 
\>\>\> $\mbf{V}' \gets \mbf{a}_1 \times \mbf{a}_2 \times \ldots \times  
   \bigl[\,\un{\mbf{a}}_{i}, \xi(\ov{\mbf{a}}_i - \un{\mbf{a}}_{i}\bigr] 
   \times \ldots \times \mbf{a}_p$                                    \\[2pt]  
\>\>\> /*$\mbf{V}' \leftarrow \mbf{a}_1 \times \mbf{a}_2 \times \ldots 
   \times \bigl[\,\xi(\ov{\mbf{a}}_i - \un{\mbf{a}}_{i}), \ov{\mbf{a}}_{i}\bigr] 
   \times\ldots \times    \mbf{a}_p$*/                                 \\[2pt] 
\>\>\> success $\gets$ $0$ (\textit{в $\mbf{V}'$ выполнены все условия})\\[2pt] 
\>\>\> \textbf{\tt{if}} success \textbf{\tt{then}}                     \\[2pt]  
\>\>\>\> $\mbf{V} \leftarrow \mbf{V} \backslash \mbf{V}'$              \\[2pt]  
\>\>\> \textbf{\tt{end if}}                                            \\[2pt] 
\>\> \textbf{\tt{until}} (\tt{success} \textbf{\tt{или}} $k > M$)\\
\> \textbf{\tt{end for}}
\end{tabbing}
\end{minipage}}  
\end{table} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
  
Переменная \texttt{success} в этом псевдокоде имеет логический тип. Число $M$ 
обозначает количество двоичных разрядов (битов) в представлении чисел с плавающей 
точкой в используемой модели вычислений (включая <<скрытый бит>>, всегда равный $1$). 
Например, для чисел с одинарной точностью $M = 24$ и для чисел с двойной точностью 
$M = 53$ согласно стандартам IEEE 754/854 на вычисления с плавающей точкой на ЭВМ 
(см., к примеру, \cite{Goldberg}). Смысл условия остановки $k > M$ состоит в том, 
что в силу конечного представления машинных чисел при делении интервала пополам 
его концы не могут приблизиться друг к другу менее чем на единицу младшего разряда 
из мантиссы.    
  
Отметим, что описанная выше процедура даёт интервальную оболочку множества возможных 
решений, но не каждая точка из неё в самом деле является решением. С другой стороны, 
ни одна из точек, лежащих за пределами полученного в результате бруса не может 
удовлетворять условиям. За графическим представлением можно обратиться, например, 
к работе \cite{Gutowski}. 
  
Алгоритм может не получить решение задачи, если неопределённость какого-либо 
измерения недооценена, т.\,е. назначена меньшей реально присутствующей погрешности. 
   
        
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\section{Выбросы и их выявление} 
\label{RegrOutlSect} 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   
\subsection{Общие идеи выявления выбросов} 

% Выявление и учёт выбросов является типичной проблемой при анализе данных. В рамках 
% интервального подхода наработан довольно широкий набор математических методов и 
% приёмов, помогающих аналитику так или иначе решать эти проблемы. Однако до описания 
% собственно математических инструментов  проясним их роль и место в процедурах 
% анализа, а также уточним смысл употребляемой терминологии. 
  
Понятие <<выброс>> в статистике и анализе данных, как правило, определяется нечётко 
и неформально. Объясняется это тем, что основания для признания измерения выбросом 
лежат за пределами формальной математической постановки задачи анализа данных и 
требуют привлечения внешних по отношению к ней знаний из предметной области и истории 
происхождения данных, специфичных в каждом конкретном случае. Тем не менее, главный 
объединяющий смысл различных определений --- указание на нарушение измерением-выбросом 
некоторой однородности (согласованности, непротиворечивости), ожидаемой для большинства 
наблюдений выборки по отношению к заданной математической модели. Подчеркнём эту особую 
роль модели и неабсолютный  характер понятия <<выброс>>, вкупе означающие, что статус 
измерения в одной и той же выборке может меняться в зависимости от вида модели, 
рассматриваемой на конкретном этапе анализа данных. Поэтому, строго говоря, 
утверждения вида <<измерение $x_i$ является выбросом в выборке $X$>> всякий раз 
должны сопровождаться оговоркой --- <<относительно такой-то модели>>, если это явно 
не следует из контекста. 

Интервальный подход даёт естественный формальный индикатор согласованности данных, 
модели и априорной информации — непустоту информационного множества, соответствующего 
задаче. Пустота информационного множества свидетельствует о возможном наличии тех или 
иных противоречий между данными и моделью. Поиск причин появления противоречий, а 
также выбор путей их преодоления --- процесс творческий и неформальный, большей частью 
опирающийся на прикладные соображения и экспертные знания о моделируемом явлении 
или процессе и условиях получения данных. Формальные приёмы и математические методы, 
задействованные в этом процессе, выполняют важную, но подчиненную роль. Они используются 
для получения информации о данных и модели, позволяющей выдвигать гипотезы о причинах  
противоречий, вырабатывать способы коррекции данных или модели и оценивать обеспечиваемые 
ею результаты. Иными словами, математические методы отвечают на вопрос <<как устроены 
данные?>>, в то время как ответы на вопросы <<почему так устроены данные?>> и 
<<что делать?>> может дать только содержательный анализ моделируемого явления. 

Причинами  возникновения противоречий в задаче анализа данных могут служить 
как некорректность измерений (вследствие нарушений условий их проведения, регистрации, 
сбоев при передаче, некорректной оценки уровня неопределённости, нештатного поведения 
моделируемой системы и т.п.), так и некорректность модели (вид модели не соответствует 
моделируемому явлению и т.п.). При использовании формальных методов выявления выбросов 
следует иметь в виду, что выбросы могут оказаться наиболее существенной частью выборки, 
проливающей свет на то, как собирались данные или каково истинное поведение изучаемой 
системы или процесса, не укладывающееся в исходные предположения. Учитывая, что 
предметом интервального анализа часто становятся малые выборки, обычная тактика 
удаления <<подозрительных>> измерений должна использоваться с особой осторожностью. 
  
%Перейдем к описанию приёмов и методов, полезных при исследовании и разрешении 
%противоречий, в том числе, обусловленных наличием выбросов. 
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\textbf{Обозначения \textcolor{red}{(подлежат обсуждению)}}
  
$s_i = (x_i,\mbf{y}_i)$ --- наблюдение, состоящее из значения входной переменной 
$x\in\mbb{R}^{m}$ и   интервального измерения $\mbf{y}_i$ выходной переменной 
$y\in \mbb{R}$. 
  
$\eus{S}_{n} = \{s_i\}_{i=1,\dots,n} = \{(x_i,\mbf{y}_i)\}_{i=1,\dots,n}$ --- выборка 
из $n$ наблюдений.

$y(x) = f(x,\beta)$ --- модель с параметрами $\beta \in \mbb{R}^{m+1}$, например, 
линейная $y(x) = \beta_0 + \beta_1 x_1 + \ldots + \beta_m x_m.$

$\varUpsilon(x)$ --- коридор совместных зависимостей. 

%$\eus{UC}(x)$ --- коридор совместных зависимостей. 
 
%$\mathrm{unc}\hspace{0.5pt}(x)$ --- коридор совместных зависимостей. 
 
%$\mathrm{uncr}\hspace{0.5pt}(x)$ --- коридор совместных зависимостей.  

$\varUpsilon(x; \eus{S}_{n})$ --- коридор совместных зависимостей, построенных 
по выборке $\eus{S}_{n}$. 


$\varOmega_i = \varOmega(s_i) = \{\,\beta \mid f(x_i,\beta) \subseteq \mbf{y}_i\}$ 
--- информационное множество наблюдения $s_i  = (x_i, \mbf{y}_i)$.

$\varOmega = \varOmega(\,\eus{S}_{n}) = \cap_{i=1}^n \varOmega_i$ --- информационное 
множество задачи построения модели $y(x) = f(x,\beta)$ по выборке  $\eus{S}_{n}$.
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\subsection{Статус измерений \textcolor{red}{(неокончательно)}} 
\label{MeasrStatusSect}
  
О влиянии некоторого интервального измерения $s = (x,\mbf{y})$ на модель, построенную 
по выборке $\eus{S}_{n}$, можно судить на основе того, в каком взаимоотношении находятся 
информационные множества $\varOmega(s)$ и $\varOmega(\eus{S}_{n})$. Такая характеризация 
полезна как для <<новых>> измерений ($s \notin \eus{S}_{n}$), так и для измерений, уже 
входящих в выборку ($s \in \eus{S}_{n}$). 
  
Измерения, добавление которых к выборке не приводит к модификации модели 
($\varOmega(\eus{S}_{n}) = \varOmega(\eus{S}_{n} \cup s)$), именуются (согласно 
\cite{PomeRodionova, Rodionova}) \textit{внутренними}, изменяющие же модель 
($\varOmega(\eus{S}_{n}) \supset \varOmega(\eus{S}_{n} \cup s)$) --- \textit{внешними}. 
В каждом из этих классов измерений дополнительно выделяют специальные подклассы 
--- \textit{граничные} измерения и \textit{выбросы} соответственно 
(Рис.~\ref{ObservStatusParam}). 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
  
\begin{figure}[ht]
\centering\small  
\setlength{\unitlength}{1mm} 
\begin{picture}(85,76) 
%\begin{picture}(76,76) 
%\centering 
\put(5.3,0)
{\includegraphics[width=85mm]{pictures/InfSetSampleObserv.pdf}} 
\put(0,72.5){(а)}
\put(49.0,72.5){(б)}
\put(0,32.5){(в)}
\put(49.0,32.5){(г)}
\put(9.5,53){$\varOmega(s)$}
\put(25,57){$\varOmega(\eus{S}_{n})$}
\put(57.5,58){$\varOmega(s)$}
\put(72.5,55){$\varOmega(\eus{S}_{n})$}
\put(17,28){$\varOmega(s)$}
\put(26,13.5){$\varOmega(\eus{S}_{n})$}
\put(74.5,18){$\varOmega(s)$}
\put(60.5,8){$\varOmega(\eus{S}_{n})$}
\end{picture} 
\caption{Информационные множества, построенные \\ по выборке $\eus{S}_{n}$ и наблюдению 
$s$ с различными статусами:\\ (а) \textit{внутреннее}, (б) \textit{граничное}, (в) 
\textit{внешнее}, (г) \textit{выброс}. } 
\label{ObservStatusParam}  
\end{figure} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\textit{Граничными} называют измерения, определяющие какой-либо фрагмент границы 
информационного множества. Очевидно, это свойство имеет смысл рассматривать для 
наблюдений, принадлежащих выборке $\eus{S}_{n}$, по которой сконструирована модель 
и её информационное множество $\varOmega(\eus{S}_{n})$. Подмножество всех граничных 
наблюдений в $\eus{S}_{n}$ играет особую роль, поскольку оно является минимальной 
подвыборкой, полностью определяющей модель. Удаление неграничных наблюдений из выборки 
не изменяет модель.     \index{граничное измерение} 
  
Среди внешних измерений особым образом выделяют \textit{выбросы} (промахи). Построение 
модели по выборке, пополненной таким наблюдением, приводит не просто к уменьшению 
информационного множества, а к его пустоте  ($\varOmega(\eus{S}_{n} \cup s) 
= \varnothing$), то есть к <<разрушению>> модели. \index{выброс} 
    
Существует экономичный способ определения статуса измерения, не требующий явного 
перестроения модели для выборки, расширенной анализируемым измерением. Анализ 
взаимоотношений информационных множеств $\varOmega(\eus{S}_{n})$ и $\varOmega(\eus{S}_{n} 
\cup s)$ или $\varOmega(\eus{S}_{n})$ и $\varOmega(s)$ можно заменить выяснением отношений 
интервала неопределённости $\mbf{y}$ анализируемого измерения $s = (x, \mbf{y})$ и 
интервального прогнозного значения рассматриваемой модели в той же точке $\varUpsilon 
(x; \eus{S}_{n})$. На Рис.~\ref{ObservStatus} анализируемые измерения показаны чёрными 
линиями, а соответствующие им интервалы прогнозов --- широкими цветными линиями (в данном 
случае их ширина не имеет содержательного смысла, а лишь упрощает восприятие наложенных 
друг на друга интервалов). 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{figure}[ht]
\centering\small  
\setlength{\unitlength}{1mm} 
\begin{picture}(60.5,45) 
\put(0,0){\includegraphics[width=60mm]{pictures/ObservStatuses.pdf}} 
\end{picture} 
\caption{Интервальные наблюдения с различными статусами: \textit{внутреннее} 
($n=1,\dots,3$), \textit{граничные} ($n=2,3$), \textit{внешние} ($n=4,\dots,8$), 
\textit{строго внешнее} ($n=6$), \textit{выбросы} ($n=7, 8$). } 
\label{ObservStatus}  
\end{figure}  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  

\begin{figure}[hb]
\centering\small  
\setlength{\unitlength}{1mm} 
\begin{picture}(60.5,45) 
\put(0,0){\includegraphics[width=60mm]{pictures/StatusPlot.pdf}} 
\end{picture} 
\caption{Диаграмма статусов для интервальных наблюдений, \\ 
показанных на Рис.~\ref{ObservStatus}. Зоны наблюдений с различными статусами обозначены
цветами: зелёный --- внутренние наблюдения, жёлтый --- внешние, красный --- выбросы.} 
\textcolor{red}{ Изменить обозначение  $h$ на $l$}
\label{StatusPlot}  
\end{figure}  
%\textcolor{red}{(неокончательный вариант)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
   
Внутреннее интервальное измерение $s = (x,\mbf{y})$ полностью содержит в себе 
прогнозный интервал, оцененный с помощью модели $\varUpsilon(x;\eus{S}_{n})$, или, 
иными словами, пересечение двух этих интервалов совпадает с прогнозным: $\mbf{y} 
\cap \varUpsilon(x;\eus{S}_{n}) =  \varUpsilon(x;\eus{S}_{n})$. Будучи перестроенной 
по выборке, пополненной подобным измерением, модель не претерпит изменений, поскольку 
соответствующее ей информационное множество окажется внутри ограничения, порожденного 
добавленным внутренним измерением, а, следовательно, пересечение с ним не изменится. 
Коридор совместных зависимостей при этом также сохранит прежний вид. 
  
Если внешнее интервальное измерение и соответствующий ему интервал прогноза имеют 
непустое пересечение, то результирующий интервал сужается по сравнению с прогнозным:  
$\mbf{y} \cap \varUpsilon(x; \eus{S}_{n}) \subset \varUpsilon(x;\eus{S}_{n})$. 
Это означает, что добавление внешнего измерения в модель уменьшит информационное 
множество задачи и коридор совместных зависимостей. Получение пустого множества 
в пересечении свидетельствует о том, что измерение, возможно, является выбросом 
по отношению к используемой модели. В некоторых ситуациях, когда требуется более 
высокий <<уровень подозрительности>>, предпринимать меры можно не при строгой пустоте 
информационного множества, а уже при некотором неестественно малом его размере 
\cite{KreinovichGeodesy}. 
  
Взаимные отношения интервалов анализируемого наблюдения $(x,\mbf{y})$ и прогнозного 
интервала рассматриваемой модели $\varUpsilon(x)$ удобно характеризовать в специальных 
терминах. Введём понятия \emph{размаха} (плечо, англ. --- high leverage)\index{размах} 
\begin{equation} 
\label{IntLeverage}
%h(x,\mbf{y}) = \frac{\r \varUpsilon(x)}{\r\mbf{y}} 
\ell(x,\mbf{y}) = \frac{\r \varUpsilon(x)}{\r\mbf{y}} 
\end{equation}
и \emph{относительного остатка} (относительное остаточное отклонение,  относительное смещение, англ. --- relative residual) 
\begin{equation} 
\label{IntResidual}
\index{относительный остаток}
r(x,\mbf{y}) = \frac{\m\mbf{y} - \m\varUpsilon(x)}{\r\mbf{y}}. 
\end{equation}
Обе величины являются относительными, поскольку нормируются на величину неопределённости 
наблюдения $y$. Размах наблюдения косвенно характеризует положение наблюдения 
в пространстве  независимых переменных $x_i$. Наблюдения с размахом выше единицы лежат 
за пределами <<области определения>> зависимости, образованной наблюдениями выборки, 
по которой построена зависимость. Остаток характеризует смещение наблюдения по откликовой 
переменной $y$ относительно коридора совместных зависимостей. Наблюдения с большими 
значениями размаха и остатка при их включении в выборку, по которой построен коридор 
совместных зависимостей, могут существенно повлиять на его вид. 
 
Размах и остаток позволяют установить статус наблюдения, проверив некоторые 
простые неравенства. 

Так для внутренних наблюдений, содержащих в себе прогнозный интервал модели, 
выполняется нестрогое неравенство 
\begin{equation} 
\label{IneqInlier} 
|r(x,\mbf{y})| \leq 1 - \ell(x,\mbf{y}),
\end{equation}
а точное равенство в нём является характеристическим условием для граничных наблюдений.

Выбросы --- наблюдения, не пересекающиеся с коридором совместных зависимостей, а потому 
они удовлетворяют неравенству 
\begin{equation} 
\label{IneqOutlier}
    |r(x,\mbf{y})| > 1 + \ell(x,\mbf{y}).
\end{equation}

Интервальные измерения, у которых величина неопределённости меньше, чем ширина 
прогнозного интервала, то есть 
\begin{equation} 
\label{IneqAbsOutsider} 
\ell(x,\mbf{y}) > 1,
\end{equation}
могут оказывать очень сильное влияние на модель и потому называются 
\index{строго внешнее измерение} \textit{строго внешними}.%    
\footnote{В \cite{PomeRodionova} и последующих работах О.Е.~Родионовой 
и А.Л.~Померанцева, предложивших эту классификацию статусов наблюдений, 
для обозначения строго внешнего наблюдения используется термин 
<<абсолютно внешнее наблюдение>>, который неудачен из-за пересечения 
смысла с общематематическими понятиями <<абсолютная величина>>, 
<<абсолютная погрешность>>, <<абсолютно непрерывный>> и т.п.} 
  
Удобным инструментом анализа ролей наблюдений и их влияния на уже имеющуюся модель 
является \textit{диаграмма статусов}, пример которой приведен на Рис.~\ref{StatusPlot}. 
Наблюдения на диаграмме статусов представляются точками на плоскости, координаты 
которых  задаются остатком и размахом.\index{диаграмма статусов} Неравенства 
(\ref{IneqInlier})--(\ref{IneqAbsOutsider}) на этой плоскости задают границы областей, 
соответствующих различным статусам наблюдений. Зона внутренних наблюдений выделена 
зелёным цветом. Наблюдения, размещенные на границе зелёной зоны, являются граничными 
для информационного множества задачи. Зона внешних наблюдений ---  жёлтая. Правее 
вертикали $\ell(x,\mbf{y}) = 1$ лежат абсолютно внешние наблюдения. Выбросы 
локализуются в красной зоне. 
% По своему предназначению диаграмма статусов наблюдений близка к графику 
% влияния, традиционно используемому для анализа роли наблюдений при построении 
% зависимостей МНК. 
 
Примечательно, что характеризация наблюдений в терминах размахов и остатков не зависит 
от размерности входной переменной $x$ и позволяет поддержать анализ статусов наблюдений 
визуальными инструментами даже в случаях, когда явное отображение информационного 
множества задачи и коридора совместных зависимостей затруднительно. По своему 
назначению диаграмма статусов интервальных наблюдений является содержательным аналогом 
широко используемого в классическом регрессионном анализе графика влияния (английский 
термин --- influence plot), который также служит для оценки степени однородности 
(похожести) наблюдений и их потенциальной влиятельности на конструируемую зависимость. 
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\subsection[Варьирование неопределённости измерений]% 
           {Варьирование величины \\*  неопределённости измерений} 
\label{VaryUncertSect} 
  
Один из приёмов выявления выбросов в задаче построения зависимости по интервальным
наблюдениям основан на интерпретации выбросов как наблюдений с недооценённой величиной 
неопределённости \cite{ZhilinDiss, Zhilin2007}. Закономерным шагом в этом случае 
становится поиск некоторой минимальной коррекции величин неопределённости интервальных 
наблюдений, необходимой для обеспечения совместности задачи построения зависимости. 
Если величину коррекции каждого интервального наблюдения 
$\mbf{y}_i = [\mathring{y}_i - \epsilon_i, \mathring{y}_i + \epsilon_i] = 
\< \mathring{y}_i, \epsilon_i \> $ выборки $\eus{S}_{n}$ выражать коэффициентом 
его уширения $w_i \geq 1$, а общее изменение выборки характеризовать суммой этих
коэффициентов, то минимальная коррекция выборки в виде вектора коэффициентов 
$w^* = (w_1^*, \dots, w_n^*)$, необходимая для совместности задачи построения
зависимости $y = f(x,\beta)$ может быть найдена решением задачи условной оптимизации 
\begin{equation} 
\label{MinSumW_Subj} 
\text{найти} \quad \min_{w,\beta}\;\Sigma_{i=1}^n w_{i} 
\end{equation}
при ограничениях
\begin{equation} 
\label{MinSumW_Constr} 
\left\{ \ 
\begin{gathered}
\mathring{y}_i - w_i \, \epsilon_i \leq f(x_i,\beta) 
   \leq \mathring{y}_i + w_i \, \epsilon_i,    \\[2pt]   
w_i \geq 1, 
\end{gathered}
\qquad 
i = 1,\dots,n. 
\right. 
\end{equation}
  
Результирующие значения коэффициентов $w_i^*$, строго превосходящие единицу, указывают 
на наблюдения, которые требуют уширения интервалов неопределённости для обеспечения 
совместности данных и модели. Именно такие наблюдения заслуживают внимания при анализе 
данных на выбросы. Значительное количество подобных наблюдений может говорить либо 
о неверно выбранной структуре зависимости, либо о том, что величины неопределённости 
измерений занижены во многих наблюдениях (например, в результате неверной оценки 
точности измерительного прибора). 
  
Следует отметить значительную гибкость языка неравенств. Он даёт возможность 
переформулировать и расширять систему ограничений \eqref{MinSumW_Constr} для учёта 
специфики данных и задачи при поиске допустимой коррекции данных, приводящей 
к разрешению исходных противоречий. Например, если имеются основания считать, 
что величина неопределённости некоторой группы наблюдений одинакова и при коррекции 
должна увеличиваться синхронно, то система ограничений (\ref{MinSumW_Constr}) может 
быть пополнена равенствами вида 
\begin{equation*} 
w_{i_1} = w_{i_2} = \cdots = w_{i_K}, 
\end{equation*} 
где $i_1, \dots, i_K$ --- номера наблюдений группы.
В случае, когда в надёжности каких-либо наблюдений исследователь уверен полностью, 
при решении задачи (\ref{MinSumW_Subj})--(\ref{MinSumW_Constr}) соответствующие 
им величины $w_i$ можно положить равными единице, т.е. запретить варьировать их 
неопределённость. 

Задача поиска коэффициентов масштабирования величины неопределённости 
(\ref{MinSumW_Subj})--(\ref{MinSumW_Constr}) сформулирована для 
распространённого случая уравновешенных интервалов погрешности и подразумевает 
синхронную подвижность верхней и нижней границ интервалов неопределённости 
измерений $\mbf y_i$ при сохранении базовых значений интервалов $\mathring{y}_i$ 
неподвижными. При необходимости постановка задачи легко обобщается. Например, если 
интервалы наблюдений не уравновешены относительно базовых значений 
(то есть $\mbf y_i = [\mathring{y}_i - \epsilon^{-}_i,\, \mathring{y}_i + 
\epsilon^{+}_i ]$ и $\epsilon^{-} \neq \epsilon^{+}$), 
то границы интервальных измерений можно варьировать независимо, масштабируя 
величины неопределённости $\epsilon^{-}_i$ и $\epsilon^{+}_i$ с помощью отдельных 
коэффициентов $w_i^{-}$  и $w_i^{+}$: 
\begin{equation} 
\label{MinSumW_Subj_Nonsym} 
\text{найти} \quad \min_{w^{-},\,w^{+},\,\beta}\;\Sigma_{i=1}^n (w_i^{-} + w_i^{+}) 
\end{equation}
при ограничениях
\begin{equation} 
\label{MinSumW_Constr_Nonsym} 
\left\{ \ 
\begin{gathered}
\mathring{y}_i - w_i^{-} \, \epsilon^{-}_i \leq f(x_i,\beta) \leq 
\mathring{y}_i + w_i^{+} \, \epsilon^{+}_i, \\[2pt] 
w_i^{-} \geq 1, \\[2pt]  
w_i^{+} \geq 1, 
\end{gathered}
\qquad i = 1,\dots,n. 
\right. 
\end{equation}
  
Для линейной по параметрам $\beta$ зависимости $y = f(x,\beta)$ задача 
(\ref{MinSumW_Subj})--(\ref{MinSumW_Constr}) представляет собою задачу линейного 
программирования, для решения которой широко доступны хорошие и апробированные 
программы в составе библиотек на различных языках программирования, в виде стандартных 
процедур систем компьютерной математики, а также в виде интерактивных подсистем 
электронных таблиц. 
  
\begin{example} 
Наблюдения из Табл.~\ref{TabOutliers} получены четырьмя различными способами \textit{A}, 
\textit{B}, \textit{C} и \textit{D}, обеспечивающими различные величины $\epsilon_i$ 
неопределённости измерений выходной переменной $\mbf y_i = [\mathring{y}-\epsilon_i, 
\,\mathring{y}+\epsilon_i]$ для точно задаваемых значений входной переменной $x_i$. 
Диаграмма рассеяния\index{диаграмма рассеяния} интервальных данных приведена 
на Рис.~\ref{DataWithOutliers}. По этим данным требуется построить линейную зависимость 
вида $y = \beta_{0} + \beta_{1} x$. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\begin{table}[htb]
  \centering
  \caption{Данные с выбросами}
  \label{TabOutliers}
  \begin{tabular}{ccccc} 
  \hline 
  \makecell{Номер\\ измерения\\$i$} & \makecell{Способ\\ измерения} & $x_i$ & 
  $\mathring{y}_i$ & $\epsilon_i$ \\ \hline 
   1 & \textit{A} &  1 &  2.13 & 0.20 \\
   2 & \textit{A} &  2 &  2.95 & 0.20 \\
   3 & \textit{A} &  3 &  5.01 & 0.20 \\
   4 & \textit{A} &  4 &  4.99 & 0.20 \\
   5 & \textit{A} &  5 &  5.97 & 0.20 \\
   6 & \textit{B} &  6 &  7.04 & 0.40 \\
   7 & \textit{B} &  7 &  8.02 & 0.40 \\
   8 & \textit{C} &  8 &  8.15 & 0.40 \\
   9 & \textit{C} &  9 & 10.01 & 0.40 \\
  10 & \textit{D} & 10 & 10.98 & 0.50 \\ \hline
  \end{tabular}
\end{table}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
% \begin{figure}[ht]
% \centering\small  
% \setlength{\unitlength}{1mm} 
% \begin{picture}(80,64) 
% \put(0,0){\includegraphics[width=80mm]{pictures/DataWithOutliers.pdf}} 
% \end{picture} 
% \caption{Данные с выбросами.} 
% \label{DataWithOutliers}  
% \end{figure}  
  
\begin{figure}[ht]
\centering 
\subfile{pictures/DataWithOutliers}
\caption{Данные с выбросами.}
\label{DataWithOutliers}  
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
Приступая к решению задачи, сразу же обнаруживаем, что её информационное множество 
пусто. В чём же причина? 
  
Чтобы понять, имеются ли в данных выбросы, для начала попытаемся решить для этих данных 
задачу \eqref{MinSumW_Subj}--\eqref{MinSumW_Constr}. Полученные в результате значения 
масштабирующих коэффициентов величины неопределённости $w_i^*$  приведены в первом 
столбце таблицы \ref{TabOutliersWeights}. Анализируя эти числа, можно прийти к выводу 
о том, что третье и восьмое наблюдения несовместны с остальными. При этом третье 
наблюдение особенно не вписывается в общую картину, поскольку сделать его совместным 
с прочими возможно только при расширении интервала измерения более, чем в четыре 
с половиной раза, хотя способ \textit{A}, которым получено третье наблюдение, является 
наиболее точным из всех четырёх. Эти соображения склоняют к заключению, что третье 
измерение вполне вероятно может оказаться результатом грубых промахов, и потому 
стоит проанализировать данные с исключением этого измерения из всех дальнейших 
построений. 
  
Что касается восьмого наблюдения, то его несовместность менее выражена. Поэтому здесь 
разумно проверить гипотезы как о грубых промахах во время проведения этого измерения, 
так и о возможной переоценке точности способа \textit{C}. С этой целью задачу 
\eqref{MinSumW_Subj}--\eqref{MinSumW_Constr} нужно решать, либо исключая из рассмотрения 
восьмое наблюдение, либо предполагая, что способ \textit{C} менее точен, чем это 
продекларировано в таблице, а значит величины неопределённости всех наблюдений, 
выполненных способом $C$, должны быть откорректированы синхронно, то есть 
\begin{equation}
w_{8} = w_{9}.
\label{W8equalsW9}
\end{equation}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  

\begin{table} [htb]
    \centering
    %\parbox{10cm}{
    \caption{Коэффициенты масштабирования \\ 
      величины неопределённости интервальных измерений\\ 
      для данных из таблицы \ref{TabOutliers}
    %}
    \label{TabOutliersWeights}}

    \begin{tabular}{ccc} 
     \hline 
     \makecell{Номер\\ измерения, \\$i$} & 
     \makecell{Решение задачи\\ \eqref{MinSumW_Subj}--\eqref{MinSumW_Constr},\\$w_i^*$} & 
     \makecell{Решение задачи\\ \eqref{MinSumW_Subj}--\eqref{MinSumW_Constr}, 
     \eqref{W8equalsW9},\\$w_i^*$} \\ 
     \hline 
    1 & 1.000 & 1.000 \\ 
    2 & 1.000 & 1.000 \\ 
    3 & 4.686 & -- \\  
    4 & 1.000 & 1.000 \\ 
    5 & 1.000 & 1.000 \\ 
    6 & 1.000 & 1.000 \\ 
    7 & 1.000 & 1,000 \\ 
    8 & 1.343 & 1.143 \\ 
    9 & 1.000 & 1.143 \\ 
    10 & 1.000 & 1.000 \\ \hline 
    \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  

Результат решения задачи \eqref{MinSumW_Subj} при ограничениях \eqref{MinSumW_Constr} 
и \eqref{W8equalsW9}, приведенный во втором столбце таблицы \ref{TabOutliersWeights}, 
говорит о том, что для совместности задачи исходная величина неопределённости измерений, 
полученных способом \textit{C}, не может иметь значение менее, чем $w_{8}^*\,\epsilon_8 
= w_{9}^*\, \epsilon_9 =1.143\cdot 0.40\approx 0.46$. Такой вывод, конечно, не может 
служить основанием автоматического увеличения ширины интервалов неопределённости восьмого 
и девятого измерений до указанного уровня, а может означать лишь необходимость 
дополнительной проверки точности способа измерений \textit{C.} 
  
Таким образом, исследования, проведённые в отношении задачи построения линейной 
зависимости по данным из таблицы \ref{TabOutliers}  позволяют сформулировать следующие 
гипотезы о причинах несовместности задачи, заслуживающие содержательной проверки: 
\begin{itemize}
\item  
и третье, и восьмое наблюдение являются результатами грубых промахов и должны 
быть исключены из дальнейшего рассмотрения; 
\item  
третье наблюдение является результатом грубого промаха и должно быть удалено 
из набора данных, а способ измерений \textit{C} менее точен, и поэтому величина 
неопределённости всех выполненных им измерений должны быть увеличена. 
\end{itemize}
  
Конечно же, наряду с гипотезами о некорректности данных не стоит забывать о всегда 
имеющейся альтернативной гипотезе о некорректном виде конструируемой модели, хотя 
для выбора иной структуры модели (скажем, квадратичной вместо линейной), как правило, 
нужны довольно весомые основания. 

Отработка этих гипотез даёт шанс конструктивно преодолеть несовместность задачи 
построения зависимости и перейти к задаче построения зависимости с непустым 
информационным множеством, которое может подвергаться дальнейшему содержательному 
анализу. 
\end{example}
  
% Приём варьирования неопределённости + неоднородность измерений = варьирование
% неопределённости индивидуально для наблюдений => 
% Гипотеза <<Выбросы, как измерения с недооцененной неопределённостью>> => поиск нижних 
% границ неопределённости, обеспечивающих совместность. Коэффициент необходимого 
% <<раздутия>> измерения — метрика несовместности. При раздутии измерений возможно добавлять 
% априорные ограничения, выражающие отношения между измерениями или их неопределённостями 
% (например, измерения сделанные одним методом/в одних условиях могут раздуваться синхронно, 
% для особо надёжных измерений раздутие может запрещаться и т.п.) 
% Связь с робастным оцениванием. 
  
% Гипотеза “Выбросы — результат неадекватности модели” => смена структуры модели 
% (квадратичная вместо линейной, кусочно-линейная вместо линейной и т.п.) =>  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   

\subsection[Метод парциальных информационных множеств]%   
           {Метод парциальных \\*  информационных множеств} 
  
%Гипотеза <<Несовместность может быть обусловлена наличием в выборке нескольких 
%совместных внутри, но несовместных между собой подвыборок>> => методом парциальных 
%информационных множеств можно выявлять структуру выборки, ее <<компоненты совместности>>.   

Если выборка несовместна, то из неё всегда можно выделить совместные подвыборки 
(одну или несколько), на основе которых могут быть построены интересующие нас оценки. 
Метод парциальных информационных множеств, например, при оценивании постоянной 
величины, позволяет прямо выявлять структуру выборки и совокупность её совместных 
подвыборок, интервалы неопределённости измерений которых имеют непустое пересечение. 
  
Рассмотрим теперь восстановление линейной зависимости вида $y(x) = \beta_{0}+\beta_{1}x$, 
зависящей от двух параметров $\beta_0$ $\beta_1$ при точных значениях аргумента $x$ 
и интервальной неопределённости измерений функции $y$. Для каждой пары интервалов 
неопределённости измерений строится соответствующее им парциальное информационное 
множество, т.\,е. множество возможных значений параметров, которое является двумерным
выпуклым многоугольником. Далее с помощью стандартных методов пересечения 
многоугольников находим пересечение парциальных информационных множеств. Непустота 
пересечения всех парциальных информационных множеств указывает на совместность всей 
выборки измерений в целом (см. \cite{Kumkov2010, KumkovIgnatenkova,Kumkov2013}). 
В противном случае выборка в целом несовместна и потому для неё имеет смысл провести 
более детальное исследование, например, выделить из неё совместные подвыборки. 
Это можно сделать с помощью несложной модификации процедуры пересечения 
парциальных информационных множеств. 
    
Отметим, что рассмотренный метод (в отличие, например, от метода максимума 
совместности, см. \S\ref{MCMSect}) не позволяет оценивать количественно 
\textit{степень несовместности выборки} или так называемый резерв 
характеристического включения всей выборки (см. \cite{SharysReserve}). 

Кроме того, при восстановлении нелинейных зависимостей или 
зависимостей с вектором параметров размерности более двух, могут возникать 
технические сложности при построении и пересечении многомерных парциальных 
информационных множеств. В частности, в нелинейном случае будет необходимо 
пересекать друг с другом не многоугольники с линейными границами, 
а множества более сложной формы. 
   
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\subsection[Выбросы являются точками излома модели]%
           {Выбросы являются точками излома \\*  кусочно-линейной модели}

Гипотеза <<Выбросы --- точки излома модели>> => детекция точек излома (допустимое 
количество точек излома, значимость излома, адекватность кусочной структуры модели)  
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\subsection{Точки чебышёвского альтернанса} 
  
      
Следуя работе \cite{SShary2018}, изложим простой полуэвристический приём для выявления 
измерений, подозрительных на выбросы, в рамках общей вычислительной схемы метода 
максимума совместности (см. \S\ref{MCMSect}). Этот приём основан на гипотезе о том, 
что <<выбросы --- это измерения, наиболее конфликтующие между собой и с другими 
измерениями>>. В математической формулировке ими могут выступать точки так называемого 
чебышёвского альтернанса. 
  
Метод максимума совместности заключается в нахождении максимума так называемого 
распознающего функционала для информационного множества задачи, и исходный пунктом 
нашей методики является то простое наблюдение, что выражения для распознающих 
функционалов \eqref{UssExpr}--\eqref{TolFuncExpr} имеют весьма специальный вид. 
Окончательное значение получается в них как минимум от значений ряда выражений 
одинаковой структуры (стоящих внутри фигурных скобок 
в \eqref{UssExpr}--\eqref{TolFuncExpr}), которые вычисляются по строкам 
матрицы данных \eqref{EmpInData}: 
\begin{equation*}
\r\mbf{b}_i - \left|\; \m\mbf{b}_i - \sum_{j=1}^n \,\mbf{a}_{ij} x_{j} \,\right| 
\end{equation*}    
или 
\begin{equation*} 
\r\mbf{b}_i + \sum_{j=1}^n \,(\r\mbf{a}_{ij})\,|x_{j}| - 
  \left|\,\m\mbf{b}_i - \sum_{j=1}^n \,(\m\mbf{a}_{ij})\, x_j \,\right|
\end{equation*}
Мы будем называть их \emph{образующими} распознающих функционалов \eqref{UssExpr} и 
\eqref{TolFuncExpr}. Фактически, их значения в точке $x = (x_{1}, x_{2}, \ldots, 
x_{n})^\top$ характеризуют отдельные измерения, давая для каждого из них меру 
совместности (согласования) данных в этом  измерении с вектором параметров 
$x = (x_{1}, x_{2}, \ldots, x_{n})^\top$. 
  
С другой стороны, выбросы --- это измерения, удаление которых резко увеличивает меру 
совместности оставшейся части выборки. Как следствие, приходим к следующей естественной 
идее. В точке максимума распознающего функционала нужно посмотреть на значения его
образующих, соответствующих отдельным измерениям, и если какие-то из этих образующих 
существенно меньше остальных, то они и являются кандидатами на выбросы. 
  
\begin{example} 
\label{ChebExample} 
Рассмотрим восстановление линейной функции одной переменной $y = \beta_{0} + \beta_{1}x$ 
по данным, которые сведены в следующую таблицу  
\begin{equation*} 
\arraycolsep=3mm 
\begin{array}{c|c|c} 
\hline 
\text{No.} & x & y \rule{0mm}{4mm}\\[1mm] 
\hline 
\rule{0mm}{4mm} 
1 & [0.9, 1.1] & [1.0, 2.0] \\[1mm]  
2 & [1.9, 2.1] & [1.5, 2.5] \\[1mm] 
3 & [2.9, 3.1] & [1.0, 2.0] \\[1mm] 
4 & [3.9, 4.1] & [2.5, 3.5] \\[1mm] 
5 & [4.9, 5.1] & [5.0, 6.0] \\[1mm] 
6 & [5.9, 6.1] & [3.5, 4.5] \\[1mm] 
7 & [6.9, 7.1] & [4.0, 5.0] \\[1mm] 
8 & [7.9, 8.1] & [4.5, 5.5] \\[1mm] 
\hline 
\end{array} 
\end{equation*} 
Эти данные изображены на Рис.~\ref{ChebExmpPic}. Брусы неопределённости 
с номерами 1, 2, 4, 6, 7 и 8 представляют собой точки, взятые с прямой 
$y = \tfrac{1}{2}x + 1$ (она показана на рисунке штриховой динией) и раздутые 
до интервалов по каждой переменной. Брусы неопределённости 3 и 5 взяты как 
выбросы, они резко выделяются своим расположением из остальных. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  

\begin{figure}[ht]
\centering\small  
\setlength{\unitlength}{1mm} 
\begin{picture}(80,64) 
\put(0,0){\includegraphics[width=80mm]{pictures/ChebExmpPic.eps}} 
\end{picture} 
\caption{Иллюстрация данных к Примеру~\ref{ChebExample}.} 
\label{ChebExmpPic} 
\end{figure}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
  
Непосредственное применение к выписанным данным метода максимума совместности даёт 
оценкой параметров --- точкой максимума распознающего функционала --- $(\beta_{0}, 
\beta_{1}) = (0.7, 0.7)$. При этом само значение максимума равно $-0.87$, так что 
информационное множество задачи (допусковое множество решений соответствующей ИСЛАУ) 
является пустым. Линейная функция, доставляющая максимум  совместности, есть 
$y = 0.7x + 0.7$, она изображена на том же Рис.~\ref{ChebExmpPic} синей линией. 
   
При этом значения образующих распознающего функционала в точке максимума, 
упорядоченные по возрастанию, таковы 
\begin{equation*} 
\arraycolsep=3mm 
\begin{array}{c|c} 
\hline 
\text{No.} & \begin{tabular}{c}
             значение\\[-1pt] 
             образующей 
             \end{tabular} 
             \rule{0mm}{6mm}\\[3mm] 
\hline 
\rule{0mm}{4mm} 
8 & -0.87 \\ 
3 & -0.87 \\ 
5 & -0.87 \\ 
7 & -0.67 \\ 
6 & -0.47 \\ 
4 & -0.07 \\ 
2 & \phantom{-}0.33 \\
1 & \phantom{-}0.33 \\ 
\hline 
\end{array} 
\end{equation*} 
Хорошо видно, что выбросы --- 3-е и 5-е измерения --- имеют наименьшие значения 
образующих, наряду с 8-м измерением, которое выбросом не является.  
\end{example} 
  
Высказанная выше идея верна по сути и нередко её применение даёт хорошие результаты 
(как в разобранном выше примере). Но всё-таки на пути её успешного применения стоят 
некоторые принципиальные ограничения, которые следует учитывать при интерпретации 
результатов расчётов. 
  
Напомним, что в пределе, когда интервалы неопределённости данных вырождаются в точки 
и мы должны восстанавливать зависимость по точным данным, метод максимума совместности  
(как слабая, так и сильная версии) переходит в чебышёвское сглаживание данных (см. 
обоснование в \cite{SSharyJCT2017, SSharyIzvAN2017, SSharyADSAA}), т.\,е. в их 
приближение в равномерной метрике.                \index{чебышёвское сглаживание}
 
В самом деле, если матрица входных данных $\mbf{X}$ и вектор выходных данных $\mbf{y}$ 
--- точечные (неинтервальные), т.\,е. $\mbf{X} = X = (x_{ij})$ и $\mbf{y} = y = (y_{i})$, 
то для всех $i, j$ 
\begin{equation*}
\r\mbf{y}_{i} = 0, \hspace{18mm} \m\mbf{y}_{i} = y_{i}, \hspace{18mm} \mbf{x}_{ij} = x_{ij}. 
\end{equation*}
Распознающий функционал множества решений принимает при этом вид 
\begin{align*}
\Tol(\beta, X,y) 
&= \min_{1\leq i\leq n}
  \left\{\, -\biggl|\,y_{i} - \sum_{j=1}^m x_{ij}\beta_j \,\biggr|\,\right\} 
 = -\max_{1\leq i\leq n} \  \biggl|\,y_{i} - \sum_{j=1}^m x_{ij}x_j \,\biggr| \\[3mm] 
&= -\max_{1\leq i\leq n} \;\left|\,\bigl(X\beta)_{i} - y_{i}\,\right| 
 = -\left\| X\beta - y\,\right\|_{\infty}. 
\end{align*} 
Здесь посредством $\|\cdot\|_\infty$ обозначается чебышёвская норма ($\infty$-норма) 
вектора в конечномерном пространстве $\mbb{R}^m$, которая определяется как 
$\|z\|_{\infty} = \max_{1\leq i\leq n}\,|z_{i}|$. Тогда 
\begin{equation*}
\max\;\Tol(\beta)\, = \,\max_{\beta\in\mbb{R}^m}\;\bigl(-\| X\beta - y\,\|_{\infty}\bigr)\, 
  = -\min_{\beta\in\mbb{R}^m}\; \| X\beta - y\,\|_{\infty}, 
\end{equation*} 
коль скоро $\,\max\,(-f(\beta)) = -\min\, f(\beta)$. 
Таким образом, максимизация распознающего функционала равносильна в этом случае 
минимизации чебышёвской нормы невязки решения. 
   
Один из основных результатов теории равномерного приближения функций --- это знаменитая 
  
\bigskip\noindent  
\textbf{Теорема Чебышёва об альтернансе} \cite{Bakhvalov,Natanson} \\  
{\sl Для того, чтобы многочлен $n$-ой степени $P(x)$ являлся многочленом наилучшего 
равномерного приближения непрерывной на интервале $[a, b]$ функции $f(x)$, необходимо 
и достаточно, чтобы на $[a, b]$ существовали по крайней мере $(n+2)$ точки $x_{0} 
< x_{1} < \ldots < x_{n} < x_{n+1}$, такие что разность $f(x_{i}) - P(x_{i})$, 
$i = 0, 1, \ldots, n+1$, принимает в них равные по абсолютной величине значения, 
которые последовательно меняют знак от точки к точке.}  
\index{теорема Чебышёва об альтернансе} 
   
\bigskip    
Название теоремы объясняется тем фактом, что точки $x_{0} < x_{1} < \ldots < x_{n} 
< x_{n+1}$, фигурирующие в формулировке, называются точками \emph{чебышёвского 
альтернанса} (от французского слова alternance --- чередование). Если ищется наилучшее 
равномерное приближение линейной функцией, т.\,е. полиномом первой степени $n = 1$, 
то $n+2 = 3$, так что точек альтернанса должно быть не менее трёх штук.
\index{чебышёвский альтернанс} Но нередко их бывает гораздо больше. 
  
Нетрудно понять, что точки альтернанса соответствуют тем измерениям, значения образующих 
для которых --- наименьшие, и из сделанного выше наблюдения следует, что для линейной 
зависимости таких точек не может быть одна или две. Их принципиально не меньше трёх, 
и, вообще говоря, может быть больше. 
  
Что происходит в случае интервальных данных? Вместо точек мы имеем теперь брусы 
неопределённости измерений в пространстве $\mbb{R}^{n+1}$, и к этой ситуации теорема 
Чебышёва становится, строго говоря, неприменимой. Тем не менее, если интервалы 
данных <<не слишком широки>> (или <<достаточно узки>>), то теорема Чебышёва всё-таки 
остаётся верной, и мы можем считать, что количество точек альтернанса остаётся 
равным как минимум $n+2$, т.\,е. $3$ в линейном случае. В частности, в рассмотренном 
выше Примере~\ref{ChebExample} брусами неопределённости данных, образующими 
<<альтернанс>> относительно построенной прямой линии $y = 0.7x + 0.7$ являются, 
как нетрудно увидеть из Рис.~\ref{ChebExmpPic}, 3-й, 5-й и 8-й брусы, т.\,е. всего 
три. Опять таки, в реальных практических ситуациях их может быть довольно много, 
что хорошо демонстрируется при решении конкретных задач. 
  
Таким образом, в методе максимума совместности выбросы, если они имеются, в силу 
принципиальных математических причин могут маскироваться обычными информативными 
измерениями. 
  
Тем не менее, если количество обрабатываемых измерений велико, то любая дополнительная 
информация о выбросах, любая техника, позволяющая сузить <<круг подозреваемых>>, может 
оказаться полезной и имеет смысл быть применённой. Особенно, когда затраты на её 
реализацию пренебрежимо малы, как это имеет место с предложенной выше методикой 
исследования образующих распознающего функционала в точке максимума. 
  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\subsection[Релаксация информационного множества]%  
           {Релаксация ограничений \\*  информационного множества}   

\begin{itemize}
\item 
Tanaka? 
\item  
H. Lahanier, E. Walter, and R. Gomeni. OMNE: A New Robust Membership-Set Estimator for
the Parameters of Nonlinear Models // Journal of Pharmacokinetics and Biopharmaceutics, 
Vol. 15, No. 2, 1987, pp. 203-219. 
\item 
Л.\,Жолен \& Co 2001 ... \cite{ApplInteAnal}  
\item 
B. Kubica 2019 ... \cite{Kubica} 

\end{itemize}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\section{Состоятельность оценок} 
  
  
\emph{Состоятельной оценкой} в вероятностной математической статистике называется 
оценка, которая сходится к значению оцениваемого параметра\index{оценка состоятельная} 
при неограниченном возрастании объёма выборки (см., к~примеру, \cite{ESVentsel, HCramer, 
Lagutin}). Понятие состоятельности играет важную роль в традиционной статистике и 
служит одним из критериев отбора <<хороших>> методов оценивания. В этом параграфе 
мы обсудим попытки распространить его на методы обработки интервальных данных, 
предпринятые в \cite{VoschininSotirov,Kurzhanski}.  
  
% Прежде всего, стоит отметить, что понятие состоятельности имеет асимптотический смысл, 
% так что при работе с выборками конечного объёма его ценность нуждается в критическом 
% дополнительном подтверждении. 
  
Фактически, в понятии состоятельности имеются два аспекта, каждый из которых важен 
и сам по себе: 
\begin{list}{}{\itemsep=0pt\itemindent=12pt\topsep=2pt\parsep=2pt} 
\label{DistAxioms}
\item[(I)] 
приближённые оценки, уточняясь, в конце концов, дают истинное значение интересующей 
нас оцениваемой величины, 
\item[(II)] 
уточнение приближённых оценок выполняется согласно конструкции предельного перехода 
(аналогичной той, что известна из математического анализа). 
\end{list} 
Следствием пункта (II) является, в частности, единственность предела и, как следствие, 
единственность значения интересующей нас величины, что очень привлекательно 
для практиков. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb]
\centering\small 
\unitlength=1mm
\begin{picture}(77,44)
\put(0,0){\includegraphics[width=77mm]{pictures/KurzhConsisPict.eps}}
\end{picture}
\caption{Рисунки из работы \cite{Kurzhanski}, иллюстрирующие} 
<<состоятельность>> интервального оценивания. 
\label{InteConsisPic}
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Но важно осознавать, что понятие состоятельности в своём исходном виде родилось 
при обработке точечных данных, для которых информационное множество, введённое 
в \S\ref{InfoSetSect} и являющееся одним из краеугольных камней анализа существенно 
интервальных данных, почти всегда пусто. Иными словами, традиционная состоятельность 
имеет содержательный смысл лишь для оценок, которые находятся <<по ту сторону 
баррикады>> совместности, т.\,е. для которых совместности с данными нет. Если же 
совместность с данными имеет место и информационное множество непусто, то все точки 
из него полностью соответствуют модели объекта или явления, и все являются 
<<состоятельными>> в смысле согласования с моделью. В этих условиях непонятно, 
зачем ещё нужна состоятельность в смысле вероятностной статистики и что она выражает. 
Если некая оценка совместна с данными (лежит в непустом информационном множестве), 
то это является выражением её наивысшего статуса в рамках рассматриваемой модели, 
и желать чего-то лучшего уже не имеет смысла. 
  
По-видимому, именно это имел в виду Л.В.\,Канторович в своей статье \cite{Kantorovich}, 
где писал: <<Обычно полученную в результате измерений избыточную систему уравнений 
обрабатывают по методу наименьших квадратов Гаусса. При этом происходит значительная 
потеря информации>>. Как нам представляется, <<потеря информации>> в данном отрывке 
--- это отсечение возможных вариантов, которые тоже удовлетворяют модели и реально 
являются решениями задачи, в угоду методу решения, который такой неоднозначности 
и многовариантности не воспринимает. 
  
<<Состоятельность>> оценок, получаемых при обработке интервальных данных, 
рассматривалась в некоторых работах. Например, в \cite{Kurzhanski} обсуждается 
гипотетическая ситуация, в которой с ростом числа интервальных наблюдений множество 
решений стягивается в точку и в пределе мы получаем однозначную оценку. Получается 
красиво! Но здесь, как нам кажется, автор смешивает <<состоятельность>> и 
<<однозначность>>. Математический предел всегда единствен, и потому состоятельная 
оценка тоже всегда одна. Получение в результате решения задачи однозначного ответа 
--- приятное и желаемое обстоятельство, поскольку его потребители (Лицо, Принимающее 
Решения и т.\,п.) ограждается от возможной неопределённости в дальнейшем выборе. 
Но всё-таки не нужно путать свойство однозначности ответа с тем, что необходимую 
оценку мы получаем в пределе. Важно понимать, что при интервальном оценивании все 
решения из непустого информационного множества --- <<состоятельные>> с точки зрения 
согласия с моделью. 
  
Кроме того, единственность оценок по интервальным данным является, как правило,
неустойчивым свойством, которое разрушается при сколь угодно малых возмущениях в данных. 
Именно таковы конфигурации на Рис.~\ref{InteConsisPic}. Мера множества таких конфигураций 
во всём множестве всевозможных ответов --- нулевая, и потому, фактически, равна нулю 
геометрическая вероятность появления таких <<эксклюзивных>> оценок при решении реальных 
практических задач. 
  
% 1) обозреть Эльясберга и его рассуждения и выводы 
% 2) эффективность оценки? нужно ли порассуждать на эту тему? 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\section{Мера вариабельности оценки} 
\label{VariabilitySect}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\subsection{Общие соображения} 
\label{VariabIdeasSect}  
  
Термином <<вариабельность>> мы называем изменчивость и неоднозначность оценки, и 
необходимость введения для неё количественной меры диктуется тем обстоятельством, 
что в задачах обработки интервальных данных ответ часто неединствен. Обычно мы 
получаем целое множество различных оценок, одинаково пригодных в качестве ответов 
к задаче и совместных (согласующихся) с её данными --- информационное множество 
задачи. <<Мера вариабельности>> как раз таки и должна характеризовать то, насколько 
мал\'{о} или обширно это множество.\index{вариабельность} Иногда в подобных ситуациях 
говорят о <<рассеянии>> оценки, но мы оставляем этот термин для обработки данных 
в теоретико-вероятностном контексте, так как он обозначает несколько другой смысл, 
нежели <<вариабельность>>.  
    
В традиционной теоретико-вероятностной статистике оценки тех или иных параметров,
как известно, сами являются случайными величинами, а мерой их рассеяния и 
неоднозначности может служить дисперсия оценки или же её среднеквадратичное 
отклонение (см., к примеру,~\cite{HCramer}). Кроме того, используются также 
средняя абсолютная разность, медианное абсолютное отклонение, среднее абсолютное 
отклонение и др. Все они выражает меру рассеяния значений оценки, понимаемой 
в том или ином конкретном смысле. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb]
\centering\small 
\unitlength=1mm
\begin{picture}(60,50)
\put(0,0){\includegraphics[width=60mm]{pictures/DiamQuestion.eps}}
\put(56,19){$x_1$} \put(19,47){$x_2$} 
\end{picture}
\caption{Мерой вариабельности оценки параметров может}
служить размер информационного множества задачи. 
\label{DiamQuestPic} 
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Что может служить аналогом перечисленных выше величин в интервальном анализе 
данных, который не оперирует вероятностными понятиями и в котором данные не несут 
вероятностных распределений?
  
В отношении вариабельности оценки параметров ответ на этот вопрос кажется достаточно
очевидным: ею может стать любая величина, характеризующая размеры информационного 
множества задачи, если оно непусто (Рис.~\ref{DiamQuestPic}). Например, это может 
быть диаметр информационного множества в той или иной метрике. Можно даже просто 
брать интервальные оценки соответствующих множеств решений интервальных систем 
уравнений, получаемые теми или иными методами интервального анализа (например, 
изложенными в книгах \cite{AlefeldHerzberg, ApplInteAnal, KalmykShokinYuld, 
SSharyBook, MooreBakerCloud} и других). Но полноценные интервальные оценки 
информационного множества имеют ряд недостатков, и некоторые из них хорошо заметны 
в сравнении с упомянутыми выше мерами вариабельности оценок в теоретико-вероятностной 
статистике. 
  
Во-первых, это излишняя детализация ответа, выдаваемого в виде бруса в $\mbb{R}^n$ 
или интервального вектора какого-нибудь другого вида, большое количество информации, 
которую еще необходимо  <<переварить>> и привести к компактной и выразительной форме. 
Требуется относительно несложная и эффективно вычислимая величина, выражаемая одним 
числом, которая давала бы общее агрегированное представление об интересующей нас 
вариабельности. Аналогично дисперсии и её аналогам она может служить <<прикидочной>> 
характеристикой качества оценок при практическом решении различных задач обработки 
интервальных данных. Конкретная форма этой величины, конечно, должна зависеть как 
от задачи, так и (возможно) от метода её решения. 
   
Во-вторых, некоторые интервальные оценки могут быть довольно сложными для нахождения. 
Так, нахождение оптимальных внешних оценок объединённого множества решений для 
интервальных систем линейных алгебраических уравнений является труднорешаемой задачей 
(<<NP-трудной>> в математических терминах). Как следствие, в общем случае использовать 
оптимальные внешние оценки информационного множества для характеризации вариабельности 
будет, по меньшей мере, непрактичным. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[h!tb]
\centering\small 
\unitlength=1mm  
\begin{picture}(84,60)
    \put(0,0){\includegraphics[width=84mm]{pictures/OuterSolEstims.eps}} 
    \put(46,57){\mbox{внешняя оценка}}  
    \put(21,49){\mbox{оптимальная внешняя оценка}}  
    \put(28,42){\mbox{слабая внешняя оценка}}          
\end{picture}
\caption{Различные способы оценивания информационного} 
множества для характеризации вариабельности оценки. 
\label{InfoSetEstims}
\index{слабая внешняя оценка} 
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
В-третьих, из ограничений предшествующего пункта следует, что некоторые интервальные 
оценки могут оказаться не очень пригодными для характеризации вариабельности точечной 
оценки параметра. Например, грубая внешняя оценка информационного множества, которая 
превосходит его размеры в несколько раз (эта ситуация типична для внешнего оценивания 
объединённого множества решений) может хуже давать представление о вариабельности, 
чем слабая внешняя оценка (см. Рис.~\ref{InfoSetEstims}). 
  
\begin{example} 
Предположим, что мы находим параметры линейной зависимости 
\begin{equation*} 
y = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2}
\end{equation*} 
по следующим интервальным данным: 
\begin{center} 
\tabcolsep=3mm
\begin{tabular}{|c||cc|c|}  
\hline 
№ & $x_1$ & $x_2$ & $y$ \\ 
\hline\hline \rule{0mm}{4mm}% 
1 & $[-3, -2]$ &  $[10, 12]$  & $[10, 12]$ \\[3pt] 
2 &  $[4, 5]$  &  $[14, 16]$  & $[18, 22]$ \\[3pt] 
3 &  $[7, 9]$  & $[-18, -15]$ & $[24, 26]$ \\[3pt] 
4 & $[-4, -2]$ &  $[14, 15]$  &  $[6, 7]$% 
\rule[-2mm]{0mm}{4mm} \\ 
\hline 
\end{tabular} 
\end{center}   
Соответствующая интервальная линейная система уравнений, которую нужно решать 
для нахождения параметров $\beta_0$, $\beta_1$, $\beta_2$ --- это \eqref{3DSampleSys}, 
рассмотренная ранее в \S\ref{ApprInfoSetSect}. Пусть уже найдена точечная оценка 
параметров, удовлетворяющая требованиям слабого согласования, т.\,е. точка из 
объединённого множества решений. Нужно найти её вариабельность, или, иными словами, 
размеры этого объединённого множества решений. 
  
Применим для внешнего оценивания информационного множества разбиение на квадратные 
подсистемы и их раздельное решение с помощью интервального метода Гаусса. Решая 
квадратную подсистему из 1-го, 2-го и 3-го уравнений, получим внешней оценкой 
для множества совместных параметров $(\beta_{0}, \beta_{1}, \beta_{2})^\top$ брус 
\begin{equation*} 
\begin{pmatrix} 
[3.3639, 28.276]   \\ 
[-0.55684, 3.1571] \\ 
[-0.56705, 0.4138] 
\end{pmatrix}.  
\end{equation*} 
Решая квадратную подсистему из 2-го, 3-го и 4-го уравнений, получим внешней 
оценкой брус 
\begin{equation*} 
\begin{pmatrix} 
[-2.1066,  20.625] \\ 
[ 1.1328,  2.7338] \\
[-0.19724, 0.40237]\\
\end{pmatrix}.  
\end{equation*} 
Пересекая эти брусы, получим 
\begin{equation} 
\label{IsectRes} 
\begin{pmatrix} 
[3.3639, 20.625] \\ 
[ 1.1328,  2.7338] \\
[-0.19724, 0.40237]\\
\end{pmatrix}.  
\end{equation} 
  
Из Рис.~\ref{TriDSolSetPic} видно, что результат разобранного примера не отличается 
высоким качеством оценки (особенно по компоненте $\beta_0$). К сожалению, это типично 
для большинства <<быстрых>> (полиномиально сложных) численных методов для внешнего 
оценивания объединённого множества решений ИСЛАУ. 
  
Если применить для внешнего оценивания множества решений более точные (но и более 
трудоёмкие) методы, например, методы дробления параметров (PPS-методы, см. 
\cite{SSharyBook}), то и результат получится существенно более точным. Но в общем 
случае пересечение даже оптимальных интервальных оценок множеств решений подсистем 
всё равно может не быть оптимальной внешней оценкой множества решений всей исходной 
системы. 
  
Так, применение PPS-алгоритма к ИСЛАУ, составленной из 1-го, 2-го и 3-го уравнений, 
даёт интервальную оболочку её объединённого множества решений 
\begin{equation*} 
\begin{pmatrix} 
[ 7.4684, 21.181] \\ 
[ 0.4154, 2.3838] \\ 
[-0.2881, 0.3261]
\end{pmatrix}.  
\end{equation*} 
Применение PPS-алгоритма к ИСЛАУ, составленной из 2-го, 3-го и 4-го уравнений, 
даёт интервальную оболочку её объединённого множества решений 
\begin{equation*} 
\begin{pmatrix} 
[ 1.4320, 18.771] \\ 
[ 1.1866, 2.7337] \\ 
[-0.1931, 0.4024] 
\end{pmatrix}.  
\end{equation*} 
В результате их пересечения для внешней оценки множества решений системы 
\eqref{3DSampleSys} получаем несколько более узкий, чем \eqref{IsectRes}, брус 
\begin{equation*} 
\begin{pmatrix} 
[ 7.4684, 18.771] \\
[ 1.1866, 2.3838] \\ 
[-0.1931, 0.3261] 
\end{pmatrix}. 
\end{equation*} 
Но он всё равно не оптимален (это хорошо видно для $\beta_0$ и не так сильно 
выражено для $\beta_1$), несмотря на оптимальность оценок для подсистем. 
\end{example} 
    
Можно предложить следующий эвристический алгоритм вычисления слабой внешней оценки 
$\mbf{V}$ для объединённого множества решений системы $\mbf{A}x = \mbf{b}$. Сначала 
инициализируем $\mbf{V}\gets\varnothing$. На очередном шаге алгоритма берём точечную 
систему уравнений $Ax = b$ из интервальной системы $\mbf{A}x = \mbf{b}$, находим её 
решение $\tilde{x}$. Это решение, строго говоря, может не существовать, но если оно 
всё-таки найдено, то в качестве следующего слабого внешнего приближения берём 
интервальную оболочку точки $\tilde{x}$ и полученного ранее бруса $\mbf{V}$, т.\,е. 
полагаем $\mbf{V}\gets\ih\,\{\mbf{V}, \tilde{x}\}$. Повторяем процесс до тех пор, 
пока не выполнено условие остановки (исчерпались ресурсы и т.\,п.). 
    
Опираясь на свойства объединённого множества решений интервальных линейных систем 
уравнений с кваадратными матрицами, можно значительно усовершенствовать этот простой 
алгоритм. Напомним следующий важный результат. 
    
\addvspace{\bigskipamount}\noindent  
\textbf{Теорема Бекка-Никеля.}\index{теорема Бекка-Никеля} 
{\sl Пусть $\mbf{A}x = \mbf{b}$ --- интервальная система линейных алгебраических 
уравнений с неособенной квадратной матрицей $\mbf{A}\in \mbb{IR}^{n\times n}$. 
Для любого индекса $\nu\in\{1,2,\ldots,n\}$ точные покоординатные оценки точек 
из объединённого множества решений --- экстремальные значения $\,\min\{\,x_{\nu} 
\mid x\in\USS\Ab\,\}$ и $\,\max\{\,x_{\nu} \mid x\in\USS\Ab\,\}$ --- достигаются 
на решениях крайних точечных систем уравнений $\,Ax = b$, т.\,е. таких, что матрица 
$A$ и вектор $b$ образованы концами интервальных элементов из $\mbf{A}$ и $\mbf{b}$ 
соответственно.}  
  
\bigskip    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\begin{table}[!htb]
\centering
\caption{Алгоритм для нахождения слабой внешней оценки\\ 
         объединённого множества решений ИСЛАУ}  
\label{WeakOuterAlgo} 
\vspace{2pt} 
\color{blue} 
\fboxsep=3mm
\fboxrule=0.5pt
\fbox{\color{black} 
\begin{minipage}{88mm}
\centering 
\begin{tabbing}
A\= AAA\= AAA\= AAA\= \hspace{4em}\=\kill
\>\>\>\>\> \hspace{10pt}\textsf{Вход}\\[3mm] 
\> Интервальная система линейных уравнений $\mbf{A}x = \mbf{b}$.          \\[7mm]
\>\>\>\>\> \hspace{7pt}\textsf{Выход}\\[3mm]
\> Слабая внешняя оценка $\mbf{V}$ для объединённого множества            \\ 
\> решений интервальной линейной системы $\mbf{A}x = \mbf{b}$.            \\[7mm]
\>\>\>\>\> \textsf{Алгоритм}\\[1mm] 
\> Инициализируем $\mbf{V}\gets\varnothing$;                              \\[2pt]  
\> \texttt{DO WHILE} ( не выполнено условие остановки )                   \\[2pt] 
\>\> берём крайнюю точечную систему 
                            уравнений $\tilde{A}x = \tilde{b}$\phantom{A} \\ 
\>\>\>  из интервальной системы $\mbf{A}x = \mbf{b}$,                     \\[2pt] 
\>\> находим решение $\tilde{x}$ для системы $\tilde{A}x = \tilde{b}$;    \\[2pt]  
\>\> полагаем $\mbf{V}\gets\ih\,\{\mbf{V}, \tilde{x}\}$;                  \\[2pt]  
\> \texttt{END DO} 
\end{tabbing} 
\end{minipage}} 
\end{table} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
    
Доказательство можно увидеть в книге \cite{SSharyBook}. Как следствие, приходим 
к более совершенному численному методу, представленному в Табл.~\ref{WeakOuterAlgo}. 
Он выбирает для решения только те точечные системы линейных уравнений, которые 
могут внести вклад в формирование наибольших и наименьших значений координат 
точек из объединённого множества решений. 
        
Рассмотрим теперь определение вариабельности оценок, имеющих сильную совместность 
с данными. Внешнее оценивание допускового множества решений ИСЛАУ является, с одной 
стороны, менее разработанной задачей, а с другой стороны --- более простой, чем 
внешнее оценивание объединённого множества решений. К ней можно применить хорошо 
развитые методы линейного программирования. 
  
  
Здесь в интервальной статистике сделано относительно мало. Одной из немногих работ 
на эту тему является \cite{SShary2019}, где предложена мера вариабельности оценки 
максимума совместности в сильном смысле. Мы рассмотрим подробнее развитые в ней 
идеи в \S\ref{SrongCmptVarSect}. 
  
Хотя информационное множество задачи восстановления зависимости не несёт никакой 
дополнительной структуры (вроде вероятностного распределения и т.\,п.), его вдумчивый 
анализ может дать очень много для уяснения свойств решения задачи и его взаимоотношения 
с моделью явления. Нужно обратить внимание на то, 
\begin{itemize} 
\item[\color{red}$\blacklozenge$]  
является ли информационное множество ограниченным \\ 
или неограниченным, 
\item[\color{red}$\blacklozenge$] 
каковы размеры информационного множества, 
\item[\color{red}$\blacklozenge$]  
является ли информационное множество связным или же \\ 
распадается на отдельные <<куски>>, 
\item[\color{red}$\blacklozenge$] 
какую форму имеет информационное множество, и т.\,п. 
\end{itemize} 
Если информационное множество неограниченно, то это, как правило, противоречит 
физическому смыслу задачи. Аналогично трактуется несвязность информационного множества 
решений, которая означает, фактически разрывную зависимость результата от входных 
данных задачи. 
  
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\subsection[Ограниченность информационного множества]% 
           {Ограниченность \\* информационного множества} 
  
Важнейшим этапом анализа вариабельности оценки является установление ограниченности 
или неограниченности информационного множества задачи. Для рассматриваемой нами задачи 
восстановления линейной функциональной зависимости этот анализ существенно зависит 
от того, каким именно взято информационное множество, т.\,е. в каком смысле мы понимаем 
согласование параметров и данных задачи и какая именно оценка ищется. Объединённое и 
допусковое множества решений ИСЛАУ, которые соответствуют слабому и сильному согласованию 
параметров и данных, ведут себя существенно по-разному, и выбор дальнейших действий 
определяется конкретным выбором информационного множества. 
  
Выяснение ограниченности/неограниченности объединённого множества решений можно осуществить 
с помощью некоторых (не всех) численных методов. С другой стороны, для такого исследования  
возможно привлечение ряда теоретических результатов, которые опираются на свойства 
интервальной матрицы системы, построенной по данным. Напомним в этой связи некоторые 
определения и факты из линейной алгебры и теории интервальных матриц. 
  
Набор векторов линейного пространства называется \emph{линейно зависимым}, если 
какой-либо один из них может быть выражен через другие векторы этого набора в виде 
линейной комбинации, не все коэффициенты которой равны нулю.\index{линейная зависимость}  
Если ни один из векторов данного набора не может быть выражен подобным образом через 
другие, то этот набор называется \emph{линейно независимым}.\index{линейная независимость}  
Равносильное определение: конечный набор векторов линейно зависим, если существуют такие 
скаляры, не все из которых равны нулю, что линейная  комбинация векторов набора с этими 
скалярами равна нулевому вектору. 
    
Как известно, квадратная матрица называется \emph{неособенной} (невырожденной, 
регулярной), если её определитель не равен нулю (см. \cite{Gantmacher, Lankaster, 
HornJohn}). Это эквивалентно отсутствию линейной зависимости между строками 
(столбцами) такой матрицы. Иначе матрица называется \emph{особенной} (вырожденной). 
  
Интервальная квадратная матрица $\mbf{A}$ называется \emph{неособенной}, если 
неособенны все точечные матрицы $A\in\mbf{A}$\index{неособенная матрица} 
\cite{SSharyBook, NeumaierBook, RohnHandbook}. Интервальная квадратная матрица $\mbf{A}$ 
называется \emph{особенной}, если она не является неособенной, и это равносильно тому, 
что матрица $\mbf{A}$ содержит хотя бы одну особенную точечную матрицу. Ближайшим 
обобщением неособенных матриц, как точечных, так и интервальных, являются матрицы 
полного ранга. \index{матрица регулярная} 
  
\emph{Рангом матрицы} называется максимальное число её линейно независимых строк или 
столбцов.\index{ранг матрицы} Как показывается в матричном анализе, эти числа совпадают 
между собой и равны максимальному из порядков ненулевых миноров рассматриваемой матрицы  
(см. \cite{Gantmacher, Lankaster,HornJohn}). Вещественная $n\times m$-матрица называется 
\emph{матрицей полного ранга}, если её ранг равен минимальному из чисел $m$ и $n$ 
(б\'{о}льшим он быть не может). \index{матрица полного ранга} 
\index{матрица неполного ранга}
  
\begin{definition}   
Назовём интервальную матрицу \textsl{матрицей полного ранга}, если все содержащиеся в ней 
точечные матрицы имеют полный ранг. Иначе, если некоторые точечные матрицы из заданной 
интервальной матрицы имеют неполный ранг, будем говорить, что интервальная матрица имеет 
\textsl{неполный ранг}. 
\end{definition} 
  
\addvspace{\bigskipamount}\noindent  
\textbf{Теорема об ограниченности множества решений}\\ 
\index{теорема об ограниченности множества решений} 
{\sl Если интервальная $n\times m$-матрица $\mbf{A}$, $n\geq m$, имеет полный ранг, 
то для интервальной системы линейных алгебраических уравнений $\mbf{A}x = \mbf{b}$ 
объединённое множество решений $\USS(\mbf{A}, \mbf{b})$ является ограниченным.}  
  
\bigskip   
Доказательство этого результата можно увидеть в работе \cite{SSharySibJCM}, а также 
(в очень кратком виде) в книге \cite{NeumaierBook}. 
  
Обратное к Теореме утверждение обусловлено дополнительными свойствами матрицы системы 
и множества решений и в общем случае может не выполняться (вопреки анонсированному 
в \cite{Rodionova} результату). Рассмотрим в качестве примера систему линейных уравнений 
\begin{equation}
\label{AntiRodSystem}
\left(
\begin{array}{@{\;}cc@{\;}}
1 & 2\\[3pt]
2 & 4\\[3pt]
3 & 6 
\end{array}
\right)\,
\begin{pmatrix}
x_{1} \\ x_{2}  
\end{pmatrix} 
=  \left(  
\begin{array}{@{\;}c@{\;}}
[10, 11]\\[3pt]
[40, 42]\\[3pt]
[90, 93] 
\end{array}
\right), 
\end{equation} 
в которой интервальность присутствует лишь в правой части. В матрице этой системы 
все строки пропорциональны, так что матрица имеет неполный ранг $1$. Кроме того, 
система \eqref{AntiRodSystem}, очевидно, несовместна, так как три её уравнения задают 
три параллельные полосы в $\mbb{R}^2$, не пересекающиеся друг с другом. Таким образом, 
объединённое множество решений системы пусто, и поэтому ограничено. 
  
В самом общем случае проверка того, имеет ли интервальная матрица полный ранг, 
представляет собой труднорешаемую задачу (NP-трудную в точных математических терминах, 
см. \cite{RohnHandbook}). Тем не менее, существуют простые достаточные условия того, 
что интервальная матрица имеет полный ранг. Сформулируем один из них, основанный 
на понятии сингулярных чисел матрицы.  
  
Напомним, что \textit{сингулярным числом} $\sigma$ вещественной $n\times m$-матрицы 
$A$ называется неотрицательное решение системы уравнений 
\begin{equation*}
\left(
\begin{array}{cc@{\,}}
0 & A^{\top} \\[1ex]
A & 0
\end{array}
\right)
\left(
\begin{array}{@{\,}c@{\,}}
x \\[1ex] y
\end{array}
\right)\ 
= \ 
\sigma\,
\left(
\begin{array}{@{\,}c@{\,}}
x \\[1ex] y
\end{array}
\right), 
\end{equation*}
отвечающее ненулевым векторам $x\in\mbb{R}^m$, $y\in\mbb{R}^n$. Сингулярные числа 
матрицы $A$ можно также определить как арифметические квадратные корни из общих 
собственных чисел матриц $A^{\top}\!A$ и $AA^{\top}$ (см. \cite{GolubVanLoan, 
Watkins, HornJohn}). Таким образом, сингулярные числа $n\times m$-матрицы --- это 
набор из $\min\{m, n\}$ неотрицательных чисел. Мы будем обозначать посредством 
$\sigma_{\min}(A)$ и $\sigma_{\max}(A)$ наименьшее и наибольшее из сингулярных 
чисел матрицы $A$.                                  \index{сингулярные числа}  
  
Нахождение сингулярных чисел матриц хорошо разработано в современном численном анализе 
(см., к примеру, \cite{GolubVanLoan}). Для их вычисления созданы надежные алгоритмы, 
а реализующие их готовые подпрограммы входят в стандартные пакеты численных методов 
линейной алгебры и системы компьютерной математики. 
      
\addvspace{\bigskipamount}\noindent
\textbf{Признак полноранговости интервальной матрицы}  \cite{SSharySibJCM} 
\index{признак полноранговости}  \\[2pt] 
{\sl Если для интервальной матрицы $\mbf{A}$ выполняется неравенство 
\begin{equation*}
\sigma_{\max} (\r\mbf{A}) \;< \;\sigma_{\min} (\m\mbf{A}),
\end{equation*} 
т.\,е. максимальное сингулярное число матрицы радиусов для $\mbf{A}$ меньше 
минимального сингулярного числа её матрицы середин, то $\mbf{A}$ имеет полный ранг.}
  
\bigskip 
Доказательство этого результата можно увидеть в работе \cite{SSharySibJCM}, и там же 
даются некоторые другие достаточные признаки полноранговости интервальных матриц. 
При их выполнении объединённое множество решений ИСЛАУ, т.\,е. множество оценок 
параметров, слабо согласующихся с интервальными данными задачи, является ограниченным. 
  
Допусковое множество решений ИСЛАУ, соответствующее сильному согласованию параметров 
и интервальных данных в задаче восстановления линейной зависимости, ведёт себя 
существенно иначе. Основным результатом на эту тему является 
  
\addvspace{\bigskipamount}\noindent\index{критерий И.А.\,Шарой}%  
\textbf{Критерий неограниченности И.А.\,Шарой}  \\ 
{\sl Непустое допусковое множество решений интервальной системы линейных алгебраических 
уравнений неограниченно тогда и только тогда, когда в матрице системы есть линейно 
зависимые точечные (неинтервальные) столбцы.}  
  
\bigskip 
Доказательство и комментарии к применению можно найти в работах \cite{IreneJCT2004, 
IreneRC2005} или в книге \cite{SSharyBook}. 
  
Критерий И.А.\,Шарой показывает, что, в действительности, допусковое множество решений 
неограниченно при довольно специфических условиях, которые реализуются лишь в случае 
точного измерения входных переменных. Во всех остальных ситуациях допусковое множество 
решений, как правило, ограниченно, что означает конечную вариабельность оценок, 
требующих сильного согласования параметров и данных. Этим сильное согласование 
принципиально отличается от слабого.  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb] 
\centering\small 
\unitlength=1mm 
\begin{picture}(80,32) 
\put(0,0){\includegraphics[width=80mm]{pictures/BoxConstraint.eps}} 
\end{picture}
\caption{Ограничения на угловой коэффициент прямой} 
(штриховые линии) при сильной совместности.  
\label{BoxConstrPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
  
Естественно возникает вопрос, почему свойства сильной совместности столь сильно 
отличаются от свойств слабой совместности? Почему допусковое множество решений 
даже одного интервального линейного уравнения с несколькими переменными всё-таки 
ограниченно? 
     
Наглядно на него можно ответить следующим образом. Требование сильной совместности 
неявным образом добавляет условия, которые ограничивают свободу выбора линии, проходящей 
через брус неопределённости измерения зависимости. В слабой совместности этого нет. 
В самом деле, условие прохождения линии через боковые грани бруса неопределённости 
налагает ограничения на угловой коэффициент прямой (см. Рис.~\ref{BoxConstrPic}, левый 
чертёж), а через него и на свободный член уравнения прямой линии. Ничего этого нет, 
если мы требуем, чтобы прямая <<просто пересекала>> брус неопределённости, как это 
заложено в понятии слабой совместности (см. Рис.~\ref{BoxConstrPic}, правый чертёж). 
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\subsection[Вариабельность оценки максимума совместности]% 
           {Мера вариабельности\\*  для оценки максимума совместности} 
\label{SrongCmptVarSect}   
  
Итак, мерой вариабельности оценки параметров в задачах анализа данных с интервальной 
неопределённостью являются размеры информационного множества задачи. Нахождение значения 
этой меры для полученных оценок параметров может рассматриваться как отдельная задача, 
но для некоторых методов восстановления зависимостей по интервальным данным искомую 
информацию можно получить непосредственно в результате их работы. Таковы, в частности 
методы исчерпывания информационного множества из \S\ref{ExhaInfoSetSect}, в которых 
строится его оценка в виде бруса или семейства брусов. Этим полезным свойством обладают 
и другие методы, в частности, метод максимума совместности (\S\ref{MCMSect}). 
  
Предположим, что, решая задачу восстановления линейной зависимости по интервальным 
данным \eqref{EmpInData} с помощью метода максимума совместности, мы нашли безусловный 
максимум для распознающего функционала Uni или Tol. Развивая идеи работы \cite{SShary2019}, 
для характеризации вариабельности оценки вектора параметров $\hat{\beta} = (\hat{\beta}_{1}, 
\hat{\beta}_{2}, \ldots, \hat{\beta}_{m})$ в линейной функции \eqref{LinFunc}, которая 
получена с помощью метода максимума совместности,\index{мера вариабельности} мы предлагаем 
величину 
\begin{equation}
\label{WEV}
\mbox{WEV}(\mbf{X}, \mbf{y})\, = \,\sqrt{m}\;\max_{\mbb{R}^n}\,\Uni 
   \cdot \Bigl(\,\max_{X\in\mbf{X}}\,\cond_{2} X\,\Bigr) \cdot 
   \frac{\displaystyle\bigl\|\arg\max_{\mbb{R}^n}\,
   \Uni\!\bigr\|_{2}}{\|\hat{\mbf{y}}\|_2} 
\end{equation} 
в случае слабой совместности, и величину 
\begin{equation}
\label{SEV}
\mbox{SEV}(\mbf{X}, \mbf{y})\, = \,\sqrt{m}\;\max_{\mbb{R}^n}\,\Tol 
   \cdot \Bigl(\,\min_{X\in\mbf{X}}\,\cond_{2} X\,\Bigr) \cdot 
   \frac{\displaystyle\bigl\|\arg\max_{\mbb{R}^n}\,
   \Tol\!\bigr\|_{2}}{\|\hat{\mbf{y}}\|_2} 
\end{equation} 
в случае сильной совместности. Величины, входящие в формулы \eqref{WEV} и \eqref{SEV}, 
имеют следующий смысл: 
\begin{description} 
\item 
$m$ --- размерность вектора параметров восстанавливаемой линейной функции \eqref{LinFunc}; 
\item 
$\|\cdot\|_2$ --- евклидова норма (2-норма) векторов в $\mbb{R}^m$, определяемая как 
\begin{equation*} 
\|x\|_2 \; = \; \sqrt{\;\sum_{i=1}^m |x_{i}|^{2}\;}; 
\end{equation*} 
\item 
$\cond_{2}X$ --- спектральное число обусловленности матрицы $X$, определяемое как 
\begin{equation*} 
\cond_{2}\,X\;  = \; \frac{\sigma_{\max}(X)}{\sigma_{\min}(X)}, 
\end{equation*} 
т.\,е. как отношение наибольшего $\sigma_{\max}(X)$ к наименьшему $\sigma_{\min}(X)$ 
сингулярных чисел матрицы; оно является расширением на прямоугольный случай хорошо 
известного понятия числа обусловленности квадратной матрицы, которое широко используется 
в вычислительной линейной алгебре (см., к примеру, \cite{GolubVanLoan,Watkins}); 
\item 
$\hat{\mbf{y}}$ --- некоторая <<наиболее представительная>> точка из интервального 
вектора $\mbf{y}$, которая берётся как 
\begin{equation} 
\label{hatyFormula}
\hat{\mbf{y}}\; = \;\tfrac{1}{2}(|\m\mbf{y} + \r\mbf{y}| + |\m\mbf{y} - \r\mbf{y}|), 
\end{equation} 
где операции <<mid>> and <<rad>> применяются покомпонентно. 
\end{description} 
   
Аббревиатура WEV является сокращением английской фразы <<\un{w}eak \un{e}stimate 
\un{v}ariability>>, а SEV --- <<\un{s}trong \un{e}stimate \un{v}ariability>>. 
Обоснование для SEV приводится в работе \cite{SShary2019}, а для WEV оно может 
быть получено совершенно аналогично, используя представление \eqref{UnionRepres} 
вместо \eqref{IsectRepres}. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb] 
\centering\small 
\unitlength=1mm 
\begin{picture}(80,44) 
\put(0,1){\includegraphics[width=80mm]{pictures/TolGraph1D.eps}} 
\put(40,6){\normalsize$\varXi_\mathit{tol}$} 
\put(20,40){\normalsize Tol} 
\end{picture}
\caption{Максимальное значение распознающего функционала} 
{\;даёт} представление о размерах допускового множества решений 
\label{MaxRecFuncPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Выписанная выше формула для $\hat{\mbf{y}}$ выглядит внушительно, но имеет ясный 
смысл. Она специально сконструирована таким образом, чтобы учесть возможную близость 
вектора-бруса $\mbf{y}$ к началу координат, когда его средняя точка бруса уже не может 
считаться <<наиболее представительной>>. Если вектор середин $\m\mbf{y}$ покомпонентно 
превосходит вектор радиусов $\r\mbf{y}$, то формула \eqref{hatyFormula} превращается, 
как нетрудно видеть, в $\hat{\mbf{y}}\, = \,\m\mbf{y}$.   
  
Несмотря на то, что мы работаем с определённой формулой \eqref{hatyFormula} для 
$\hat{\mbf{y}}$, стоит отметить, что взятие этой точки является, по большому счёту, 
делом здравого смысла. Общий подход к определению $\hat{\mbf{b}}$ состоит в том, что 
это должна быть <<наиболее представительная>> точка из вектора правых частей $\mbf{b}$, 
и в некоторых ситуациях её выбор может быть отличным от \eqref{hatyFormula}. К~примеру, 
$\hat{\mbf{b}}$ может быть точечным результатом измерения (её базовым значением), вокруг 
которого далее строится интервал неопределённости измерения, основываясь на информации 
о погрешности измерительного прибора. 
  
Наряду с \eqref{WEV} и \eqref{SEV} в качестве меры относительной вариабельности оценки 
параметров можно рассмотреть величины  
\begin{equation} 
\label{RelVarEst} 
m\,\Bigl(\,\max_{X\in\mbf{X}}\,\cond_{2} X\,\Bigr)
   \,\frac{\max_{\,\mbb{R}^m}\,\Tol}{\|\hat{\mbf{b}}\|_2} 
\quad\text{ и }\quad 
m\,\Bigl(\,\min_{X\in\mbf{X}}\,\cond_{2} X\,\Bigr)   
   \,\frac{\max_{\,\mbb{R}^m}\,\Tol}{\|\hat{\mbf{b}}\|_2}, 
\end{equation} 
которые также могут оказаться полезны. Как WEV, SEV, так и величины \eqref{RelVarEst} 
определены для интервальных линейных систем $\mbf{X}\beta = \mbf{y}$ с ненулевой правой 
частью. Они могут принимать как положительные, так и отрицательные значения или же быть 
бесконечными. Последнее происходит в единственном случае $\,\cond_{2}\calX = \infty$, 
когда все точечные матрицы $A\in\vert\mbf{A}$ имеют неполный ранг, т.\,е. когда  
$\sigma_{\min}(A) = 0$ для каждой $A\in\vert\mbf{A}$. Тогда мера вариабельности 
полагается равной бесконечности. 
  
Не отягощая изложение большими математическими выкладками,  можно наглядно показать, 
что величина SEV адекватно характеризует размеры непустого допускового множества 
решений для широкого класса практически важных задач, имея ясный и наглядный смысл. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[!htb]
\centering\small 
\unitlength=1mm
\begin{picture}(110,36)
\put(0,0){\includegraphics[width=110mm]{pictures/TolSteepFlat.eps}}
\put(28,6){\normalsize$\TSS$} 
\put(90,6){\normalsize$\TSS$} 
\end{picture}
\caption{Помимо \ максимума \ распознающего \ функционала} 
размеры множества решений определяются крутизной графика 
\label{SteepFlatPic} 
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb] 
\centering\small 
\unitlength=1mm 
\begin{picture}(80,44) 
\put(0,0){\includegraphics[width=80mm]{pictures/ScaledTolGraph.eps}} 
\put(42,4){\normalsize$\TSS$} 
\end{picture}
\caption{Размеры множества решений пропорциональны} 
максимальному значению распознающего функционала 
\label{ScaledSizePic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Допусковое множество решений интервальной системы линейных алгебраических уравнений 
является множеством нулевого уровня распознающего функционала Tol (см. детали в книге 
\cite{SSharyBook} или работе \cite{SShary2012}). Другими словами, это пересечение 
подграфика функционала Tol с координатной плоскостью $\Tol = 0$ (см. иллюстрацию на 
Рис.~\ref{MaxRecFuncPic}). Как следствие, величина максимума распознающего функционала 
может при прочих равных условиях служить мерой того, насколько велико или мало 
допусковое множество решений. Чем больше $\max\,\Tol$, тем больше размеры множества 
решений, и наоборот. Дополнительные факторы, которые составляют <<прочие равные 
условия>>,  --- это наклон (крутизна) кусков гиперплоскостей, из которых составлен 
полиэдральный график распознающего функционала Tol (в одномерном случае, который 
изображён на Рис.~\ref{MaxRecFuncPic} и Рис.~\ref{SteepFlatPic}, это прямые линии). 
Наклон гиперплоскостей определяется коэффициентами задающих их уравнений, которые 
равны концам интервалов данных \eqref{EmpInData}. Величина этого наклона (крутизны) 
может быть обобщённо охарактеризована в терминах числа обусловленности точечных 
матриц, образованных из концов интервалов данных.  Наконец, множитель 
\begin{equation*}
\frac{\|\arg\max\,\Tol\|_{2}}{\|\hat{\mbf{b}}\|_2} \ 
   = \  \frac{\|\hat{x}\|_2}{\|\hat{\mbf{b}}\|_2} 
\end{equation*}
является масштабирующим коэффициентом, который помогает обеспечить соразмерность 
окончательного значения вариабельности и величины решения задачи, т.\,е. вектора 
$\arg\max\,\Tol$ и вектора правой части системы линейных уравнений, построенной 
по интервальным данным задачи. Таким образом получается формула \eqref{SEV}. 
  
На практике вычисление величин WEV и SEV затрудняется необходимостью вычисления 
максимума или минимума чисел обусловленности всех точечных матриц, содержащихся 
в данной интервальной матрице. Если эта интервальная матрица <<не слишком широка>>, 
то можно приближённо считать 
\begin{equation*} 
\max_{X\in\mbf{X}}\,\cond_{2} X \approx \cond_{2}(\m\!\mbf{X}),  
  \qquad \min_{X\in\mbf{X}}\,\cond_{2} X \approx \cond_{2}(\m\!\mbf{X}). 
\end{equation*} 
Для широких интервальных матриц этот рецепт уже не годится. 
  
Рассмотрим другую конструкцию для меры вариабельности SEV, которая позволяет обойти 
эти трудности. Определим  
\begin{equation*}
\mbox{SEV}(\mbf{X}, \mbf{y})\, = \,\sqrt{m}\;\max_{\mbb{R}^n}\,\Tol 
   \cdot\,\cond_{2}\calX \,\cdot \frac{\displaystyle\bigl\|\arg\max_{\mbb{R}^n}\, 
   \Tol\!\bigr\|_{2}}{\|\hat{\mbf{y}}\|_2},  
\end{equation*} 
где все величины, входящие в правую часть формулы, имеют прежний смысл, а  $\calX$ --- 
специальная матрица, называемая \textit{матрицей поконцевых комбинаций} для интервальной 
матрицы $\mbf{X}$, имеющая размер $N\times m$, где $N\leq n\cdot 2^m$, $n$ --- количество 
измерений, и составленная из комбинаций концов интервальных элементов вдоль каждой 
строки матрицы данных $\mbf{X}$. 
  
  
\begin{example} 
Рассмотрим интервальную линейную систему уравнений \eqref{Sample2DSys},  
\begin{equation*} 
\left( 
\begin{array}{@{\,}cc@{\,}} 
1 & [0, 1] \\[3pt]
1 & [1, 2] \\[3pt]
1 & [2, 3] 
\end{array} 
\right)
\begin{pmatrix}
\beta_{0} \\[3pt] \beta_{1} 
\end{pmatrix}
= 
\left(
\begin{array}{@{\,}c@{\,}}
[4, 10] \\[3pt]  
[8, 11] \\[3pt]  
[6, 14]
\end{array}
\right), 
\end{equation*} 
разобранную в Примере~4.7.1. Здесь $n = 3$, матрица поконцевых комбинаций есть 
$6\times 2$-матрица 
\begin{equation*} 
\calX \  = \ 
\begin{pmatrix}
1 & 0 \\[2pt] 1 & 1 \\[2pt] 1 & 1 \\[2pt] 1 & 2 \\[2pt] 1 & 2 \\[2pt] 1 & 3 
\end{pmatrix}, 
\end{equation*} 
в которой три $2\times 2$-блока соответствуют наборам комбинаций по отдельным строкам 
исходной матрицы. Используя любую систему компьютерной математики (Scilab, Matlab, Octave 
и пр.) можно найти её обусловленность в спектральной норме: 
\begin{equation*} 
\cond_{2}\calX = 4.1085. 
\end{equation*} 
С помощью этих же программных систем нетрудно запустить соответствующую версию программы 
\texttt{tolsolvty} и найти максимум распознающего функционала Tol и точку, в которой 
он  достигается: 
\begin{equation*} 
\max_{\mbb{R}^2}\,\Tol = 1, \hspace{17mm} 
   \arg\max_{\mbb{R}^2} \,\Tol = (8.0318, 0.96824)^{\top}. 
\end{equation*} 
Отметим, что приведённый выше результат получен при расчёте программой \texttt{tolsolvty} 
с допусками на целевую функцию, норму суперградиента и точность по аргументу, равными 
$10^{-9}$. С другими значениями этих параметров (в частности, взятыми по умолчанию 
$10^{-6}$) могут получиться немного другие результаты. 
  
Наконец, так как вектор правой части отделён от нуля, то в данном случае 
\begin{equation*} 
\hat{\mbf{y}} = \m\mbf{y} = (7, 9.5, 10)^\top. 
\end{equation*} 
В целом, 
\begin{equation*} 
\SEV\  = \  \sqrt{m}\;\max_{\mbb{R}^m}\,\Tol \cdot 
   \,\cond_{2}\,\calX\, \cdot \frac{\displaystyle 
   \bigl\|\,\arg\max_{\mbb{R}^m} \,\Tol\bigr\|_{2}}{\|\hat{\mbf{y}}\|_2} = 3.0389. 
\end{equation*} 
    
Допусковое множество решений системы уравнений \eqref{Sample2DSys} изображено 
на Рис.~\ref{SampleIDataPic}, и интервальная оболочка этого множества решений 
--- это прямоугольник $([5, 10], [-1, 3])$. Видно, что диаметр допускового 
множества решений совпадает с диагональю этого прямоугольника, т.\,е. равен 
\begin{equation*} 
\sqrt{(10 - 5)^2 + (3 - (-1))^2} = 6.4031. 
\end{equation*} 
Как видим, половина этого значения (<<радиус>> информационного множества) неплохо 
согласуется со значением SEV. 
\end{example} 
   
Нулевое значение SEV получается при $\max_{\mbb{R}^m}\,\Tol = 0$, и размеры 
информационного множества задачи при этом считаются нулевыми. Конкретные примеры 
показывают, что в некоторых случаях это может быть неверным, а допусковое множество 
решений может иметь конечные размеры. С другой стороны, ситуация $\max_{\mbb{R}^m}\,
\Tol = 0$ является неустойчивой и разрушается при сколь угодно малых возмущениях 
задачи, когда множество решений может стать пустым. Как следствие, нужно признать, 
что при этом нулевое значение SEV в целом адекватно характеризует размеры множества 
решений. 
    
Рассмотрим пример неустойчивого допускового множества решений, которое изменяется 
скачкообразно при малых возмущениях системы уравнений. 
   
\begin{example} 
Для всех интервальных $2\times 2$-систем линейных уравнений вида 
\begin{equation} 
\label{UnstableISys} 
\begin{pmatrix}
[-1, 1] & [-1, 1] \\[2pt] 
   1    &   -1    \\[2pt] 
\end{pmatrix}
\begin{pmatrix}
x_{1} \\[2pt] x_{2}
\end{pmatrix}
=
\begin{pmatrix}
{[-1, 1]} \\[2pt]
{[1, 1 + \eta]} 
\end{pmatrix}, \qquad \eta \geq 0,  
\end{equation} 
допусковое множество решений является одинаковым для всех неотрицательных чисел 
$\eta$: это отрезок прямой линии, соединяющий точки $(0,-1)$ и $(1,0)$. Оно изображено 
на  Рис.~\ref{Exmp0Pic}. Диаметр множества решений является существенно ненулевым (более 
точно, $\sqrt{2}$), но безусловный максимум распознающего функционала Tol для всех 
рассматриваемых систем равен нулю. Он достигается в точке $(0.5,-0.5)$, средней точке 
множества решений. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[!htb]
\centering\small
\unitlength=1mm
\begin{picture}(70,46)
\put(0,0){\includegraphics[width=70mm]{pictures/DegenSolSet.eps}} 
\put(28,12){\normalsize$\varXi_\mathit{tol}$} 
\end{picture}
\caption{Допусковое множество решений} 
\;интервальной системы уравнений \eqref{UnstableISys}. 
\label{Exmp0Pic} 
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
В то же время, если $\eta > 0$, то любое сколь угодно малое увеличение нижнего конца 
интервала $[1,1+\eta]$ в правой части второго уравнения делает допусковое множество 
решений пустым. Любое сколь угодно малое уменьшение верхнего конца интервала $[-1, 1]$ 
из первой компоненты вектора правой части приводит к тому же эффекту. 
\end{example} 
  
В рассмотренном примере мы построили матрицу поконцевых комбинаций $\calX$ полностью.  
Количество всевозможных комбинаций концов интервальных элементов в каждой строке исходной 
матрицы было невелико --- всего лишь $2$, и потому матрица $\calX$ имеет размер всего 
лишь $6\times 2$. Что делать, если в каждой строке матрицы имеется значительное число 
существенно интервальных элементов, так что величина $n\cdot 2^m$ является большой? 
Это может произойти, к примеру, в моделях множественной (многофакторной) регрессии, 
когда исследуются функциональные зависимости со значительным числом переменных.  
  
Естественный выход из затруднения состоит в том, что вместо полной матрицы $\calX$ нужно 
брать редуцированную матрицу поконцевых комбинаций $\tilde{\calX}$, в которой присутствуют 
не все возможные комбинации концов интервальных элементов, а лишь достаточный их набор, 
который позволяет адекватно оценить допусковое множество решений. Дело в том, что среди 
всевозможных комбинаций концов интервалов присутствуют такие, которые несущественны, 
не являются <<активными>> ограничениями для множества решений, а потому их можно 
не учитывать. 
  
Для решения вопроса о том, какие концы интервальных элементов из матрицы $\mbf{X}$ 
следует оставить для ограничения допускового множества решений, вспомним характеризацию  
\eqref{TSScharact} для его точек: 
\begin{equation*} 
\beta\in\TSS(\mbf{X}, \mbf{y}) 
   \qquad \Longleftrightarrow\qquad 
   \mbf{X}\cdot\beta\;\subseteq\;\mbf{y}.  
\end{equation*}  
Предположим, что известна принадлежность вектора $\beta$ какому-то определённому 
ортанту пространства $\mbb{R}^n$, т.\,е. известны знаки компонент вектора $\beta$, 
то произведение $\mbf{X}\cdot\beta$ можно переписать более определённым образом. 
В силу правила \eqref{NumIntProduct} компоненты вектора $\mbf{X}\cdot\beta$ имеют вид 
\begin{equation} 
\label{XBetaComp} 
(\mbf{X}\beta)_{i} \  = \  
   \Biggl[\;\sum_{j=1}^n x'_{ij}\beta_{j}, \sum_{j=1}^n x''_{ij}\beta_{j} \Biggr], 
\end{equation} 
где $x'_{ij}$, $x''_{ij}$ --- концы интервалов $\mbf{x}_{ij}$, определяемые по знакам 
$\beta_j$ на основе правила \eqref{NumIntProduct}: 
\begin{equation*} 
x'_{ij} = \left\{ 
\begin{array}{ll}  
\un{\mbf{x}}_{ij}, & \text{если} \  \beta_{j}\geq 0, \\[1mm] 
\ov{\mbf{x}}_{ij}, & \text{если} \  \beta_{j}\leq 0, 
\end{array} 
\right. 
\hspace{12mm} 
x''_{ij} = \left\{ 
\begin{array}{ll}  
\ov{\mbf{x}}_{ij}, & \text{если} \  \beta_{j}\geq 0, \\[1mm] 
\un{\mbf{x}}_{ij}, & \text{если} \  \beta_{j}\leq 0. 
\end{array} 
\right. 
\end{equation*} 
В этих условиях включение  $\mbf{X}\cdot\beta\;\subseteq\;\mbf{y}$ равносильно 
системе $2m$ штук линейных неравенств между концами интервалов $(\mbf{X}\beta)_i$ 
и $\mbf{b}_i$: 
\begin{equation*} 
\left\{ 
\begin{array}{l}
\sum_{j=1}^n x'_{ij}\beta_{j}  \geq \un{\mbf{y}}_{i}, \\[3mm] 
\sum_{j=1}^n x''_{ij}\beta_{j} \leq \un{\mbf{y}}_{i}, \\[3mm] 
\qquad  i = 1,2,\ldots,m. 
\end{array} 
\right. 
\end{equation*} 
  
С другой стороны, с учётом представления \eqref{XBetaComp} эта система может быть 
переписана в следующем виде 
\begin{equation*} 
\left\{ 
\begin{array}{l}
\sum_{j=1}^n x'_{ij}\beta_{j}  \in\mbf{y}_{i}, \\[3mm] 
\sum_{j=1}^n x''_{ij}\beta_{j} \in\mbf{y}_{i}, \\[3mm] 
\qquad  i = 1,2,\ldots,m, 
\end{array} 
\right. 
\end{equation*} 
т.\,е. как интервальная система линейных алгебраический уравнений 
\begin{equation*} 
\left( 
\begin{array}{@{\;}c@{\;}} 
(\,x'_{ij}) \\[2mm] 
(\,x''_{ij}) 
\end{array}
\right) \beta = 
\left( 
\begin{array}{@{\;}c@{\;}} 
\mbf{y} \\[2mm] \mbf{y} 
\end{array} 
\right)
\end{equation*} 
с $2m\times n$-матрицей, образованной из матриц $(\,x'_{ij})$ и $(\,x''_{ij})$, 
которые поставлены друг на друга, и $2m$-вектором в правой части, образованном 
таким же способом из векторов $\mbf{y}$. Так как информация о принадлежность решения 
конкретному ортанту получается в процессе применения метода максимума совместности, 
когда мы находим оценку $\hat{\beta} = \arg\max \Tol$, то из сказанного следует способ 
образования редуцированной матрицы поконцевых комбинаций $\tilde{\calX}$. 
  
Определённая ущербность наших рассуждений состоит в том, что множество решений может 
не лежать целиком в одном ортанте. 
    
Например, можно порекомендовать брать в качестве матрицы $\tilde{\calX}$ 
матрицу, составленную из матриц нижних концов, матриц верхних концов и какого-то 
числа матриц различных комбинаций концов 
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\section{Восстановление зависимостей \\* по ненакрывающим выборкам} 
\label{NCovSampleProcSect} 
  
  
Все предшествующие результаты о построении зависимостей по интервальным данным исходили 
из предположения, что выборка брусов неопределённости измерений --- накрывающая или 
хотя бы частично накрывающая (искажённая лишь выбросами). Что делать, если у нас 
нет никакой информации о том, что интервальные результаты измерений являются 
накрывающими, или даже более того, что эти результаты, скорее, ненакрываюшие? 
    
Помимо имеющейся выборки $\eus{S}_{n} = \{(\mbf{x}_{i}, \mbf{y}_{i})\}_{i=1}^n$ 
другой информации о зависимости у нас нет, и строить функциональную зависимость нужно,  
отталкиваясь только от $\eus{S}_n$. Если включения истинных значений независимых и 
зависимых переменных в брусы неопределённости измерений $(\mbf{x}_{i}, \mbf{y}_{i})$ 
не выполнено, то остаётся единственный критерий, по которому можно определять насколько 
хороша или плоха восстанавливаемая зависимость --- это её расстояние до этих брусов, 
воспринимаемых как какие-то целостные объекты.\index{ненакрывающая выборка} Примерно 
в таком же духе рассматриваются интервалы данных и брусы неопределённости измерений 
в <<символьном анализе данных>> Л.\,Биллард и Э.\,Дидея \cite{BillardDiday}, 
с единственным отличием, что основой символьного анализа данных являются методы 
традиционной вероятностной статистики, а не методы интервального анализа. 
  
%
%  Всякую ненакрывающую выборку с помощью подходящего увеличения неопределённости 
%  можно превратить в накрывающую, и этим приёмом в самом деле можно иногда 
%  пользоваться на практике. Но один из главных принципов обработки данных состоит 
%  в том, что "с данными нужно обращаться бережно". Поэтому ниже мы обсудим, что 
%  делать с ненакрывающей выборкой как таковой, без её существенного изменения 
%  коррекции. 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb]
\centering\small 
\unitlength=1mm
\begin{picture}(70,50)
\put(0,0){\includegraphics[width=77mm]{pictures/NCovSampleProc.eps}}
\end{picture}
\caption{Построение зависимости по ненакрывающей интервальной выборке.}
\label{NCovSampleProcPic} 
\end{figure}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
Рассмотрим простейший случай описанной выше ситуации, когда независимые переменные 
задаются точно, а брусы неопределённости превращаются в отрезки неопределённости 
(см.~Рис~\ref{NCovSampleProcPic}), так что выборка имеет вид $\eus{S}_{n} = \{(x_{i}, 
\mbf{y}_{i})\}_{i=1}^n$. При значениях аргументов $x_i$ искомая функция принимает 
значения $f(x_{i}, \beta)$, $i = 1,2,\ldots,n$ (на Рис~\ref{NCovSampleProcPic} 
они обозначены крестиками), и тогда соответствующие <<остатки>> $e_i$ интервальных 
измерений равны расстоянию от этих $f(x_{i},\beta)$ до интервалов $\mbf{y}_i$, т.\,е. 
\begin{equation*} 
e_i = \dist\bigl(f(x_{i},\beta), \mbf{y}_{i}\bigr),  \qquad  i = 1,2,\ldots,n. 
\end{equation*} 
Согласно формуле \eqref{IntvalDist} эти величины равны 
\begin{equation*} 
e_{i}(\beta)\; = \;\max\,\bigl\{\bigl|(f(x_{i},\beta) - \un{\mbf{y}}_{i}\bigr|, 
   \bigl|(f(x_{i},\beta) - \ov{\mbf{y}}_{i}\bigr|\bigr\},  \qquad i = 1,2,\ldots,n. 
\end{equation*} 
Для отыскания параметра $\beta$, при котором функция $y = f(x,\beta)$ наилучшим образом 
приближает интервальные данные естественно взять тогда такое $\hat{\beta}$, на котором 
достигается минимум какой-либо нормы вектора остатков $e(\beta) = \bigl(e_{i}
(\beta)\bigr)_{i=1}^n$, т.\,е.  
\begin{equation} 
\label{NCovParamEst} 
\hat{\beta} \, := \,\arg\min_{\beta} \|e(\beta)\|. 
\end{equation} 

%  Чаусова Елена says: метод прямого интервального сглаживания 
%                      простое интервальное приближение (ПИП) 
  
В работе \cite{ZvyaginSShary} показывается, что в случае восстановления линейной 
зависимости, т.\,е. когда 
\begin{equation*}
f(x,\beta) = \beta_{0} + \beta_{1}x_{1} + \ldots + \beta_{m}x_{m},      
\end{equation*}
для любого выбора нормы $\|\cdot\|$ в пространстве векторов $\mbb{R}^n$ функция 
$\|e(\beta)\|$ является выпуклой по $\beta$. Как следствие, мы можем применить 
для решения задачи \eqref{NCovParamEst} развитые численные методы выпуклой оптимизации. 
  
В частности, если в качестве нормы берётся 1-норма или чебышёвская норма 
(максимум-норма) векторов, то функция $\|e(\beta)\|$ является полиэдральной, 
т.\,е. её график составлен из кусков гиперплоскостей. 
  
Нетрудно сообразить, что описанный выше метод удовлетворяет принципу соответствия, 
сформулированному в \S\ref{CorresPrincpSect}. При стягивании ширины интервалов 
неопределённости к нулю мы получим в пределе метод восстановления зависимости 
по точечным данным, минимизирующий какую-то норму вектора остатков наблюдений. 
В частности, если в качестве нормы $\|\cdot\|$ взята евклидова норма (2-норма) 
то в пределе получим классический метод наименьших квадратов (МНК). 
\index{метод наименьших квадратов} 
  
В целом мы будем называть предложенный выше метод аббревиатурой <<ПИА>> --- 
\emph{простая интервальная аппроксимация}.\index{простая интервальная аппроксимация} 
  
\begin{example}   
Рассмотрим восстановление линейной зависимости вида 
\begin{equation*} 
y = \beta_{1}x_{1} + \beta_{0} 
\end{equation*} 
по данным 
\begin{equation} 
\label{TestSample} 
\arraycolsep=3mm 
\begin{array}{c||c|c|c}
x  &    1    &   2    &  3 \\
\hline 
y  &  [1, 2.5] & [2, 3] & [1.5, 2]   
\rule{0mm}{4mm}
\end{array}\;. 
\end{equation} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{figure}[htb] 
\centering\small  
\unitlength=1mm
\begin{picture}(80,60) 
\put(0,0){\includegraphics[width=80mm]{pictures/NCovGraphs.eps}}  
\end{picture} 
\caption{Восстановление\, линейной\, зависимости} 
по ненакрывающей интервальной выборке \eqref{TestSample}.  
\label{NCovGraphsPic} 
\end{figure} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
Информационное множество задачи непусто. 
С помощью метода максимума совместности получается зависимость 
\begin{equation} 
\label{MaxComptFunc} 
y = -0.25\,x + 2.625, 
\end{equation} 
изображённая на Рис.~\ref{NCovGraphsPic} зелёной линией и проходящая через все 
интервалы неопределённости. Но метод ПИА с чебышёвской нормой в качестве наилучшей, 
с его точки зрения, выдаёт зависимость 
\begin{equation} 
\label{SIAFunc} 
y = 0.25\,x + 1.625, 
\end{equation} 
которая резко отличается от \eqref{MaxComptFunc} (она изображена красной линией 
на Рис.~\ref{NCovGraphsPic}), и задающие её параметры не лежат в информационном 
множестве.  
\end{example} 
  
Различие в результатах методов максимума совместности и простой интервальной 
аппроксимации кажется разительным и непонятным. Особенно шокирует факт 
грубого игнорирования построенной прямой \eqref{SIAFunc} коридора совместных 
зависимостей для задачи с данными \eqref{TestSample}. 
  
В действительности, всё это вполне объясняется тем принципиальным фактом, 
что при простой аппроксимации интервальных данных мы совершенно игнорируем 
содержательный смыслов интервалов, которые могут быть вместилищами для истинных 
значений, как это было для накрывающих выборок. Теперь это просто какие-то брусы, 
<<болванки>> (или что-то аналогичное), без какого-либо дополнительного смысла, 
между которыми нам наилучшим образом нужно провести график восстанавливаемой 
зависимости, и никаких других данных для решения задачи у нас нет. 
  
Более того, метод простой интервальной аппроксимации с чебышёвской метрикой 
в приведённом выше примере честно построил прямую линию наилучшего приближения, 
которая отстоит от каждого из отрезков неопределённости на расстоянии $0.875$, и 
это наименьшая возможная величина в данном случае (выполнены условия чебышёвского 
альтернанса). Но цена отказа от свойства накрытия выборки оказывается очень 
чувствительной. 
  
Рассмотренные эффекты характерны для задач с непустым информационным множеством. 
Если же интервальные данные таковы, что информационное множество задачи пусто, 
то различие результатов метода ПИА и методов, опирающихся на накрывающие 
интервальные данные, делается несущественным или вовсе исчезает. 
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\newpage
  
\chapter*{Обозначения}
\addcontentsline{toc}{chapter}{Обозначения}
\markboth{Обозначения}{Обозначения}
  
  
{\tabcolsep=4pt 
\begin{longtable}[c]{ll}
$\Rightarrow$ & логическая импликация                              \\[3pt]
$\Leftrightarrow$ & логическая равносильность                      \\[3pt]
$\&$      &   логическая конъюнкция, связка <<и>>                  \\[3pt]
$\rightarrow$ &  отображение множеств; предельный переход          \\[3pt] 
$\leftarrow$  &  оператор присваивания в алгоритмах                \\[8pt] 
%%  
$\varnothing$ & пустое множество                                   \\[3pt]
$x\in X$  &   элемент $x$ принадлежит множеству $X$                \\[3pt]
$x\not\in X$& элемент $x$ не принадлежит множеству $X$             \\[3pt]
$X\cup Y$   & объединение множеств $X$ и $Y$                       \\[3pt]
$X\cap Y$   & пересечение множеств $X$ и $Y$                       \\[3pt]
$X\setminus Y$ & разность множеств $X$ и $Y$                       \\[3pt]
$X\subseteq Y$ & множество $X$ есть подмножество множества $Y$     \\[3pt]
$X\subset Y$   & $X$ есть собственное подмножество множества $Y$   \\[3pt] 
$X\times Y$ & прямое декартово произведение множеств $X$ и $Y$     \\[8pt]
%%  
$\mbb{R}$  &   множество вещественных (действительных) чисел       \\[3pt]
$\mbb{IR}$ &   классическая интервальная арифметика                \\[3pt]
$\mbb{KR}$ &   полная интервальная арифметика Каухера              \\[3pt]
$\mbb{R}^n$ &  множество вещественных $n$-мерных векторов          \\[3pt]
$\mbb{IR}^n$, $\mbb{KR}^n$ 
             & множества $n$-мерных интервальных векторов          \\[3pt]
$\mbb{R}^{m\times n}$ & множество вещественных $m\times n$-матриц  \\[3pt]
$\mbb{IR}^{m\times n}$, $\mbb{KR}^{m\times n}$   
                       & множества интервальных $m\times n$-матриц \\[8pt]
%%  
$:=     $     &  равенство по определению                          \\[3pt]
$\approx$     &  приблизительно равно                              \\[3pt]
$[a,b]$       & интервал с нижним концом $a$ и верхним $b$         \\[3pt]
$]a,b[$       & открытый интервал с концами $a$ и $b$              \\[3pt]
$\un{\mbf{a}}$, $\,\inf\mbf{a}$ & левый конец интервала $\mbf{a}$  \\[3pt]
$\ov{\mbf{a}}$, $\,\sup\mbf{a}$ & правый конец интервала $\mbf{a}$ \\[3pt]
$\m\mbf{a}$  & середина интервала $\mbf{a}$                        \\[3pt]
$\w\mbf{a}$  & ширина интервала $\mbf{a}$                          \\[3pt] 
$\r\mbf{a}$  & радиус интервала $\mbf{a}$                          \\[3pt] 
$\rer\mbf{a}$ & относительный радиус интервала $\mbf{a}$           \\[3pt] 
$|\mbf{a}|$  & абсолютное значение (модуль) интервала $\mbf{a}$    \\[3pt] 
$\langle\mbf{a}\rangle$  & мигнитуда интервала $\mbf{a}$           \\[3pt] 
$\ih X$    & интервальная оболочка множества $X\subseteq\mbb{R}^n$ \\[3pt] 
$\wedge$   & операция минимума по включению                        \\[3pt] 
$\vee$     & операция максимума по включению                       \\[3pt] 
$\dist$    & расстояние (метрика) на множестве интервалов          \\[3pt] 
$\Dist$    & векторнозначное расстояние (мультиметрика)            \\[8pt] 
%% 
$\dom f$   & область определения функции $f$                       \\[3pt]
$\ran(f, X)$  & область значений функции $f$ на множестве $X$      \\[3pt]
$\min$, $\max$  & операции взятия минимума и максимума             \\[3pt] 
$\sum$          & символ суммы нескольких слагаемых                \\[3pt] 
$\|\cdot\|$     & векторная или матричная норма                    \\[3pt] 
$\sigma_{\min}(X)$ & минимальное сингулярное число матрицы $X$     \\[3pt]      
$\sigma_{\max}(X)$ & максимальное сингулярное число матрицы $X$    \\[8pt]  
%%
$x^\ast$        & истинное значение измеряемой величины            \\[3pt]  
$\mathring{x}$  & базовое измеренное значение величины             \\[3pt]  
$\varOmega$   &   информационное множество задачи                  \\[3pt] 
$\varXi$      &   множество решений интервального уравнения        \\[3pt] 
$\varPi$      &   прогнозный коридор                               \\[3pt] 
$\varUpsilon$ &   коридор совместных зависимостей 
\end{longtable}
}
  
%  
%Если $x$ --- вектор, то его подвектор, состоящий из компонент $x_k$
%с индексами $k$ из некоторого индексного подмножества $K$ обозначается
%через $x_K$, а дополнительный к нему вектор --- через $x_{\not\in K}$
%или $x_{\ne k}$, если $K = \{k\}$. Аналогичных соглашений будем
%придерживаться и в отношении матриц, так что, к примеру, если $A$
%является $m\times n$-матрицей, то $A_{:,\neq k}$ --- это матрица размера
%$m\times(n-1)$, полученная из $A$ удалением $k$-го столбца.
  
Интервалы и другие интервальные величины (векторы, матрицы и др.) всюду
в тексте обозначаются жирным математическим шрифтом, например, $\mbf{A}$,
$\mbf{B}$, $\mbf{C}$, \ldots, $\mbf{x}$, $\mbf{y}$, $\mbf{z}$, тогда как
неинтервальные (точечные) величины никак специально не выделяются.
Арифметические операции с интервальными величинами --- это операции
классической интервальной арифметики $\mbb{IR}$ или же полной интервальной 
арифметики Каухера $\mbb{KR}$. 
  
Если не оговорено противное, под векторами (точечными или интервальными) 
всюду понимаются вектор-столбцы. 
  
Значительная часть описываемых в книге алгоритмов снабжается псевдокодами
на неформальном алгоритмическом языке, основные конструкции и ключевые слова 
которого должны быть понятны читателю из начального курса программирования. 
В частности, операторные скобки
%\begin{description}
%\itemsep 0pt
\begin{list}{}{\itemsep 2pt\topsep 7pt\leftmargin 20pt}
\item
\texttt{DO FOR}\, \ldots \,\texttt{END DO}\; означают оператор цикла
со счётчиком,\\*  \phantom{a}\quad  который задаётся после \texttt{FOR},
\item
\texttt{DO WHILE}\, \ldots \,\texttt{END DO}\; означают оператор цикла
с предусловием,\\*  \phantom{a}\quad  стоящим после \texttt{WHILE},
\item
\texttt{IF}\, \ldots \,\texttt{THEN}\, \ldots \,\texttt{END IF}\; или 
\;\texttt{IF}\, \ldots \,\texttt{THEN}\, \ldots \,\texttt{ELSE}\, \ldots 
\,\texttt{END IF}\\*  \phantom{a}\quad  означают  условные операторы 
с условием, стоящим после \texttt{IF}.
\end{list}
%\end{description}
  
В циклах <<\texttt{DO FOR}>> ключевое слово <<\texttt{TO}>> означает увеличение 
счётчика итераций от начального значения до конечного (положительный шаг), 
а ключевое слово <<\texttt{DOWNTO}>> --- уменьшение счётчика итераций 
(отрицательный шаг). По умолчанию значения счётчика изменяется на единицу. 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   
\begin{thebibliography}{999}
\small\parskip=0mm 
  
\bibitem{AlefeldHerzberg} 
\textsc{Алефельд Г., Херцбергер Ю.} Введение в интервальные вычисления. 
-- Москва: Мир, 1987. 
  
\bibitem{Alimov1980}  
\textsc{Алимов Ю.И.} Альтернатива методу математической статистики. 
-- Москва: Знание, 1980. 
  
\bibitem{AlimovKravtsov} 
\textsc{Алимов Ю.И., Кравцов Ю.А.} Является ли вероятность <<нормальной>> физической 
величиной? // Успехи физических наук. -- 1992. -- Т.~162, №7. -- С.~149--182. 
  
\bibitem{Akhiezer}   
\textsc{Ахиезер Н.И.} Лекции по теории аппроксимации. 2-е изд. -- Москва: Наука, 1965. 
  
\bibitem{AschDavBook} 
\textsc{Ащепков Л.Т., Давыдов Д.В.} Универсальные решения интервальных задач оптимизации 
и управления. -- Москва: Наука, 2006. 
  
\bibitem{IntApplication2021}
\textsc{Баженов А.Н.} Естественнонаучные и технические применения интервального 
анализа: учебное пособие. -- Санкт-Петербургский политехнический университет 
Петра Великого -- Санкт-Петербург, 2021. -- 83~с. \  Доступно 
на \url{https://elib.spbstu.ru/dl/5/tr/2021/tr21-169.pdf} 
  
\bibitem{PTE2021} 
\textsc{Баженов А.Н., Коваль А.Н., Толстяков С.Ю., Мухин Е.Е., Дмитриев А.М., 
Самсонов Д.С.} Стенд для термовакуумных механических испытаний // Приборы и техника 
эксперимента. -- 2021. -- Вып.~1. -- С.~151--152. 
  
\bibitem{Jaccard2022}
\textsc{Баженов А.Н., Тельнова А.Ю.}
Обобщение коэффициента Жаккара для анализа данных с интервальной неопределённостью //
Измерительная техника. -- 2022. №~12, стр-?
  
\bibitem{BazaraShetty}
\textsc{Базара М., Шетти К.} Нелинейное программирование. Теория и алгоритмы. 
-- Москва: Мир, 1982. 
  
\bibitem{Bakhvalov} 
\textsc{Бахвалов Н.С., Жидков Н.П., Кобельков Г.Н.} Численные методы. -- Москва: 
Бином-Лаборатория базовых знаний, 2003 и другие издания книги. 
  
\bibitem{DiscrMath} 
\textsc{Белоусов А.И., Ткачев С.Б.} Дискретная математика. --  Москва: Изд-во МГТУ 
им.~Н.Э.\,Баумана, 2004. 744~с. 
  
\bibitem{Berger} 
\textsc{Берже М.} Геометрия. Т.~1--2. -- Москва: Мир, 1984. 
  
\bibitem{Biokhimiya} 
Биохимия. Учебник для вузов / Т.Л.\,Алейникова, Л.В.\,Авдеева, Л.Е.\,Андрианова и др.; 
Под ред. Е.С.\,Северина. -- Москва: ГЭОТАР-МЕД, 2004. 
  
\bibitem{Bohr1928} 
\textsc{Бор Н.} Квантовый постулат и новое развитие атомистики // Успехи физических наук. 
-- 1928. -- Т.~8. -- С.~306--337. 
  
\bibitem{MultivalMaps} 
\textsc{Борисович Ю.Г., Гельман Б.Д., Мышкис А.Д., Обуховский В.В.} Введение в теорию 
многозначных отображений и дифференциальных включений. -- Москва: URSS, 2010, 2016. 224 с.  
  
\bibitem{Bronstejn}  
\textsc{Бронштейн Е.М.} Об одной возможной вероятностной интерпретации интервальной 
величины // Вычислительные технологии. -- 2014. -- Т.~19, №5. -- С.~12--14. 
  
\bibitem{Vapnik} 
\textsc{Вапник В.Н.} Восстановление зависимостей по эмпирическим данным. -- Москва: 
Наука, 1979. 
    
\bibitem{ESVentsel}  
\textsc{Вентцель Е.С.} Теория вероятностей. 6-е изд. — Москва: Высшая школа, 1999. 
  
\bibitem{Vorontsova} 
\textsc{Воронцова Е.А.} Линейная задача о допусках для интервальной модели межотраслевого 
баланса // Вычислительные технологии. -- 2017. -- Т.~22, №2. -- С.~67--84. 
  
\bibitem{VoschininSotirov}
\textsc{Вощинин А.П., Сотиров Г.Р.} Оптимизация в условиях неопределенности. 
-- Москва: Изд-во МЭИ; София: Техника, 1989. -- 224~с. 
  
\bibitem{VoschiBochkovSotirov}
\textsc{Вощинин А.П., Бочков А.Ф., Сотиров Г.Р.} Метод анализа данных при интервальной 
нестатистической ошибке // Заводская лаборатория. -- 1990. -- Т.~56, №7. -- С.~76--81.
  
\bibitem{Voschinin2002}
\textsc{Вощинин А.П.} Интервальный анализ данных: развитие и перспективы // Заводская 
лаборатория. -- 2002. -- Т.~68, №1. -- С.~118--126. 
  
\bibitem{VoschininIMRO} 
\textsc{Вощинин А.П.} Задачи анализа с неопределёнными данными --- интервальность 
и/или случайность?  // Труды Международной конференции по вычислительной математике. 
Рабочие совещания. Редакторы Ю.И.\,Шокин, А.М.\,Федотов, С.П.\,Ковалев, Ю.И.\,Молородов, 
А.Л.\,Семенов, С.П.\,Шарый. Совещание <<Интервальная математика и методы распространения 
ограничений>> ИМРО-2004. -- Издательство ИВМиМГ СО РАН: Новосибирск, 2004. 
-- С. 147--58.   Электронная версия доступна на  
\url{http://www.nsc.ru/interval/Conferences/IMRO_04/Voschinin.pdf} 
  
\bibitem{Gaganov} 
\textsc{Гаганов А.А.} О сложности вычисления интервала значений полинома от многих 
переменных // Кибернетика. -- 1985. -- №4. -- С.~6--8. 
  
\bibitem{Gantmacher} 
\textsc{Гантмахер Ф.Р.} Лекции по аналитической механике. 3-е изд. 
-- М.: Физматлит, 2002. 
%Википедия. Теорема\_Лиувилля\_o\_сохранении\_фазового\_объёма.\\ 
%https://ru.wikipedia.org/wiki/Теорема\_Лиувилля\_o\_сохранении\_фазового\_объёма. 
%\textsc{Яковенко Г.Н.} Лекции по теоретической механике. Устойчивость, колебания, 
%гамильтонова механика. МФТИ. Учебное издание. Москва. 2003. 188 стр. 
  
\bibitem{Gnedenko} 
\textsc{Гнеденко Б.В.} Курс теории вероятностей. -- Москва: URSS, 2019, 
а также другие издания книги. 
  
\bibitem{GolubVanLoan} 
\textsc{Голуб Дж., ван Лоан Ч.} Матричные вычисления. -- Москва: Мир, 1996, 2013. 
  
\bibitem{GorbanBook} 
\textsc{Горбань И.И.} Феномен статистической устойчивости. -- Киев: Наукова думка, 2014. 
  
\bibitem{GorbanPaper}
\textsc{Горбань И.И.} Статистическая устойчивость --- математическая закономерность или 
физический феномен? // Математичні машини і системи. -- 2017. -- №3. -- С.~102--110.  
  
\bibitem{GUM} 
ГОСТ 34100.3–2017/ISO/IEC Guide 98-3:2008.  неопределённость измерения . Часть 3. 
Руководство по выражению неопределённости измерения (ISO/IEC Guide 98-3:2008, ЮТ). 
Межгосударственный стандарт. -- Москва: Стандартинформ, 2017. 
  
\bibitem{JCGM102RU} 
ГОСТ 34100.3.2-2017/ISO/IEC Guide 98-3:2008. неопределённость измерения. Часть 3. 
Руководство по выражению неопределённости измерения. Дополнение 2. Обобщение 
на случай произвольного числа выходных величин (ISO/IEC Guide 98-3:2008, ЮТ). 
Идентичен Guide 98-3/Suppl 2:2011. Межгосударственный стандарт. Издание 
официальное. -- Москва: Стандартинформ, 2017. 
  
\bibitem{GOSTDirect} 
ГОСТ Р 8.736-2011 Государственная система обеспечения единства измерений (ГСИ). 
Измерения прямые многократные. Методы обработки результатов измерений. 
Основные положения. -- Москва: Стандартинформ, 2019.  
  
\bibitem{GareyJohnson} 
\textsc{Гэри М., Джонсон Д.} Вычислительные машины и труднорешаемые задачи. 
-- Москва: Мир, 1982. 
  
\bibitem{deGroot} 
\textsc{де Гроот М.} Оптимальные статистические решения. -- Москва: Мир, 1974. 
  
\bibitem{DemidenkoBook} 
\textsc{Демиденко Е.З.} Оптимизация и регрессия. -- Москва: Наука, 1989. 
  
\bibitem{DemidenkoNote} 
\textsc{Демиденко Е.З.} Комментарий II к статье А.П.~Вощинина, А.Ф.~Бочкова 
и Г.Р.~Сотирова <<Метод анализа данных при интервальной нестатистической ошибке>> // 
Заводская лаборатория. -- 1990. -- Т.~56, № 7. -- С.~83--84. 
  
\bibitem{DemidMaronShuval} 
\textsc{Демидович Б.П., Марон И.А., Шувалова Э.З.} Численные методы анализа. -- 
Москва: Наука, 1961, 1963, 1967. Санкт-Петербург: <<Лань>>, 2010, 2021. 
  
\bibitem{Dobronets} 
\textsc{Добронец Б.С., Попова О.А.} Вычислительный вероятностный анализ: модели 
и методы. -- Лань: Санкт-Петербург, 2020. 
  
\bibitem{DraperSmith} 
\textsc{Дрейпер Н., Смит Г.} Прикладной регрессионный анализ. -- Москва: Вильямс, 2016, 
а также более ранние издания -- В 2-х кн. Финансы и статистика: Москва, 1986, 1987.  
  
\bibitem{Dyvak} 
\textsc{Дывак Н.П.} Задачи математического моделирования статических систем 
с интервальными данными. -- Тернополь: Экономична думка, ТНЭУ, 2011. 
(на украинском языке) 
  
\bibitem{DuboisPrade}
\textsc{Дюбуа Д., Прад А.} Теория возможностей. Приложения к представлению знаний  
в информатике. -- Москва: Радио и связь, 1990. 
  
\bibitem{EvtuPosypRyTu} 
\textsc{Евтушенко Ю.Г., Посыпкин М.А., Рыбак Л.А., Туркин А.В.} Численный метод 
аппроксимации множества решений системы нелинейных неравенств // International Journal 
of Open Information Technologies. -- 2016. -- Vol.~4, No.~12. -- P.~1--6. 
  
\bibitem{EliseevaYuzbashev} 
\textsc{Елисеева И.И., Юзбашев М.М.} Общая теория статистики. -- Москва: Финансы 
и статитстика, 2015, а также более раниие издания. 
  
\bibitem{OskorbinKazakhs}  
\textsc{Ергалиев Е.К., Мадияров М.Н., Оскорбин Н.М., Смолякова Л.Л.} Согласование 
базы данных в прикладном интервальном анализе // Известия Алтайского госуд. 
университета. -- 2022. -- № 1 (123). -- С.~89--94.  \ 
% DOI: 10.14258/izvasu(2022)1-14. 
  
\bibitem{PhysChem99}  
\textsc{Еремин В.В., Каргов С.И., Кузьменко Н.Е.} Задачи по физической химии. 
Часть 2. Химическая кинетика. Электрохими. Методическая разработка для студентов 
химических и биологических факультетов университетов. Под общей редакцией акад. 
проф.~В.В.\,Лунина. -- Москва, 1999. 
% http://www.chem.msu.su/rus/teaching/eremin/7.html  
  
\bibitem{IIEremin} 
\textsc{Еремин И.И.} Противоречивые модели оптимального планирования. -- Москва: Наука, 
1988. 
  
\bibitem{ErmakovZhiglyavski} 
\textsc{Ермаков С.М., Жиглявский А.А.} Математическая теория оптимального эксперимента. 
-- Москва: Наука, 1987. 
  
\bibitem{Zhilin2003}  
\textsc{Жилин С.И.} Эксперименты по оцениванию параметров эмпирической зависимости 
методом наименьших квадратов и методом центра неопределённости  //  Известия Алтайского 
государственного университета. -- 2003. -- №1. -- С.~24--27. 
  
\bibitem{ZhilinDiss}  
\textsc{Жилин С.И.} Нестатистические методы и модели построения и анализа зависимостей. 
-- Барнаул, 2004. -- Диссертация на соискание учёной степени канд. физ.-мат. наук 
по специальности 05.13.01 <<системный анализ, управление и обработка информации>>. 
Доступна на \url{http://www.nsc.ru/interval/Library/ApplDiss/Zhilin.pdf} 
  
\bibitem{ApplInteAnal}
\textsc{Жолен Л., Кифер М., Дидри О., Вальтер Э.} Прикладной интервальный анализ. 
-- Москва; Ижевск: Изд-во <<РХД>>, 2007. 467~с.
  
\bibitem{Zadeh}
\textsc{Заде Л.} Понятие лингвистической переменной и его применение к принятию 
приближённых решений. -- Москва: Мир, 1976. 
  
\bibitem{Zaidel} 
\textsc{Зайдель А.Н.} Элементарные оценки ошибок измерений. -- Ленинград: <<Наука>>, 
Ленинградское отделение, 1968.  
  
\bibitem{ZvyaginSShary} 
\textsc{Звягин М.А., Шарый С.П.} Об одном подходе к восстановлению зависимостей 
по ненакрывающим интервальным данным // Вычислительные Технологии. -- в печати.  
  
\bibitem{ZeldovichMyshkis} 
\textsc{Зельдович Я.Б., Мышкис А.Д.} Элементы прикладной математики. 3-е изд. 
-- М.: Наука, 1972. 592 с. 
  
\bibitem{ZukhovitskiyAvdeeva}
\textsc{Зуховицкий С.И., Авдеева Л.И.} Линейное и выпуклое программирование. 
-- М.: Наука, 1967. 
  
\bibitem{Zykov}
\textsc{Зыков~А.\,А.} Основы теории графов. М.: Вузовская книга, 2004.
  
\bibitem{InteWebSite} 
\textsc{Интервальный анализ и его приложения} --- тематический веб-сайт 
\url{http://www.nsc.ru/interval/}
  
\bibitem{KalmykShokinYuld} 
\textsc{Калмыков С.А., Шокин Ю.И., Юлдашев З.Х.} Методы интервального анализа. 
-- Новосибирск: Наука, 1986. 
  
\bibitem{KantorSpivak} 
\textsc{Кантор О.Г., Спивак С.И., Талипова Р.Р.} Параметрическая идентификация 
математических моделей химической кинетики // Системы и средства информатики. -- 2017. 
-- Т.~27, №3. -- С.~145--154. 
  
\bibitem{Kantorovich}
\textsc{Канторович Л.В.} О некоторых новых подходах к вычислительным методам и обработке 
наблюдений // Сибирский Математический Журнал. -- 1962. -- Т. 3, №5. -- С.~701--709. 
  
\bibitem{CorrPrincBook} 
\textsc{Кедров Б.М. и др.} Принцип соответствия. Историко-методологический анализ. 
-- Москва: Наука, 1979. 
  
\bibitem{Kleene}
\textsc{Клини С.К.} Математическая логика. -- Москва: Мир, 1973. 
  
\bibitem{UFN1997}  
\textsc{Козлов Ю.В., Мартемьянов В.П., Мухин К.Н.} Проблема массы нейтрино 
в современной нейтринной физике // Успехи Физических Наук. -- 1997. -- Т.~167. 
-- С.~849--885. 
  
\bibitem{Kolmogorov} 
\textsc{Колмогоров А.Н.} Основные понятия теории вероятностей. -- 2-е изд. -- Москва: 
Наука, 1974. 
   
\bibitem{HCramer} 
\textsc{Крамер Г.} Математические методы статистики. -- Москва: Мир, 1975.   
  
\bibitem{Christofides} 
\textsc{Кристофидес Н.} Теория графов. Алгоритмический подход. Перевод с англ. 
-- Москва: Мир, 1978. 
  
\bibitem{ANKrylov} 
\textsc{Крылов А.Н.} Лекции о приближённых вычислениях. -- Москва: ГИТТЛ, 1954. 
  
\bibitem{Kumkov2010}  
\textsc{Кумков С.И.} Обработка экспериментальных данных ионной проводимости 
расплавленного электролита методами интервального анализа // Расплавы. -- 2010. 
-- №3. -- C.~79--89. 
  
\bibitem{KumkovIgnatenkova} 
\textsc{Кумков С.И., Игнатенкова Л.А.} Интервальный подход к оцениванию стабильности 
характеристик стандартного образца // Вычислительные технологии. -- 2017. -- Том 22, 
№ 2. -- С.~85--98. 
  
\bibitem{Kurzhanski} 
\textsc{Куржанский А.Б.} Задача идентификации --- теория гарантированных оценок // 
Автоматика и Телемеханика. -- 1991. -- №1. -- С.~3--26. 
  
\bibitem{Lagutin} 
\textsc{Лагутин М.Б.} Наглядная математическая статистика. -- Москва: Бином. 
Лаборатория знаний, 2011. 
  
\bibitem{Laser}
\textsc{Лазер KLM-532-100.} Технические характеристики. \  URL: 
\url{http://www.fti-optronic.com/Zelenye-lazery-532-nm/Zelenyyi-DPSS-lazer-KLM-532-x.html} 
  
\bibitem{Lankaster}
\textsc{Ланкастер П.} Теория матриц. --- Москва: Наука, 1982. 
  
\bibitem{LemeshkoPostovalov}  
\textsc{Лемешко Б.Ю., Постовалов С.Н.} 
О решении задач статистического анализа интервальных наблюдений // Вычислительные 
Технологии. -- 1997. -- Т.~2. №1. -- С.~28–36. 
  
\bibitem{Linnik}
\textsc{Линник Ю.В.} Метод наименьших квадратов и основы теории обработки 
наблюдений. 2-е изд. -- Москва: ГИФМЛ, 1962. 
  
\bibitem{Maistrov}  
\textsc{Майстров Л.Е.} Развитие понятия вероятности. -- Москва: Наука, 1980.   
  
\bibitem{MaksimovOskorbin}
\textsc{Максимов А.В., Оскорбин Н.М.}
Многопользовательские информационные системы: основы теории и методы исследования. 
-- 2-е изд. испр. и доп. -- Барнаул : Изд-во Алт. ун-та, 2013. -- 264 с.
  
\bibitem{Malikov} 
\textsc{Маликов М.Ф.} Основы метрологии. Часть первая. Учение об измерении. 
-- Комитет по делам мер и измерительных приборов при Совете Министров СССР: 
Москва, 1949. -- 479 с. 
  
\bibitem{vonMises}  
\textsc{Мизес Р.} Вероятность и статистика. -- Москва: URSS, 2009. 
  
\bibitem{MostellerTukey}   
\textsc{Мостеллер Ф., Тьюки Дж.} Анализ данных и регрессия. -- Москва: Финансы 
и статистика, 1982. 
  
\bibitem{MetrolVocab} 
Международный словарь по метрологии: основные и общие понятия и соответствующие 
термины: пер. с англ. и фр. / Всерос. науч.-исслед. ин-т метрологии им.~Д.И.\,Менделеева, 
Белорус. гос. ин-т метрологии. Изд. 2-е, испр. — СПб.: НПО <<Профессионал>>, 2010. — 
82 с.  
  
\bibitem{MudrKush-LAD} 
\textsc{Мудров В.И., Кушко В.Л.} Метод наименьших модулей. -- Москва: URSS, 2013, 
а также более ранние издания. 
    
\bibitem{MudrovKushko} 
\textsc{Мудров В.И., Кушко В.Л.} Методы обработки измерений. Квазиправдоподобные оценки. 
-- Москва: URSS, 2014, а также более ранние издания. 
  
\bibitem{Natanson}  
\textsc{Натансон И.П.} Конструктивная теория функций. -- Москва-Ленинград: ГИТТЛ, 1949. 
  
\bibitem{Nesterov1999}
\textsc{Нестеров В.М.} Твинные арифметики и их применение в методах и алгоритмах 
двустороннего интервального оценивания. дисс. \ldots д.ф.-м.н. Санкт-Петербург, 
Санкт-Петербургский институт информатики и автоматизации РАН, 1999, 234 с. 
    
\bibitem{NovitskiZograf}
\textsc{Новицкий П.В., Зограф И.А.} Оценка погрешностей результатов измерений. 
-- Ленинград: <<Энергоатомиздат>>, 1991.  
   
\bibitem{NoskovSI} %%  вставить ссылку в текст!   
\textsc{Носков С.И.} Технология моделирования объектов с нестабильным функционированием 
и неопределённостью в данных. -- Иркутск: Облинформпечать, 1996. 
  
\bibitem{AubinEkeland} 
\textsc{Обэн Ж.-П., Экланд И.} Прикладной нелинейный анализ. -- Москва: <<Мир>>, 1988. 
     
\bibitem{Orlov1991}
\textsc{Орлов А.И.} 
Часто ли распределение результатов наблюдений является нормальным? // 
Заводская лаборатория. -- 1991. -- Т.~57, №7. -- С.~64--66. 
  
\bibitem{Orlov2006} 
\textsc{Орлов А.И.} Прикладная статистика. -- Москва: Экзамен, 2006. 
    
\bibitem{OrlovLutsenko}  
\textsc{Орлов А.И., Луценко Е.В.} Системная нечёткая интервальная математика.  
-- Краснодар: Издательство КубГАУ, 2014. – 600 с. 
  
\bibitem{Orlov2016} 
\textsc{Орлов А.И.} Распределения реальных статистических данных не являются 
нормальными // Научный журнал КубГАУ. --  2016. -- №117(03). -- С.~71-90.  
  
\bibitem{Oskorbin1983}  
\textsc{Оскорбин Н.М.} Некоторые задачи обработки информации в управляемых системах // 
Cинтез и проектирование многоуровневых иерархических систем. Материалы конференции. 
-- Барнаул: Алтайский государственный университет, 1983. -- С.~64--70. 
 
\bibitem{OskorbinZhilinDronov}
\textsc{Оскорбин Н.М., Жилин С.И., Дронов С.В.} Сравнение статистической и 
нестатистической оценок параметров эмпирической зависимости. // Известия Алтайского 
государственного университета. -- 1998. -- №4. --- C.~38--41. 

\bibitem{OskorbinMaksiZhilin}
\textsc{Оскорбин Н.М., Максимов А.В., Жилин С.И.} 
Построение и анализ эмпирических зависимостей методом центра неопределенности // 
Известия Алтайского государственного университета. -- 1998. -- №1. -- С.~35--38. 
  
\bibitem{PolyakNazin} %%  вставить ссылку в текст!  
\textsc{Поляк Б.Т., Назин С.А.} Оценивание параметров в линейных многомерных системах 
с интервальной неопределенностью // Проблемы управления и информатики. -- 2006. -- 
№1-2. -- С.~103--115.  
  
\bibitem{PomeRodionova}  
\textsc{Померанцев А.Л., Родионова О.Е.} Построение многомерной градуировки методом 
простого интервального оценивания // Журнал Аналитической Химии. -- 2006. -- Т.~61, №10. 
-- С.~1032--1047. 
  
\bibitem{IntervalAnalysisExamples} 
Примеры и программы анализа интервальных данных в системе компьютерной математики Octave 
--- \url{https://github.com/szhilin/octave-interval-examples}
  
\bibitem{Poincare} 
\textsc{Пуанкаре А.} Теория вероятностей. -- Ижевск: РХД, 1999. 
  
\bibitem{Pfanzagl} 
\textsc{Пфанцагль И.} Теория измерений. -- Москва: Мир, 1976. 
  
\bibitem{RMG29-2013} 
РМГ 29--2013 ГСИ. Рекомендации по межгосударственной стандартизации. Государственная 
система обеспечения единства измерений. Метрология. Основные термины и определения. 
Издание официальное. -- Москва:  Стандартинформ, 2014. 
  
\bibitem{R50MNK} 
Р 50.2.028-2003 ГСИ. Алгоритмы построения градуировочных характеристик средств измерений 
состава веществ и материалов и оценивание их погрешностей (неопределённостей). Оценивание 
погрешности (неопределённости) линейных градуировочных характеристик при использовании 
метода наименьших квадратов. Издание официальное. --- М.: Издательство стандартов, 2003. 
  
\bibitem{Rabinovich1978} 
\textsc{Рабинович С.Г.} Погрешности измерений. — Ленинград: Энергия, 1978. 
  
\bibitem{HRaifa} 
\textsc{Райфа Г.} Анализ решений. Введение в проблему выбора в условиях неопределённости. 
-- Москва: Наука, 1977.  
  
\bibitem{Remez} 
\textsc{Ремез Е.Я.} Основы численных методов чебышёвского приближения. -- Киев: 
Наукова Думка. 1969. 
  
\bibitem{Roberts} 
\textsc{Робертс~Ф.С.} Дискретные математические модели с приложениями к социальным, 
биологическим и экологическим задачам. -- Москва: Наука, 1986. 
  
\bibitem{RoginsShulman} 
\textsc{Рогинская М.М., Шульман В.С.} О суммах по Минковскому большого числа малых 
слагаемых // Функциональный анализ и его приложения. -- 2018. -- Т.~52, №3. -- C.~88--91. 
   
\bibitem{Rodionova} 
\textsc{Родионова О.Е.} Интервальный подход к анализу больших массивов физико-химических 
данных. -- Диссертация  \ldots\, доктора физико-математических наук. -- Институт 
физической химии им.~Н.Н.\,Семёнова РАН: Москва, 2007. 
  
\bibitem{Sekei}  
\textsc{Секей Г.} Парадоксы в теории вероятностей и математической статистике. 
-- Москва: Мир, 1990. 
  
\bibitem{Semenov2012}  
\textsc{Семёнов К.К.} Нечёткие переменные как способ формализации характеристик 
погрешности в задачах математической обработки // Информатика и её применения. -- 
2012. -- Т.~6, Вып.~2. -- С.~101-112. 
  
\bibitem{SemenovKK16} 
\textsc{Семёнов К.К.} Программные средства обработки неточных данных, выполняемой 
в средствах измерений // Известия Самарского научного центра Российской Академии Наук. 
-- 2016. -- Т.~18, №4(7). -- С.~1444-1455. 
  
\bibitem{Skibitski} 
\textsc{Скибицкий Н.В.} Построение прямых и обратных статических характеристик 
объектов по интервальным данным // Заводская лаборатория. Диагностика материалов. 
-- 2017. -- Т.~83, №1, Ч.~I. -- С.~87--93. 
  
\bibitem{SmirnovDunBark} 
\textsc{Смирнов Н.В., Дунин-Барковский И.В.} Курс теории вероятностей и математической 
статистики для технических приложений. -- Москва: Наука, 1965. 
  
\bibitem{Sobol} 
\textsc{Соболь И.М.} Численные методы Монте-Карло. -- Москва: Наука, 1973. 
  
\bibitem{SpivakEtAl}   
\textsc{Спивак С.И., Зиганшина Ф.Т., Исмагилова А.С.} Информативность кинетического 
эксперимента и области неопределенности параметров кинетических моделей // Системы 
и средства информатики. -- 2018. -- Т.~28, №1. -- С.~77--88. 
  
\bibitem{MathEncycl}
Статистика. Математическая энциклопедия, том~5. -- Москва: Советская Энциклопедия, 1985. 
-- С.~169. 
  
\bibitem{MalSovEncycl}
Статистика // Малая советская энциклопедия. -- М.: Советская Энциклопедия, 1960. 
-- Т.~8. -- С.~1090. 
  
\bibitem{Stetsyuk} 
\textsc{Cтецюк П.И.} Субградиентные методы ralgb5 и ralgb4 для минимизации овражных 
выпуклых функций // Вычислительные технологии. -- 2017. -- Т.~22, №2. -- С.~127--149. 
  
\bibitem{SukhanovSlava}  
\textsc{Суханов В.А.} Исследование  эмпирических  зависимостей:  нестатистический подход.  
Сборник научных статей / под. ред. Н.М.\,Оскорбина и П.И.\,Кузьмина.  -- Барнаул: Изд-во 
Алтайского ун-та, 2007. -- 290~с. 
  
\bibitem{SukhanovSI} 
\textsc{Суханов С.И.} Интервальный анализ в задачах моделирования пространственного 
положения геообъектов. -- Барнаул : Изд-во АлтГУ, 2016. -- 110 с. \ 
Доступна на \url{http://elibrary.asu.ru/handle/asu/3393} 
  
\bibitem{Schrijver} 
\textsc{Схрейвер А.} Теория линейного и целочисленного программирования: в 2-х т.  
-- Москва: Мир, 1991. 
  
\bibitem{FPTarasenko} 
\textsc{Тарасенко Ф.П.} Непараметрическая статистика. -- Томск: Издательство Томского 
университета, 1976. 

\bibitem{JCT97}
\textsc{Телерман В.В., Сидоров В.А., Ушаков Д.М.} Интервальные и мульти-интервальные 
расширения в недоопределенных моделях // Вычислительные технологии. -- 1997. -- Т.~2, №1.
-- С.~62--70. 
  
\bibitem{Tudorovski1948} 
\textsc{Тудоровский А.И.} Теория оптических приборов. Изд. 2-е, перераб. и доп. 
В 2-х ч. --  М.-Л.: 1. Общая часть. 1948, 662 с. 
    
\bibitem{Tutubalin1972} 
\textsc{Тутубалин В.Н.} Теория вероятностей: Краткий курс и научно-методические 
замечания. -- Москва: Изд-во МГУ, 1972. 
  
\bibitem{Tutubalin1977} 
\textsc{Тутубалин В.Н.} Границы применимости (вероятностно-статистические методы и их
возможности). -- Москва: Знание, 1977. 
  
\bibitem{Tutubalin1993} 
\textsc{Тутубалин В.Н.} Вероятность, компьютеры и обработка результатов эксперимента // 
Успехи физических наук. -- 1993. -- Т.~163, №7. -- С.~93-109. 
  
\bibitem{Tutubalin2008}  
\textsc{Тутубалин В.Н.} Теория вероятностей. -- М.: Издательский центр <<Академия>>, 
2008. -- 368 с. 
  
\bibitem{GOSTIndirect} 
МИ 2083-90 Рекомендация. Государственная система обеспечения единства измерений. 
Измерения косвенные. Определение результатов измерений и оценивание их погрешностей. 
Издание официальное. --- М.: Издательство стандартов, 1991. 
  
\bibitem{WhittakerRobinson} 
\textsc{Уиттекер Э., Робинсон Г.} Математическая обработка результатов наблюдений. 
-- Москва: ОНТИ, 1935. 
  
\bibitem{Watkins} 
\textsc{Уоткинс Д.} Основы матричных вычислений. -- Издательство: Бином. Лаборатория 
знаний, 2017. 
  
\bibitem{Uspenskii}
\textsc{Успенский В.А., Верещагин Н.К., Плиско В.Е.} Вводный курс математической логики. 
-- Москва: ФИЗМАТЛИТ, 2007. 128~с.
  
\bibitem{CzechBook} 
\textsc{Фидлер М., Недома Й., Рамик Я., Рон И., Циммерманн К.} Задачи линейной оптимизации 
с неточными данными. -- Москва-Ижевск: Издательство <<РХД>>, 2008. 
  
\bibitem{HansenWalster} 
\textsc{Хансен Э., Уолстер Дж.У.} Глобальная оптимизация с помощью методов интервального 
анализа. -- Москва-Ижевск: Издательство <<РХД>>, 2012. 
  
\bibitem{Khatskevich} 
\textsc{Хацкевич И.Г.} Метод обработки одного типа измерительной информации //   
Математическое обеспечение космических экспериментов. -- Москва: Наука, 1978. 
-- С.~82--87. 
  
\bibitem{Khlebnikov1996} 
\textsc{Хлебников А.И.} О методе центра неопределённости // Журнал аналитической 
химии. -- 1996. -- Т.~51, №3. -- С.~347--348. 
  
\bibitem{Khlebnikov1999}
\textsc{Хлебников А.И.} О проблемах использования метода центра неопределенности 
для обработки экспериментальных данных // Вычислительные технологии. -- 1999. 
-- Т.~4, №4. -- С.~80--81. 
  
\bibitem{HornJohn}
\textsc{Хорн Р., Джонсон Ч.} Матричный анализ. --- Москва: Мир, 1989. 
  
\bibitem{Chaikovskii}  
\textsc{Чайковский Ю.В.} О природе случайности. -- Москва: Центр системных исследований, 
2001, 2004. 
  
\bibitem{Chelpanov} 
\textsc{Челпанов В.Г.} Учебник логики. -- Москва: Научная Библиотека, 2010. — 128 c. 
  
\bibitem{IreneJCT1997}   
\textsc{Шарая И.А.} О дистрибутивности в классической интервальной арифметике //  
Вычислительные технологии. -- 1997. -- Т.~2, №1. -- С.~71--83. 
  
\bibitem{IreneJCT2004}    
\textsc{Шарая И.А.} Ограничено ли допустимое множество решений интервальной системы? //
Вычислительные технологии. -- 2004. -- Т.~9, №3. -- С.~108--112. 
  
\bibitem{IreneSoft} 
\textsc{Шарая И.А.} Пакет \texttt{IntLinIncR2} для визуализации множеств решений 
интервальных  линейных систем с тремя неизвестными. -- Программное обеспечение, 
доступное  на \url{http://www.nsc.ru/interval/sharaya/irash} 
  
\bibitem{IreneJCT2015} 
\textsc{Шарая И.А.} Метод граничных интервалов для визуализации полиэдральных множеств 
решений // Вычислительные технологии. -- 2015. -- Т.~20, №1. -- С.~75--103. 
  
\bibitem{lineqs} 
\textsc{Шарая И.А.} Пакеты программ \texttt{lineq} и \texttt{lineqs} для визуализации 
множеств решений систем линейных неравенств. -- Программное обеспечение, доступное  
на \url{http://www.nsc.ru/interval/sharaya/irash.html#prog} 
  
\bibitem{SharysReserve} 
\textsc{Шарая И.А., Шарый С.П.} Резерв характеристического включения для интервальных 
линейных систем отношений // Вычислительные технологии. -- 2021 -- Т.~26, №3. -- 
С.~61--85. 
  
\bibitem{SSharyIzvAN97}  
\textsc{Шарый С.П.} Алгебраический подход к анализу линейных статических систем 
с интервальной неопределённостью // Известия РАН. Теория и системы управления. -- 
1997. -- № 3. -- С. 51--61. 
  
\bibitem{SShary2012} 
\textsc{Шарый С.П.} Разрешимость интервальных линейных уравнений и анализ данных 
с неопределённостями // Автоматика и телемеханика. -- 2012. -- №2. -- С.~111--125. 
  
\bibitem{SSharySibJCM}  
\textsc{Шарый С.П.} Об интервальных матрицах полного ранга // Сибирский журнал 
вычислительной математики. –– 2014. –– Т.~17, №3. –– С.~289--304. 
   
\bibitem{SSharyJCT2017} 
\textsc{Шарый С.П.} Сильная согласованность в задаче восстановления зависимостей 
при интервальной неопределенности данных // Вычислительные технологии. -- 2017. 
-- Т.~2, №2. -- C.~150--172. 
    
\bibitem{SSharyIzvAN2017} 
\textsc{Шарый С.П.} Метод максимума согласования для восстановления зависимостей 
по данным с интервальной неопределённостью // Известия Академии Наук. Теория и 
системы управления. -- 2017. -- №6. -- C.~3--19. 
  
\bibitem{SShary2018}  
\textsc{Шарый С. П.} Выявление выбросов в методе максимума согласования при анализе 
интервальных данных // Сборник трудов Всероссийской конференции по математике 
с международным участием <<МАК-2018>>. -- Барнаул : Изд-во АлтГУ, 2018. -- С. 215--218. \\ 
Доступна на \url{http://elibrary.asu.ru/handle/asu/6303}
  
\bibitem{SShary2019}
\textsc{Шарый С.П.} О мере вариабельности оценки параметров в статистике интервальных 
данных // Вычислительные технологии. -- 2019. -- Т.~24, №5. -- С.~90--108. 
   
\bibitem{SSharyPLab2020} 
\textsc{Шарый С.П.} Задача восстановления зависимостей по данным с интервальной 
неопределённостью // Заводская лаборатория. Диагностика материалов. -- 2020. 
-- Т.~86, №1. -- С.~62--74. \  
% DOI: 10.26896/1028-6861-2020-86-1-62-74 
    
\bibitem{SSharyBook} 
\textsc{Шарый С.П.} Конечномерный интервальный анализ. -- ФИЦ ИВТ: 
Новосибирск, 2022. \     Электронная книга, доступная 
на \url{http://www.nsc.ru/interval/Library/InteBooks/SharyBook.pdf} 
  
\bibitem{SShary2022} 
\textsc{Шарый С.П.} Восстановление функциональных зависимостей по данным с интервальной 
неопределённостью // Информатика и системы управления. -- 2022. -- №3 (73). -- 
С.~130--143. 
  
\bibitem{SSharyZhilin} 
\textsc{Шарый С.П., Жилин С.И.} Простые, быстрые и надёжные способы максимизации 
распознающего функционала // Вычислительные Технологии. -- 2023. -- в печати. 
  
\bibitem{SharysJCT2013}
\textsc{Шарый С.П., Шарая И.А.} Распознавание разрешимости интервальных уравнений 
и его приложения к анализу данных // Вычислительные технологии. -- 2013. -- Т. 18. 
-- №3. -- С.~80--109. 
  
\bibitem{ShorZhurbenko} 
\textsc{Шор Н.З., Журбенко Н.Г.} Метод минимизации, использующий операцию растяжения 
пространства в направлении разности двух последовательных градиентов // Кибернетика. 
-- 1971. -- №3. -- С.~51--59. 
  
\bibitem{Shreider} 
\textsc{Шрейдер Ю.А.} Равенство, сходство, порядок. -- Москва: Наука, 1971. 
   
\bibitem{Eliasberg76} 
\textsc{Эльясберг П.Е.} Определение движения по результатам измерений. -- Москва: 
<<Наука>>, 1976. 
  
\bibitem{Eliasberg83} 
\textsc{Эльясберг П.Е.} Измерительная информация: сколько её нужно? как её обрабатывать? 
-- Москва: <<Наука>>, 1983. 
  
\bibitem{YavorskiDetlaf}  
\textsc{Яворский Б.М., Детлаф А.А.} Справочник по физике для инженеров и студентов ВУЗов. 
-- Москва: Наука, 1968, а также последующие издания. 
  
\bibitem{Iakovlev1968}
\textsc{Яковлев А.Г.} Машинная арифметика мультиинтервалов // Вопросы кибернетики
(Научный Совет по компл. проблеме <<Кибернетика>>: АН СССР). -- 1986. – Вып. 125. 
-- С.~66--81. 
  
\bibitem{Allen} 
\textsc{Allen J.F.} Maintaining knowledge about temporal intervals // Communications 
of the ACM. -- 1983. -- Vol.~26, Issue~11. -- P.~832--843. \  DOI: 10.1145/182.358434 
  
\bibitem{Amrhein2019} 
\textsc{Amrhein, V., Greenland, S., McShane D.} Scientists rise up against 
statistical significance // Nature. -- 2019. -- Vol.~567. -- P.~305--307. \  
\url{https://www.nature.com/articles/d41586-019-00857-9}
  
\bibitem{Artbauer} 
\textsc{Artbauer O.} Application of interval, statistical, and fuzzy methods 
to the evaluation of measurements // Metrologia. -- 1988. -- Vol.~25. -- P.~81--86. 
 
\bibitem{UniCalc}
\textsc{Babichev A.B., Kadyrova O.B., Kashevarova T.P., Leshchenko A.S., Semenov A.L.} 
UniCalc, a novel approach to solving systems of algebraic equations // Interval 
Computations. -- 1993. -- Vol.~2. -- P.~29--47.  
  
\bibitem{BillardDiday} 
\textsc{Billard L., Diday E.} Symbolic Data Analysis: Conceptual Statistics 
and Data Mining. -- Chichester: John Wiley \& Sons, 2006. 

\bibitem{BronKerbosch} 
\textsc{Bron~C., Kerbosch~J.} Algorithm 457: Finding all cliques of an undirected 
graph // Communications of the ACM. -- 1973. -- Vol.~16. -- Issue~9. -- P.~575--577.
\doi{10.1145/362342.362367}.
  
\bibitem{DRS4} 
DRS4 microchip. \  URL: 
\url{https://www.psi.ch/sites/default/files/import/drs/DocumentationEN/DRS4_rev09.pdf}
     
\bibitem{EppsteinEtAl} 
\textsc{Eppstein~D., Löffler~M., Strash~D.} Listing all maximal cliques in sparse 
graphs in near-optimal time // Algorithms and Computation: Proc. / ISAAC 2010, Jeju Island, 
Korea, 2010. -- Berlin, Heidelberg: Springer, 2010. -- P.~403--414 -- (Lecture Notes in 
Computer Science; 6506). \  \doi{10.1007/978-3-642-17517-6\_36}
  
\bibitem{HuCHuZH}  
\textsc{Hu C., Hu Z.H.} On statistics, probability, and entropy of interval-valued 
datasets // Lesot MJ. et al. (eds) Information Processing and Management of Uncertainty 
in Knowledge-Based Systems. IPMU 2020. Communications in Computer and Information 
Science, vol 1239. -- Cham: Springer, 2020. 
  
\bibitem{CorderForeman} 
\textsc{Corder G.W., Foreman D.I.} Nonparametric Statistics for Non-Statisticians. 
A Step-by-Step Approach. -- Hoboken, N.J.: John Wiley and Sons, 2009. 
  
\bibitem{CowenEllison} 
\textsc{Cowen S., Ellison S.L.R.}  
Reporting measurement uncertainty and coverage intervals near natural limits // 
Analyst. -- 2006. -- Vol.~131, Issue~6. -- P.~710--717. 
  
\bibitem{ThickSet}  
\textsc{Desrochers B., Jaulin L.} Thick set inversion // Artifical Intelligence. 
-- 2017. -- Vol.~249. -- P.~1--18. 
  
\bibitem{Domes2020}
\textsc{Domes, F. et al.} Rigorous global filtering methods with interval unions // 
Beyond Traditional Probabilistic Data Processing Techniques: Interval, Fuzzy etc. 
Methods and Their Applications. Editors: Kosheleva, O., Shary, S.P., Xiang, G., 
Zapatrin, R. (Eds.) Springer International Publishing, 2020. P.~249--267. \ 
DOI: 10.1007/978-3-030-31041-7
 
\bibitem{KreinovichGeodesy}  
\textsc{Dbouk H., Sch{\"o}n S., Neuman I. Kreinovich V.}
When can we be sure that measurement results are consistent: 1-D interval case and beyond 
// Technical Report: UTEP-CS-20-67, University of Texas at El Paso, July 2020. 
\url{https://scholarworks.utep.edu/cgi/viewcontent.cgi?article=2457&context=cs_techrep} 
%// Reliable Computing. -- 2021. -- Vol.~??, No.~?. -- P. ???--???. 

\bibitem{Eurachem2007}  
\textsc{Ellison S.L.R., Williams A. (Eds.)} EURACHEM/CITAC Guide: Use of uncertainty 
information in compliance assessment. First edition, 2007. \\ 
\url{https://www.eurachem.org/images/stories/Guides/pdf/Interpretation_with_expanded_uncertainty_2007_v1.pdf} 

\bibitem{Twins1981}
\textsc{Garde\~{n}es E., Trepat A., Janer J.M.} Approaches to simulation and to the linear 
problem in the SIGLA system // Freiburger Intervall-Berichte. -- 1981. -- No.~8. 
-- S.~1--28. 
  
\bibitem{Goldberg} 
\textsc{Goldberg D.} What every computer scientist should know about floating point 
arithmetic // ACM Computing Surveys. -- 1991. -- Vol.~23, No.~1. -- P.~5--48. 
    
\bibitem{Gutowski} 
\textsc{Gutowski M.W.} Interval experimental data fitting // Focus on Numerical Analysis, 
ed. by J.P.~Liu. -- New York, Nova Science Publishers, 2006. -- P.~27--70. 

\bibitem{Habib}
\textsc{Habib M., McConnell R., Paul C., Viennot L.} Lex-BFS and partition refinement, 
with applications to transitive orientation, interval graph recognition, and consecutive 
ones testing // Theor. Comput. Sci. -- 2000. -- Т. 234. -- С. 59-84.  \  
\doi{10.1016/S0304-3975(97)00241-7} 
  
\bibitem{SIVIAMatlab}
\textsc{Herrero P.} An efficient implementation of the Set Inversion 
via Interval Analysis (SIVIA) algorithm in Matlab. \ 
URL: \url{https://gitlab.doc.ic.ac.uk/pherrero/vsivia}
      
\bibitem{InCpp}
\textsc{Hyvönen, E., Pascale, S.D.} InC++ library family for interval computations // 
Reliable computing, Supplement (Extended abstracts of APIC’95: International workshop 
on applications of interval computations, El Paso, TX, 23–25, February 1995, pp. 23–25.  
      
\bibitem{MetrolVocabOrig} 
International Vocabulary of Metrology --- Basic and General Concepts and Associated 
Terms (VIM 3rd edition). JCGM 200:2012. Электронная версия доступна на  
\url{https://www.bipm.org/en/publications/guides/vim.html}
  
\bibitem{IEEE-1788} 
IEEE 1788-2015 --- Standard for Interval Arithmetic.  
Электронная версия доступна на \url{https://standards.ieee.org/ieee/1788/4431/} 
  
\bibitem{ISM} 
ISM radio band. Электронный ресурс  \url{https://en.wikipedia.org/wiki/ISM_radio_band} 
  
\bibitem{IUPACtable} 
IUPAC Periodic Table of Elements. URL: 
\url{https://iupac.org/what-we-do/periodic-table-of-elements/} 
  
\bibitem{JaulinWalter} 
\textsc{Jaulin L., Walter E.} Guaranteed nonlinear parameter estimation via interval 
computations // Interval Computations. -- 1993. -- Vol.~3, No.~3. -- P.~61--75. 

\bibitem{Kabir2017}
\textsc{Kabir, S., Wagner, C., Havens, T. C., Anderson, D. T., Aickelin, U.}
Novel Similarity Measure for Interval-Valued Data Based on Overlapping Ratio // IEEE International Conference on Fuzzy Systems(FUZZ-IEEE 2017), IEEE. \doi{10.1109/FUZZ-IEEE.2017.8015623}
   
\bibitem{KearfottKreinovich}    
\textsc{Kreinovich V., Kearfott R.B.} Beyond convex? Global optimization is feasible 
only for convex objective functions: a theorem // J. Global Optimization. -- 2005. 
-- Vol.~33, Issue~4. -- P.~617--624. 
  
\bibitem{InteNotation} 
\textsc{Kearfott, R.B., Nakao, M., Neumaier, A., Rump, S., Shary, S.P., van Hentenryck, 
P.} Standardized notation in interval analysis // Вычислительные Технологии. -- 
2010. -- Т.~15, №1. -- С.~7--13. 
  
\bibitem{Kreinovich} 
\textsc{Kreinovich V.} Why intervals? A simple limit theorem that is similar 
to limit theorems from statistics // Reliable Computing. -- 1995. -- Vol.~1. 
-- P.~33--40.  
  
\bibitem{KreLakRohnKahl} 
\textsc{Kreinovich V., Lakeyev A.V., Rohn J., Kahl P.} Computational Complexity 
and Feasibility of Data Processing and Interval Computations. -- Dordrecht: Kluwer, 
1998.  
  
\bibitem{KreinovichShary}  
\textsc{Kreinovich V., Shary S.} Interval methods for data fitting under uncertainty: 
a probabilistic treatment // Reliable Computing. -- 2016. -- Vol.~23. -- P.~105--140. 
  
\bibitem{VladikShary-2021} 
\textsc{Kreinovich V., Shary S.P.} How probabilistic methods for data fitting deal 
with interval uncertainty: a more realistic analysis / Departmental Technical Report 
UTEP-CS-21-102. University of Texas at El Paso. -- El~Paso, 2021. \   
URL: \url{https://scholarworks.utep.edu/cs_techrep/1635} 
  
\bibitem{Kubica} 
\textsc{Kubica B.J.} Interval methods for solving nonlinear constraint satisfaction, 
optimization and similar problems. From inequalities systems to game solutions. 
-- Cham, Switzerland: Springer, 2019. 
  
\bibitem{Kumkov2013}  
\textsc{Kumkov, S.I., Mikushina, Yu. V.} Interval approach to identification 
of catalytic process parameters // Reliable Computing. -- 2013. -- Vol.~19. 
-- P.~197--214. 
  
\bibitem{LakeKreino}  
\textsc{Lakeyev A.V., Kreinovich V.} If input intervals are small enough, then interval 
computations are almost always easy // \textsl{Reliable Computing, 1995, Supplement} 
(Extended Abstracts of APIC'95: International Workshop on Applications of Interval 
Computations, El Paso, TX, Febr. 23--25, 1995). -- P.~134--139. 
  
\bibitem{Laveuve}
\textsc{Laveuve S.E.} Definition einer Kahan-Arithmetic und ihre Implementierung // 
\textsl{Interval Mathematics} / Nickel~K., ed. -- Berlin: Springer Verlag, 1975. -- 
P.~236--245. -- (\textsl{Lecture Notes in Computer Science; vol.~29}). 
  
\bibitem{MayerBook} 
\textsc{Mayer G.} Interval Analysis and Automatic Result Verification. -- Berlin-Boston: 
de Gruyter, 2017.  
  
\bibitem{IUPAC-2016} 
\textsc{Meija, J., Coplen, T.B., Berglund, M., Brand, W.A., De Bièvre, P., 
Gröning, M., Holden, N.E., Irrgeher, J., Loss, R.D., Walczyk, T., Prohaska, T.} 
Atomic weights of the elements 2013 (IUPAC Technical Report) // Pure and Applied 
Chemistry. -- 2016. -- Vol.~88, Issue~3. -- P.~265--291. \   
\doi{10.1515/pac-2015-0305}  
  
\bibitem{BoundApprHandbook} 
\textsc{Milanese M., Norton J., Piet-Lahanier H., Walter E. (Eds.)} Bounding 
Approaches to System Identification. -- Plenum Press, New York, 1996. \   
\doi{10.1007/978-1-4757-9545-5} 

\bibitem{IntervalUnions2017N} 
\textsc{Montanher T., Domes F., Schichl H., Neumaier A.} Using interval unions to solve 
linear systems of equations with uncertainties. // BIT Numer Math. -- 2017. -- Vol.~57, 
No.~3. -- P.~901--926. \     \doi{10.1007/s10543-017-0657-x} 
  
\bibitem{Moore1992} 
\textsc{Moore R.E.} Parameter sets for bounded-error data // Mathematics and Computers 
in Simulation. -- 1992. -- Vol.~34, Issue~2. -- P.~113--119. 
  
\bibitem{MooreBakerCloud} 
\textsc{Moore R.E., Kearfott R.B., Cloud M.J.} Introduction to Interval Analysis. 
-- SIAM, Philadelphia, 2009. 
 
\bibitem{OpticsSchema}
\textsc{Mukhin E.E., Kurskiev G.S., Gorbunov A.V., Samsonov D.S., Tolstyakov S.Yu., 
Razdobarin A.G., Babinov N.A., Bazhenov A.N., Bukreev I.M., Dmitriev A.M., et al.} 
Integration of Thomson scattering and laser-induced fluorescence in ITER divertor // 
Nuclear Fusion. -- 2019. -- Vol.~59, No.~8. -- 086052. 
\doi{10.1088/1741-4326/ab1cd5}  
  
\bibitem{EAMurphy} 
\textsc{Murphy E.A.} One cause? Many causes? The argument from the bimodal distribution // 
Journal of Chronic Diseases. -- 1964. -- Vol.~17. -- P.~301--324. 
  
\bibitem{NeumaierBook} 
\textsc{Neumaier A.} Interval Methods for Systems of Equations. -- Cambridge 
University Press, 1990. 
  
\bibitem{Nesterov1997} 
\textsc{Nesterov V.M.} Interval and twin arithmetics // Reliable Computing. -- 1997. 
-- Vol.~3. -- P.~369--380. 
  
\bibitem{NguyenKreinWuXiang} 
\textsc{Nguyen H.T., Kreinovich V., Wu B., Xiang G.} Computing Statistics 
under Interval and Fuzzy Uncertainty. Applications to Computer Science and Engineering. 
-- Springer, Berlin-Heidelberg, 2012. 
  
\bibitem{Nurminski} 
\textsc{Nurminski E.A.} Separating plane algorithms for convex optimization // 
Mathematical Programming. -- 1997. -- Vol.~76. -- P.~373–391. 
    
\bibitem{OctaveInterval} 
Octave \texttt{interval} package. Real-valued interval arithmetic. \\ 
URL: \url{https://octave.sourceforge.io/interval/} 
  
\bibitem{SIVIAPython} 
A Python binding for ibex-lib. \   URL: \url{https://github.com/benEnsta/pyIbex}
      
\bibitem{GraviConstChina}  
\textsc{Qing Li, Chao Xue, Jian-Ping Liu, et al.} Measurements of the gravitational 
constant using two independent methods // Nature. -- 2018. -- Vol.~560. -- P.~582--588. \  
\doi{10.1038/s41586-018-0431-5} 
  
\bibitem{Rabinovich2005} 
\textsc{Rabinovich S.G.} Measurement Errors and Uncertainties. Theory and Practice.
Third Edition. -- New York: AIP Press, Springer, 2005. 
  
\bibitem{RohnOverDeterm} 
\textsc{Rohn J.} Enclosing solutions of overdetermined systems of linear interval 
equations // Reliable Computing. -- 1996. -- Vol.~2. -- P.~167--171. 
  
\bibitem{RohnHandbook} 
\textsc{Rohn, J.} A Handbook of Results on Interval Linear Problems. Technical report 
No.~V-1163. -- Prague: Institute of Computer Science, Academy of Sciences of the Czech 
Republic, 2012.   \  Доступен на  
\url{http://www.nsc.ru/interval/Library/Surveys/ILinProblems.pdf} 
  
\bibitem{GraviConstItaly}  
\textsc{Rosi G., Sorrentino F., Cacciapuoti L., Prevedelli M., Tino G.M.} 
Precision measurement of the Newtonian gravitational constant using cold atoms //  
Nature. 2014. Vol.~510, P.~518--521. \  DOI: 10.1038/nature13433  \\   
Предварительная версия работы депонирована в репозитории arXiv.org, статья 
\url{https://arxiv.org/pdf/1412.7954.pdf}

\bibitem{ModalIABook} 
\textsc{Sainz M.A., Armengol J., Calm R., Herrero P., Jorba L.J., Vehi J.}
Modal Interval Analysis: New Tools for Numerical Information. -- Cham, Switzerland: 
Springer, 2014. -- (\textsl{Lecture Notes in Mathematics; vol.~2091}). 

\bibitem{IntervalUnions2017}
\textsc{Schichl H., Domes F., Montanher T., Kofler K.} Interval unions // 
BIT Numerical Mathematics. -- 2017. -- Vol.~57. -- P.~531–556. 

\bibitem{Donoho2017}
\textsc{Donoho D.} 50 years of Data Science // Journal of Computational and Graphical 
Statistics. -- Vol.~26. -- Issue~4. -- P.~745--766. 
  
\bibitem{IreneRC2005} 
\textsc{Sharaya I.A.} On unbounded tolerable solution sets // Reliable Computing.  
-- 2005. -- Vol.~11, Issue~5. -- P.~425--432. 
\doi{10.1007/s11155-005-0049-9} 
    
\bibitem{IreneRC2014} 
\textsc{Sharaya I.A.} Boundary intervals method for visualization of polyhedral 
solution sets // Reliable Computing. -- 2014. -- Vol.~19, Issue~4. -- P.~435--467. 
    
\bibitem{SSharySurvey2002} 
\textsc{Shary S.P.} A new technique in systems analysis under interval uncertainty 
and ambiguity // Reliable Computing. -- 2002. -- Vol.~8, No.~5. -- P.~321-418. \ 
\doi{10.1023/A:1020505620702} 
  
\bibitem{SSharyJGO2016} 
\textsc{Shary S.P.} Maximum consistency method for data fitting under interval 
uncertainty // Journal of Global Optimization. -- 2016. -- Vol.~66, No.~1. 
-- P. 111--126. \ 
\doi{10.1007/s10898-015-0340-1} 
  
\bibitem{SSharyADSAA}   
\textsc{Shary S.P.} Weak and strong compatibility in data fitting problems 
under interval uncertainty // Advances in Data Science and Adaptive Analysis. -- 
2020. -- Vol.~12, No.~1. -- 2050002. \ 
\doi{10.1142/S2424922X20500023} 
  
\bibitem{SShary-arXiv}   
\textsc{Shary S.P.} A variability measure for estimates of parameters in interval 
data fitting. -- Работа депонирована 11.III.2020 года в репозитории \texttt{arxiv.org}  
как статья 2003.05126. 
  
\bibitem{SharyMoradi} 
\textsc{Shary S.P. Moradi B.} Solving interval linear least squares problems 
by PPS-methods //  Numerical Algorithms. -- 2021. -- Vol.~87, No.~1. -- P.~41--75.  
  
\bibitem{StolfiFigueiredo} 
\textsc{Stolfi J., de Figueiredo L.H.} Self-validated numerical methods and applications. 
-- Rio de Janeiro: IMPA, Brazilian Mathematics Colloquium monograph, 1997. 
  
\bibitem{Tukey1962}
\textsc{Tukey J.W.} The Future of Data Analysis // 
Annals of Mathematical Statistics -- 1962. -- Vol.~33, Issue~1. -- P.~1--67 \  
\doi{10.1214/aoms/1177704711} 

\bibitem{Tukey1972}
\textsc{Tukey J.W.} Data Analysis, Computation and Mathematics //
Quarterly of Applied Mathematics. -- 1972. -- Vol.~30, -- P. 51-–65.

\bibitem{ChemInternatl} 
Standard atomic weights of 14 chemical elements revised // Chemistry International. 
-- 2018. -- Vol.~40, Issue 4. -- P.~23--24. \  \doi{10.1515/ci-2018-0409} 
    
\bibitem{Wasserman} 
\textsc{Wasserman L.} All of Nonparametric Statistics. -- New York: Springer, 2006. 
  
\bibitem{Wasserstein} 
\textsc{Wasserstein, R.L.,  Schirm, A.L., Lazar, N.A.} Moving to a world beyond 
<<$p < 0.05$>> // The American Statistician. -- 2019. -- Vol.~73. -- P.~1--19. \   
\doi{10.1080/00031305.2019.1583913} 
  
%\bibitem{WilkinBeliakov}   
%\textsc{T. Wilkin and G. Beliakov}, ``The Mode of Interval-Valued Data'', 
%2019 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2019, pp. 1-6, \\
% \doi{10.1109/FUZZ-IEEE.2019.8858850} 
  
\bibitem{Zhilin2005}  
\textsc{Zhilin S.I.} On fitting empirical data under interval error // 
Reliable Computing. -- 2005. -- Vol.~11. -- P.~433--442. \ 
\doi{10.1007/s11155-005-0050-3}  
  
\bibitem{Zhilin2007}  
\textsc{Zhilin S.I.} Simple method for outlier detection in fitting experimental data 
under interval error // Chemometrics and Intelligent Laboratory Systems. -- 2007. 
-- Vol.~88, No.~1. -- P.~60-68. \  
\doi{10.1016/j.chemolab.2006.10.004}  
  
\bibitem{Jaccard}  Коэффициент Жаккара. 
URL: \url{https://en.wikipedia.org/wiki/Jaccard_index} 
%\textsc{Jaccard P.} Distribution de la flore alpine dans le Bassin des Dranses 
%et dans quelques regions voisines // Bull. Soc. Vaudoise sci. Natur. 1901. V. 37. 
%Bd. 140. S. 241—272.
  
\end{thebibliography} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\addcontentsline{toc}{chapter}{Предметный указатель} 
\raggedright\small\printindex 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak
  
\begin{itemize}
  
\item ДЗ С.И.\,Жилину --- написать параграф про выбросы  
      
\item ДЗ С.П.\,Шарому: Обозреть список вкладов в интервальный подход к решению 
задачи восстановления зависимостей.
    
\item ДЗ С.П.\,Шарому: дописать определение информационного множества. 
  
\item ДЗ Баженову: п. 3.3.3 Доработать текст, написать (или убрать) 
      содержательно про $t$-нормы 
  
\item ДЗ С.П.\,Шарому: п.3.7:  ... Пример 3.7.1. Иллюстративный пример вариабельности 
выборки. Разбор нескольких примеров нормы. Содержательное обсуждение. 
  
\item ДЗ Баженову: п.3.7: Доработать оформление рисунка 3.20. 

\item ДЗ Баженову: привести ссылки на цитируемые публикации к стандартному виду
   
\item ДЗ Кумкову С.И.: к разделу 4.4 --- глубоко продумать и прочувствовать
      определение прогнозного коридора и изложить нам свои новые идеи о том, 
      как строить его в случае пустого информационного множества, 
      когда коридора совместных зависимостей нет. 
  
\end{itemize}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\end{document} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
